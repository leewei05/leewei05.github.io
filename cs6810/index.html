<!doctype html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <link rel="shortcut icon" href="/images/favicon.png" />

<title>Fall 2024: CS 6810 Computer Architecture&nbsp;|&nbsp; :)</title>
<meta
  name="title"
  content="Fall 2024: CS 6810 Computer Architecture"
/>
<meta
  name="description"
  content="Week 1 Introduction and metrics
Week 2 Metrics and ISA
Week 3 To improve the performance of a processor, we introduce a technique called Pipelining. Pipelining splits instructions into multiple stages. Ideally, Throughput increases by a factor of # of the stages.
Pipeling are usually 5 stages: IF, ID, EXE, MEM, WB.
Instruction Fetch(IF): fetch instruction from memory and PC = PC &#43; 4. Instruction Decode(ID): read registers from register file and sign extension for immediate value."
/>
<meta
  name="keywords"
  content=""
/>

  <meta name="author" content="Lee Wei" />




<meta property="og:title" content="Fall 2024: CS 6810 Computer Architecture" />
<meta property="og:description" content="Week 1 Introduction and metrics
Week 2 Metrics and ISA
Week 3 To improve the performance of a processor, we introduce a technique called Pipelining. Pipelining splits instructions into multiple stages. Ideally, Throughput increases by a factor of # of the stages.
Pipeling are usually 5 stages: IF, ID, EXE, MEM, WB.
Instruction Fetch(IF): fetch instruction from memory and PC = PC &#43; 4. Instruction Decode(ID): read registers from register file and sign extension for immediate value." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://leewei.co/cs6810/" /><meta property="article:section" content="blog" />
<meta property="article:published_time" content="2024-12-05T20:47:54-06:00" />
<meta property="article:modified_time" content="2024-12-05T20:47:54-06:00" />




<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Fall 2024: CS 6810 Computer Architecture"/>
<meta name="twitter:description" content="Week 1 Introduction and metrics
Week 2 Metrics and ISA
Week 3 To improve the performance of a processor, we introduce a technique called Pipelining. Pipelining splits instructions into multiple stages. Ideally, Throughput increases by a factor of # of the stages.
Pipeling are usually 5 stages: IF, ID, EXE, MEM, WB.
Instruction Fetch(IF): fetch instruction from memory and PC = PC &#43; 4. Instruction Decode(ID): read registers from register file and sign extension for immediate value."/>




<meta itemprop="name" content="Fall 2024: CS 6810 Computer Architecture">
<meta itemprop="description" content="Week 1 Introduction and metrics
Week 2 Metrics and ISA
Week 3 To improve the performance of a processor, we introduce a technique called Pipelining. Pipelining splits instructions into multiple stages. Ideally, Throughput increases by a factor of # of the stages.
Pipeling are usually 5 stages: IF, ID, EXE, MEM, WB.
Instruction Fetch(IF): fetch instruction from memory and PC = PC &#43; 4. Instruction Decode(ID): read registers from register file and sign extension for immediate value."><meta itemprop="datePublished" content="2024-12-05T20:47:54-06:00" />
<meta itemprop="dateModified" content="2024-12-05T20:47:54-06:00" />
<meta itemprop="wordCount" content="2775">
<meta itemprop="keywords" content="" />
<meta name="referrer" content="no-referrer-when-downgrade" />

    
    
    
    <link href="/bundle.min.css" rel="stylesheet" />

    

    


    
</head>

  <body>
    <header>
      <nav>
  <a
    href="/"
    
    >Home</a
  >

  <a
    href="/blog/"
    
    >Blog</a
  >


  <a href="/index.xml">
    <svg xmlns="http://www.w3.org/2000/svg" class="icon" viewBox="0 0 448 512">
      
      <path
        d="M0 64C0 46.3 14.3 32 32 32c229.8 0 416 186.2 416 416c0 17.7-14.3 32-32 32s-32-14.3-32-32C384 253.6 226.4 96 32 96C14.3 96 0 81.7 0 64zM0 416a64 64 0 1 1 128 0A64 64 0 1 1 0 416zM32 160c159.1 0 288 128.9 288 288c0 17.7-14.3 32-32 32s-32-14.3-32-32c0-123.7-100.3-224-224-224c-17.7 0-32-14.3-32-32s14.3-32 32-32z"
      />
    </svg>
    RSS
  </a>

</nav>

<h1>Fall 2024: CS 6810 Computer Architecture</h1>


    </header>
    <main>
      
  
    
      <p>
        <i>
          <time
            style="color: var(--text-light);"
            datetime="2024-12-05"
            pubdate
          >
            2024-12-05
          </time>
        </i>
      </p>
    
  
  
  <content>
    <h3 id="week-1">Week 1</h3>
<p>Introduction and metrics</p>
<h3 id="week-2">Week 2</h3>
<p>Metrics and ISA</p>
<h3 id="week-3">Week 3</h3>
<p>To improve the performance of a processor, we introduce a technique called <code>Pipelining</code>.
<code>Pipelining</code> splits instructions into multiple stages. Ideally, Throughput increases by a factor of # of the stages.</p>
<p>Pipeling are usually 5 stages: IF, ID, EXE, MEM, WB.</p>
<ol>
<li>Instruction Fetch(IF): fetch instruction from memory and PC = PC + 4.</li>
<li>Instruction Decode(ID): read registers from register file and sign extension for immediate value.</li>
<li>Execution(EXE): execute the instruction with one input register 0 and either register 1 or immediate value. Computing branch can also be in this stage.</li>
<li>Access Memory(MEM): execute load or store instruction.</li>
<li>Write Back(WB): write value back to register file.</li>
</ol>
<p>Each stage has a buffer that passes information to the next stage. These buffers are controlled by controlling signals.</p>
<p>One problem for pipelining is to balance the clock period of each stage since the lowest circuit delay determines the clock cycle.</p>
<h3 id="week-4">Week 4</h3>
<p>Pipeline Hazards are events that restrict the pipeline flow.</p>
<ol>
<li>Structural Hazard: resource conflicts. For instance, processor with one memory unit could have structural hazard when fetching instruction and executing memory instruction at the same time.</li>
<li>Data Hazard:</li>
<li>Control Hazard:</li>
</ol>
<p>Static branch predictor: fixed prediction.</p>
<h3 id="week-5">Week 5</h3>
<p>Scoreboarding is a technique for allowing instructions to execute out of order when there are sufficient resources and no data dependences. It utilizes in-order issues.</p>
<p>Scoreboarding limitation</p>
<ul>
<li>Structural hazard: functional units are busy for the current instruction.</li>
<li>Resolving WAW, RAW, WAR with stalls.</li>
<li>Registers are only read when they are both available.</li>
</ul>
<p>Main idea:</p>
<ul>
<li>Split ID into two stages:
<ul>
<li>Issue: decode instruction, check for structural hazard and WAW.</li>
<li>Read operands: wait until no RAW hazard, read data from registers.</li>
</ul>
</li>
<li>Execution</li>
<li>Write Back: check for WAR</li>
</ul>
<h3 id="week-6">Week 6</h3>
<p>Tomasulo&rsquo;s Algorithm</p>
<ul>
<li>Reservation stations: a buffer infront of every function unit, so that processor doesn&rsquo;t stall when there&rsquo;s structural hazard.</li>
<li>Register renaming: read value for operands without reading from register file. These renaming data are stored in the rename table.</li>
<li>A common data bus(CDB) connects every reservation station.</li>
</ul>
<p>Tomasulo limitation</p>
<ul>
<li>Branch stall execution. Tomasulo doesn&rsquo;t allow branch prediction. It waits until branch is resolved.</li>
<li>Loads and stores are performed in order.</li>
</ul>
<p>How can we support branch prediction - speculation execution</p>
<p>Multi-issue processors</p>
<ul>
<li>Superscalar: instructions are chosen dynamically by the hardware.</li>
<li>VLIW: instructions are chosen statically by the compiler. Intel Itanium</li>
</ul>
<p>For Tomasulo algorithm, we cannot tell which instructions are after branch instruction due to out-of-order execution.</p>
<ol>
<li>Identify instructions after the branch.</li>
<li>Exception in specualtive code should be buffered before actually raising the exception.</li>
<li>Precise exception: when a exception is raised, all instructions after the exception are squashed.</li>
</ol>
<p>Add a reorder buffer to keep track the original order when issuing instructions.</p>
<p>Issue inorder -&gt; Execute out-of-order -&gt; Commit inorder</p>
<h3 id="week-7">Week 7</h3>
<p>Instruction can only be fetched when a branch is resolved.</p>
<p>Why do we need reservation station when we have reorder buffer?</p>
<p>reorder buffer holds output
reservation station buffers input</p>
<p>Tomasulo with Hardware Speculation</p>
<p>Issue -&gt; Execute -&gt; Write Result(ROB) -&gt; Commit</p>
<p>Trace cache</p>
<p>Midterm review: all until superscalars</p>
<p>Macro-op fusion: Fuses simple instruction combinations to reduce instruction count, kind of like Peephole optimization.</p>
<p>Practical limitations to ILP: programs can only have a certain level of concurrency</p>
<h3 id="week-9">Week 9</h3>
<p>Midterm review + Midterm</p>
<h3 id="week-10">Week 10</h3>
<p>Temporal Locality: recent memory access will have higher chances to be accessed again.
Spatial Locality: locations near the cenet memory access will have higher chances to be accessed.</p>
<p>SRAM: cache
DRAM: Memory</p>
<p>Cache Block placement</p>
<ul>
<li>Fully Associative(one set): block can go any where.
<ul>
<li>Have lower miss rate.</li>
<li>Must search the whole cache to find the block.</li>
</ul>
</li>
<li>Direct Mapped: block can only go to location <code>mod blocksize</code>.
<ul>
<li>Simplest approach.</li>
<li>Blocks map to the same location, resulting in higher miss rate.</li>
<li>Only have one replacement policy.</li>
</ul>
</li>
<li>Set Associative: n-way Associative, each set can have at most n blocks.
<ul>
<li>Higher level caches: 2- or 4-way common (faster search).</li>
<li>Lower level caches: 8- to 32-way common.</li>
</ul>
</li>
</ul>
<p>Cache Block Identification: Tag - Index - Block Offset</p>
<ul>
<li>Example: Cache 32 KBytes, 2-way, 64 Bytes per line, Address 32 bits
<ul>
<li>0x000249F0 = (0000 0000 0000 0010 0100 1001 1111 0000)_2</li>
<li>Block offset = log_2 64 = 6 bits</li>
<li>Index = log_2(32K / 64 / 2 (2-way)) = 15 - 6 - 1 = 8 bits</li>
<li>Tag = 32 - 8 - 6 = 18 bits</li>
</ul>
</li>
</ul>
<p>Eviction Methods: which cache block to evict?</p>
<ul>
<li>Random</li>
<li>Least-recently-used(LRU)</li>
<li>Not-recently-used(NRU): any cache block other than most-recently-used.</li>
</ul>
<p>Inclusive cache</p>
<ul>
<li>lower level cache has a copy of every block in higher-level caches.
<ul>
<li>pros: in parallel systems, if lower-level cache is not presented, system doesn&rsquo;t need to search higher level cache.</li>
<li>cons: need to evict each level&rsquo;s cache block if a cache block is evicted.
Exclusive cache</li>
</ul>
</li>
<li>each level has dintict cache blocks.
<ul>
<li>pros: efficient use of space since there is no duplicate cache block.</li>
<li>cons: cache coherence across different processors.</li>
</ul>
</li>
</ul>
<p>Average Memory Access Time(AMAT) = Hit time + Miss rate * Miss penalty = Hit rate * Hit time + Miss rate * Miss time</p>
<ul>
<li>Hit time is always there because whether the block we&rsquo;re trying to access is in the cache, we will need to check the cache.</li>
</ul>
<p>Techniques for reducing hit time</p>
<ul>
<li>Victim Cache: stores blocks that are evicted from L1. (Also reduces Miss rate or Miss penalty)</li>
</ul>
<p>Techniques for reducing miss penalty</p>
<ul>
<li>Early restart: request words in normal orde and send requested word to processor as soon as it arrives, not wait until cache line is filled.</li>
<li>Critical word first: request the exact word from memory and sends requested word to processor as soon as it arrives.</li>
<li>Merging write buffer: CPU only stalls on write when write buffer full. (Write may overtake early write)</li>
</ul>
<p>Cache Miss Types</p>
<ul>
<li>Compulsory(Cold) miss: when a block is accessed for the first time.</li>
<li>Capacity miss: cache was evicted due to capacity.</li>
<li>Conflict miss: cache was evicted due to capacity of set. (Fully associative does not have this miss type)</li>
</ul>
<p>Reducing Cold Miss Rates</p>
<ul>
<li>Large block size</li>
<li>Prefetch: speculate future instr/data accesses and fetch them into cache. Prefetch should not be late or too early.
<ul>
<li>Hardware prefetch: sequential and strided prefetching. Stream buffer(works well for instruction caches) to prevent cache pollution.</li>
<li>Software prefetch: explicit prefetch instructions. Software prefetch with loop unrolling or software pipeline.
<ul>
<li>Restricted to loops with array accesses and it&rsquo;s hard to get right.</li>
</ul>
</li>
</ul>
</li>
<li>High associativity caches</li>
</ul>
<p>Basic Cache Optimization(+ improvement, - worse)</p>
<ul>
<li>Larger block size: + Miss rate, - Miss penalty. Doesn&rsquo;t affect Hit time and power consumption.</li>
<li>Bigger cache: + Miss rate(improves capacity misses), - Hit time, - Power.</li>
<li>High associativity: + Miss rate(improves conflict misses), - Hit time, - Power.</li>
<li>Multilevel caches: + Miss penalty(data might be found in L2 cache).</li>
<li>Give priority to read misses: read misses and there are write misses in the write buffer, read misses are handled first. + Miss penalty.</li>
<li>Avoid virtual to physical address translation lookup: + Hit time.</li>
</ul>
<p>Compiler Optimizations:</p>
<ul>
<li>Instruction reordering: reduce conflict misses</li>
<li>Data reordering: 2 arrays v.s. struct, loop interchange(swap nested loops to access memory in sequential ), loop fusion, blocking(access blocks of data)</li>
</ul>
<h3 id="week-11">Week 11</h3>
<p>Virtual Memory</p>
<ul>
<li>Programs are too large to fit in physical memory. A method to share physical memory for a large amount of processes.</li>
<li>Each process has its own, full, address space.
<ul>
<li>Virtual Memory: pages</li>
<li>Physical Memory: frames</li>
</ul>
</li>
<li>Recently not used pages may be swapped to disk. (Swap disk)</li>
<li>Virtual Memory miss: page fault. Page not in memory, so OS needs to retrieve pages from disk(very slow).</li>
</ul>
<p>Page Table</p>
<ul>
<li>OS maintains a table that maps all virtual pages to physical page frames.</li>
<li>One PT per process(page table register points to the Page table of the current process) and one for the OS.</li>
<li>Memory is fully associative.</li>
<li>OS maintains a list of free frames.</li>
</ul>
<p>Page Table stores info for translating virtual page number to physical page number.</p>
<p>Methods to make Page Tables space-efficient</p>
<ul>
<li>Inverted page table: a hash table that maps physical and virtual pages.</li>
<li>Hierarchical page table: n-level page table. Only the N-th level page table has the value of physical frames.</li>
</ul>
<p>Paging means that every memory access involves 2 memory accesses: 1. get physical address 2. get data from physical address.</p>
<p>What can we do to make paging faster?</p>
<p>Translation Lookaside Buffer</p>
<ul>
<li>A full-associative(Content Addressable Memory, CAM) cache of Page Table entries. Parallel lookup.</li>
<li>Every entry has many bits(Valid, R/W, User/Supervisor, Dirty, Access), a tag, a data(physcial page number).</li>
</ul>
<p>Virtually tagged problems</p>
<ul>
<li>Synonyms(alias problem): different virtual addresses points to the same physical address.
<ul>
<li>Write to copy 1 would not be reflected in copy 2.</li>
</ul>
</li>
<li>Homonyms: same virtual addresses different physical address due to process switching.
<ul>
<li>Possible solution: 1. flush cache on context switch(increase miss rate) 2. add PID to cache tag.</li>
</ul>
</li>
</ul>
<p>Methods to address a cache in a virtual-memory system</p>
<ul>
<li>Physically Indexed, physically tagged: translation first, increasing L1 hit time.</li>
<li>Virtually Indexed, virtually tagged: cannot distinguish synonyms/homonyms in cache.</li>
<li>Virtually Indexed, physically tagged: L1 cache indexed virtual address, tags can be checked after translation.</li>
<li>Physically Indexed, virtually tagged: not practical.</li>
</ul>
<p>Does physically indexed, physically tagged mean TLB and cache have to be accessed sequentially? Not if PageSize &gt; #Sets * BlockSize</p>
<h3 id="week-12">Week 12</h3>
<p>Motivation for multicores</p>
<ul>
<li>Power wall, ILP wall</li>
</ul>
<p>Parallel architecture = computing model + communication model</p>
<ul>
<li>Computing model: organization of cores and how data is processed</li>
<li>Communication model: how cores communication?
<ul>
<li>Shared memory: explicit synchronization(via loads and stores)</li>
<li>Message passing: implicit synchronization(via messages)</li>
</ul>
</li>
</ul>
<p>Multicore processors</p>
<ul>
<li>Uniform memory access (UMA): physically centralized memory -&gt; Symmetric Multiprocessor(SMP)</li>
<li>Non-Uniform memory access (NUMA): physically distributed memory -&gt; Distributed Shared-Memory</li>
</ul>
<p>Communication Model</p>
<ul>
<li>Threads communication is done through shared memory variables</li>
<li>Explicit data synchronization, done by the developers</li>
</ul>
<p>The main goal for Cache Coherence is to make caches invisible.</p>
<p>Single Write Multiple Reader</p>
<ul>
<li>Write Propagation: writes are eventually visible in all processors.</li>
<li>Write Serialization: writes are in the same order in all processors.</li>
</ul>
<p>Cache Coherence Protocol: keep track of what processors have copies of what data.</p>
<ul>
<li>Invalidate protocols: get rid of data with old values, usually used with write-back caches.
<ul>
<li>+: multiple writes to the cache block only require one invalidation.</li>
<li>+: less bandwidth since it doesn&rsquo;t need to send new value of the data.</li>
<li>-: write-back data to memory when evicting a modified block.</li>
</ul>
</li>
<li>Update protocols: update every caches&rsquo; copy of data, usually used with write-through caches.
<ul>
<li>+: new value can be re-used without the need to ask for it again.</li>
<li>+: data can always be read from memory.</li>
<li>-: possible multiple useless updates.</li>
</ul>
</li>
</ul>
<p>How can cache coherence protocols be implemented?</p>
<ul>
<li>Software coherence: programmer or compiler-controlled</li>
<li>Hardware coherence: add state bits to cache lines to track state of the line.
<ul>
<li>Exclusive state: block is cached only in this cache, has not been modified, but can be modified without permission. Freely modify and upgrade modified state.</li>
</ul>
</li>
</ul>
<h3 id="week-13">Week 13</h3>
<p>Cache Coherence Protocol Implementations</p>
<ul>
<li>Snooping: all cache controllers monitor all other caches&rsquo; activites and maintain the state of their lines.
<ul>
<li>Information is shared in a common bus. Bus does not scale well.</li>
<li>Each cache has a bus-side controller that monitors all transactions.</li>
<li>Two types of snooping: 1. write invalidation 2. write update</li>
</ul>
</li>
<li>Directory: a central control device directly handles all cache activies.
<ul>
<li>Directory acts as a serialization to provide ordering.</li>
</ul>
</li>
</ul>
<p>MSI Protocol</p>
<ul>
<li>Invalid: block is not present. Need to fetch it from memory or other cache.</li>
<li>Shared: in &gt; 1 caches.</li>
<li>Modified: in 1 cache. Processor can read/write directly.</li>
</ul>
<p>MESI Protocol has one more Exclusive state.</p>
<p>Coherence misses: when a block is not in the cache because it was invalidated by a write from another processor.</p>
<ul>
<li>Hard to reduce due to communication and sharing of data in parallel application.</li>
<li>False sharing: processor modify different words of the cache block but end up invalidating the complete block.
<ul>
<li>False sharing coherence misses increase with larger cache line size.</li>
</ul>
</li>
</ul>
<p>Problems for snooping on a common shared bus</p>
<ul>
<li>When should memory provide data?</li>
<li>What if we need to Write-back?</li>
<li>Conflict when processor and bus-side controller check the cache</li>
<li>State transitions may require several steps</li>
<li>What to do if there are conflicting requests(race conditions) on the bus?
<ul>
<li>Transient states</li>
</ul>
</li>
</ul>
<p>Problems for snooping with multi-level hierarchies</p>
<ul>
<li>Processor interacts with L1 while bus-side controller interacts with L2.
<ul>
<li>Inclusive cache and M state caches in L1 must also be in L2</li>
<li>Propagate all transactions to L1</li>
</ul>
</li>
</ul>
<h3 id="week-14">Week 14</h3>
<p>Snooping implementation has bottleneck at the common data bus. Thus we introduce snooping with split-transaction buses.</p>
<ul>
<li>Create buffer for each cache to hold pending transactions.</li>
<li>Send <code>negative acknowledgement (NACK)</code> when buffers are full.</li>
<li>Snooping with Ring can enforce write serialization with home node. If there are multiple racing writes, ties are broken via the home node.</li>
</ul>
<p>Directory contains a line state and sharing bit-vector.</p>
<ul>
<li>Line state: invalid(00), shared(01), modified(10)</li>
<li>Sharing vector: not cached(00), shared(01)</li>
</ul>
<p>Directory operation</p>
<ul>
<li>It is necessary to collect all acknowledgements(ACK) with write that has multiple sharers.</li>
<li>Complex state changes, directory must also receive ACK.</li>
</ul>
<p>Implementation difficulties for direcotry operation</p>
<ul>
<li>Operations have to be serialized locally.</li>
<li>Operations have to be serialized at directory.</li>
</ul>
<p>Directory Overhead grows with number of cores.</p>
<ul>
<li>Baseline overhead: (number of cores + 1 dirty bit / cache block size * 8 bits)</li>
<li>Cached Directories</li>
<li>Limited Pointer Directories</li>
</ul>
<p>Distributed Directories</p>
<ul>
<li>Local, Home, Remote nodes.</li>
</ul>
<p>Memory Consistency is a specification, which specifies the order of loads and stores.</p>
<ul>
<li>Memory Consistency model is governed by 1. Core pipeline(memory reorder) 2. Coherence Protocol</li>
</ul>
<p>Sequential Consistency(SC): 1. Result should be the same in a time-shared multiprocessor 2. Relative order should be maintained in one thread</p>
<ul>
<li>Sequential Consistency is what should load really happens.
<ol>
<li>Threads issue memory operations in program order</li>
<li>Before issuing next memory operation threads wait until last issued memory operation completes (i.e., performs w.r.t. all other processors)</li>
<li>A read is allowed to complete only if the matching write (i.e., the one whose value is returned to the read) also completes</li>
</ol>
</li>
</ul>
<p>Issue: memory operation leaves the processor and becomes visible to the memory subsystem.
Performed: memory operation appears to have taken place.</p>
<ul>
<li>Performed with reference to processor X: performed to processor X.</li>
<li>Globally performed or complete: performed to all processors.</li>
</ul>
<p>Merging write buffer executes memory operations in the following sequence:</p>
<ul>
<li>write foo(200)</li>
<li>write A(400)</li>
<li>write flag(204)</li>
<li>write bar(404)</li>
</ul>
<p>foo and flag will be written to memory before A and bar.</p>
<p>Write-serialization: per variable. Write to same location by different processors are seen in same order by all processors.
Write-atomicity: across threads</p>
<p>In-window Speculation:</p>
<ul>
<li>Speculation: read from cache before commiting. If no change, then commit; If there&rsquo;s a change, then squash and replay.</li>
<li>Write-prefetcing: obtain read-exclusive out-of-order or in parallel. However, the write should be in program.</li>
</ul>
<h3 id="week-15">Week 15</h3>
<p>Relaxed Memory Consistency Models:</p>
<ul>
<li>Total Store Ordering (TSO): relaxes W -&gt; R. Not guarantee because there could be a store buffer.</li>
<li>Partial Store Ordering (PSO): relaxes W -&gt; R and W -&gt; W.</li>
<li>Relaxes Memory Ordering (RMO): relaxes all four memory orders.</li>
<li>Release Consistency (RC): relaxes all four memory orders but provides release store and acquire load.
<ul>
<li>Reads and writes are allowed to bypass both reads and writes.</li>
<li>Previous reads and writes must complete before release completes.</li>
<li>No reads and writes can complete before acquire completes.</li>
</ul>
</li>
<li>IBM Power: relaxes all four memory orders and write atomicity. Provides 2 types of barriers.</li>
</ul>
<p>Every relaxed consistency model ensures single thread dependencies.</p>
<p>Release Consistency:</p>
<ul>
<li>Writer-initiated invalidation</li>
<li>Without Writer-initiated invalidation</li>
</ul>
<p>Out-of-thin-air problem</p>
<p>Progress Axiom: a store should be eventually visible for all processors.</p>
<p>Synchronization is necessary to ensure that operations in a parallel program happen in the correct order.</p>
<ul>
<li>Conditional Variable: signal</li>
<li>Mutual exclusion</li>
</ul>
<p>Without memory consistency model, we cannot implement different types of synchronization.</p>
<p>Can Sequential Consistency implement mutually exclusion?</p>
<ul>
<li>Yes. Peterson algorithm, but it is not practical for multiple processors.</li>
<li>For Relaxes models need to use fences.</li>
</ul>
<p>Building blocks for synchronization. Special instructions(RMW atomic) of the hardware to implement locks.</p>
<ul>
<li>Test &amp; set: reads a memory location and sets it to value 1.</li>
<li>Compare &amp; swap: it is used more frequently. Check the value and swap if the value is equal.</li>
<li>Fetch &amp; add: fetch value from memory location and atomically increment it.</li>
<li>Load Link(or Load Locked)/Store Conditional(LL/SC): don&rsquo;t need to retain exclusive state.</li>
</ul>
<h3 id="week-16">Week 16</h3>
<p>Implementation of Read Modified Write(RMW) instructions:</p>
<ul>
<li>Lock the bus: disallows other threads</li>
<li>Cache line blocking: to obtain read-exclusive state, invalidates other processors&rsquo; cache. Once a processor gains exclusive state,
other processors will receive NACK from the processor that has exclusive access.</li>
</ul>
<p>Exclusive Access</p>
<ul>
<li>Directory retries when receives NACK</li>
<li>With Snooping, processors retires when receives NACK</li>
</ul>
<p>RMW acts like a memory fence(flush write buffer before RMW).</p>
<p>Techniques for reducing test and set traffic:</p>
<ul>
<li>Test and test-&amp;-set relies on cache coherece. It grabs a lock one time.</li>
<li>Test and set with exponential back-off, retry test and set after pause.</li>
</ul>

  </content>
  
  

    </main>
    <footer>
      
  <span>© 2024 Lee Wei</span>


  <span>
    |
    Made with
    <a href="https://github.com/maolonglong/hugo-simple/">Hugo ʕ•ᴥ•ʔ Simple</a>
  </span>


    </footer>

    
</body>
</html>
