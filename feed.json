{
  "version": "https://jsonfeed.org/version/1",
  "title": "Sir",
  "home_page_url": "https://leewei.co",
  "feed_url": "https://leewei.cofeed.json",
  "description": "A minimal hugo theme focus on content",
  "favicon": "https://leewei.co/assets/favicon.ico",
  "expired": false,
  "author": {
    "name": "Calvin Tran",
    "url": "https://leewei.co"
  },
  "items": [
    
    

    
    {
      "id": "5b8059958504cf9f893f5c35c03201563c141945",
      "title": "Deep Learning",
      "summary": "",
      "content_text": "Day 1 What is Neural Network?\nNeuron: holds a number between 0 and 1. The number is also called \u0026ldquo;Activation\u0026rdquo;. Network: multiple layers of neurons connected together. A layer is a series of neurons. Each layer\u0026rsquo;s output affect the neurons of the next layer.\nWhy the layers?\nEach layer construct some parts of information for the next layer until the output layer. Each neuron in a layer has a weight that connects to the next layer\u0026rsquo;s neuron. This weight can be seen as a parameter that help detect a certain pattern of our input.\nA weighted sum could be any number, but we want this sum value to fit between 0 and 1. We can utilize a Sigmoid function(old school) or Rectified linear unit.\nBias for inactivity, which can be subtracted to the weighted sum before inputing to Sigmoid or ReLU.\nLearning: finding the right weights and biases.\n",
      "content_html": "\u003ch3 id=\"day-1\"\u003eDay 1\u003c/h3\u003e\n\u003cp\u003eWhat is Neural Network?\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eNeuron: holds a number between 0 and 1. The number is also called \u0026ldquo;Activation\u0026rdquo;.\u003c/li\u003e\n\u003cli\u003eNetwork: multiple layers of neurons connected together.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eA layer is a series of neurons. Each layer\u0026rsquo;s output affect the neurons of the next layer.\u003c/p\u003e\n\u003cp\u003eWhy the layers?\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eEach layer construct some parts of information for the next layer until the output layer.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eEach neuron in a layer has a weight that connects to the next layer\u0026rsquo;s neuron.\nThis weight can be seen as a parameter that help detect a certain pattern of our input.\u003c/p\u003e\n\u003cp\u003eA weighted sum could be any number, but we want this sum value to fit between 0 and 1.\nWe can utilize a Sigmoid function(old school) or Rectified linear unit.\u003c/p\u003e\n\u003cp\u003eBias for inactivity, which can be subtracted to the weighted sum before inputing to Sigmoid or ReLU.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eLearning: finding the right weights and biases.\u003c/strong\u003e\u003c/p\u003e\n",
      "url": "https://leewei.co/posts/deep-learning/",
      "date_published": "21096-21-09T920:2121:00-06:00",
      "date_modified": "21096-21-09T920:2121:00-06:00",
      "author": {
        "name": "Calvin Tran",
        "url": "https://leewei.co"
      }
    }
    
  ]
}