<!doctype html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <link rel="shortcut icon" href="/images/favicon.png" />

<title>Fall 2025: CS 6300 Artificial Intelligence&nbsp;|&nbsp; :)</title>
<meta
  name="title"
  content="Fall 2025: CS 6300 Artificial Intelligence"
/>
<meta
  name="description"
  content="Week 1 What is Artificial Intelligence? The science of making machines that act rationally. Rational: An agent acts rationally if it strives to maximally achieve its pre-defined goals. The focus is on the decisions made, not the thought process. Being rational means maximizing your expected utility. Agent is an entity that perceives its environment and acts upon it. Reflex agents: Choose actions based solely on the current percept. Planning agents: Make decisions by considering the future consequences of their actions."
/>
<meta
  name="keywords"
  content=""
/>

  <meta name="author" content="Lee Wei" />




<meta property="og:title" content="Fall 2025: CS 6300 Artificial Intelligence" />
<meta property="og:description" content="Week 1 What is Artificial Intelligence? The science of making machines that act rationally. Rational: An agent acts rationally if it strives to maximally achieve its pre-defined goals. The focus is on the decisions made, not the thought process. Being rational means maximizing your expected utility. Agent is an entity that perceives its environment and acts upon it. Reflex agents: Choose actions based solely on the current percept. Planning agents: Make decisions by considering the future consequences of their actions." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://leewei.co/cs6300/" /><meta property="article:section" content="blog" />
<meta property="article:published_time" content="2025-09-10T21:22:31-06:00" />
<meta property="article:modified_time" content="2025-09-10T21:22:31-06:00" />




<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Fall 2025: CS 6300 Artificial Intelligence"/>
<meta name="twitter:description" content="Week 1 What is Artificial Intelligence? The science of making machines that act rationally. Rational: An agent acts rationally if it strives to maximally achieve its pre-defined goals. The focus is on the decisions made, not the thought process. Being rational means maximizing your expected utility. Agent is an entity that perceives its environment and acts upon it. Reflex agents: Choose actions based solely on the current percept. Planning agents: Make decisions by considering the future consequences of their actions."/>




<meta itemprop="name" content="Fall 2025: CS 6300 Artificial Intelligence">
<meta itemprop="description" content="Week 1 What is Artificial Intelligence? The science of making machines that act rationally. Rational: An agent acts rationally if it strives to maximally achieve its pre-defined goals. The focus is on the decisions made, not the thought process. Being rational means maximizing your expected utility. Agent is an entity that perceives its environment and acts upon it. Reflex agents: Choose actions based solely on the current percept. Planning agents: Make decisions by considering the future consequences of their actions."><meta itemprop="datePublished" content="2025-09-10T21:22:31-06:00" />
<meta itemprop="dateModified" content="2025-09-10T21:22:31-06:00" />
<meta itemprop="wordCount" content="2593">
<meta itemprop="keywords" content="" />
<meta name="referrer" content="no-referrer-when-downgrade" />

    
    
    
    <link href="/bundle.min.css" rel="stylesheet" />

    

    


    
</head>

  <body>
    <header>
      <nav>
  <a
    href="/"
    
    >Home</a
  >

  <a
    href="/blog/"
    
    >Blog</a
  >


  <a href="/index.xml">
    <svg xmlns="http://www.w3.org/2000/svg" class="icon" viewBox="0 0 448 512">
      
      <path
        d="M0 64C0 46.3 14.3 32 32 32c229.8 0 416 186.2 416 416c0 17.7-14.3 32-32 32s-32-14.3-32-32C384 253.6 226.4 96 32 96C14.3 96 0 81.7 0 64zM0 416a64 64 0 1 1 128 0A64 64 0 1 1 0 416zM32 160c159.1 0 288 128.9 288 288c0 17.7-14.3 32-32 32s-32-14.3-32-32c0-123.7-100.3-224-224-224c-17.7 0-32-14.3-32-32s14.3-32 32-32z"
      />
    </svg>
    RSS
  </a>

</nav>

<h1>Fall 2025: CS 6300 Artificial Intelligence</h1>


    </header>
    <main>
      
  
    
      <p>
        <i>
          <time
            style="color: var(--text-light);"
            datetime="2025-09-10"
            pubdate
          >
            2025-09-10
          </time>
        </i>
      </p>
    
  
  
  <content>
    <h3 id="week-1">Week 1</h3>
<ul>
<li>What is Artificial Intelligence? The science of making machines that <strong>act rationally</strong>.</li>
<li><strong>Rational</strong>: An agent acts rationally if it strives to maximally achieve its pre-defined goals. The focus is on the decisions made, not the thought process.</li>
<li>Being rational means <strong>maximizing your expected utility.</strong></li>
<li><strong>Agent</strong> is an entity that perceives its environment and acts upon it.
<ul>
<li><strong>Reflex agents</strong>: Choose actions based solely on the current percept.</li>
<li><strong>Planning agents</strong>: Make decisions by considering the future consequences of their actions.</li>
</ul>
</li>
<li>Optimal planning: A search algorithm is optimal if it is guaranteed to find a least-cost solution.</li>
<li>Complete planning: A search algorithm is complete if it is guaranteed to find a solution if one exists.</li>
<li>A <strong>Search Problem</strong> consists of:
<ul>
<li>A state space: The set of all possible states the world can be in.</li>
<li>A successor function (with actions and costs).</li>
<li>A start state and a goal state.</li>
</ul>
</li>
<li>A solution is a sequence of actions which leads from the start state to the goal state.</li>
<li>State Space Graph: A mathematical representation of a search problem where each node is a unique world state.</li>
<li>Search Tree: A tree of plans and their outcomes. A path from the root to a leaf represents a plan.</li>
<li><strong>General Tree search</strong>: An algorithm that expands potential plans and maintains a fringe of partial plans under consideration.
<ul>
<li>Fringe: A data structure to store partial plans.</li>
<li>Expansion: Generating successor nodes from a given node.</li>
<li>Exploration strategy: The order in which nodes are expanded.</li>
</ul>
</li>
<li><strong>Depth-First Search</strong>: A search strategy that expands the deepest node first.
<ul>
<li>Fringe is a LIFO stack.</li>
<li>Time complexity: O(b^m), where &lsquo;b&rsquo; is the branching factor and &rsquo;m&rsquo; is the maximum depth.</li>
<li>Space complexity: O(b*m), as it only needs to store the current path and its siblings.</li>
<li>Completeness: Not complete if the search space is infinite. Can be made complete by setting a depth limit.</li>
<li>Optimality: Not optimal, as it finds the &ldquo;leftmost&rdquo; solution without considering the cost.</li>
</ul>
</li>
</ul>
<h3 id="week-2">Week 2</h3>
<ul>
<li><strong>Breadth-First Search (BFS)</strong>: A search strategy that expands the shallowest unexpanded node first.
<ul>
<li>Fringe is a FIFO queue.</li>
<li>Time complexity: O(b^s), where &lsquo;b&rsquo; is the branching factor and &rsquo;s&rsquo; is the depth of the shallowest solution.</li>
<li>Space complexity: O(b^s).</li>
<li>Completeness: Yes.</li>
<li>Optimality: Yes, but only if all actions have the same cost (e.g., cost = 1).</li>
</ul>
</li>
<li><strong>Iterative Deepening Search (IDS)</strong>: Combines the space advantage of DFS with the optimality of BFS. It performs a series of depth-limited DFS searches, progressively increasing the depth limit.
<ul>
<li>Most of the work is done at the lowest level of the search.</li>
</ul>
</li>
<li><strong>Uniform Cost Search (UCS)</strong>: A search strategy that expands the node with the lowest cumulative cost first.
<ul>
<li>Fringe is a Priority Queue, where the priority is the cumulative cost (g(n)).</li>
<li>Time complexity: O(b^(C*/e)), where C* is the cost of the optimal solution and e is the minimum cost of an action.</li>
<li>Space complexity: O(b^(C*/e)).</li>
<li>Completeness: Yes, if edge costs are positive.</li>
<li>Optimality: Yes.</li>
</ul>
</li>
<li><strong>Drawback of UCS</strong>: It can explore in every direction, which can be inefficient if the search space is large.</li>
<li><strong>Graph Search vs. Tree Search</strong>:
<ul>
<li><strong>Tree Search</strong>: May explore the same state multiple times if it can be reached through different paths.</li>
<li><strong>Graph Search</strong>: Avoids redundant work by keeping track of visited states in a &ldquo;closed set&rdquo;. It only expands nodes that have not been explored before.</li>
</ul>
</li>
<li><strong>Informed Search</strong>: A search strategy that uses problem-specific knowledge in the form of a heuristic function to guide the search.</li>
<li><strong>Heuristic Function (h(n))</strong>: A function that estimates the cost of the cheapest path from a state &rsquo;n&rsquo; to a goal state.</li>
<li><strong>Greedy Best-First Search</strong>: Expands the node that appears to be closest to the goal, based on the heuristic function.
<ul>
<li>It can be misled by inaccurate heuristics and may not find the optimal solution.</li>
</ul>
</li>
<li><em><em>A</em> Search</em>*: A best-first search algorithm that combines the advantages of UCS (backward cost) and Greedy Search (forward cost).
<ul>
<li>Evaluation function: f(n) = g(n) + h(n), where g(n) is the cumulative cost from the start node to &rsquo;n&rsquo;, and h(n) is the estimated cost from &rsquo;n&rsquo; to the goal.</li>
<li>The algorithm terminates when a goal state is dequeued from the fringe.</li>
</ul>
</li>
<li><strong>Admissible Heuristics</strong>: A heuristic &lsquo;h&rsquo; is admissible (or optimistic) if it never overestimates the true cost to the nearest goal. That is, 0 &lt;= h(n) &lt;= h*(n), where h*(n) is the true cost.</li>
<li><strong>Consistent Heuristics</strong>: A heuristic &lsquo;h&rsquo; is consistent if the estimated cost from a node &rsquo;n&rsquo; to the goal is less than or equal to the actual cost of moving to a successor &rsquo;n&rsquo; and then the estimated cost from &rsquo;n&rsquo; to the goal.
<ul>
<li>Consistency implies admissibility.</li>
</ul>
</li>
<li><strong>Optimality of A</strong>*:
<ul>
<li><strong>Tree Search</strong>: A* is optimal if the heuristic is admissible.</li>
<li><strong>Graph Search</strong>: A* is optimal if the heuristic is consistent.</li>
<li>UCS is a special case of A* where h(n) = 0.</li>
</ul>
</li>
</ul>
<h3 id="week-3">Week 3</h3>
<ul>
<li><strong>Adversarial Search</strong>: Used for decision-making in competitive environments where agents have conflicting goals.</li>
<li><strong>Value of a state</strong>: The utility of a state, representing the best possible outcome achievable from that state.</li>
<li><strong>Minimax Algorithm</strong>: For deterministic, zero-sum games (e.g., Tic-Tac-Toe, Chess). One player (MAX) aims to maximize the score, while the other (MIN) aims to minimize it.
<ul>
<li>It&rsquo;s a recursive algorithm that performs a complete depth-first search of the game tree.</li>
<li><strong>Time Complexity</strong>: O(b^m), where &lsquo;b&rsquo; is the branching factor and &rsquo;m&rsquo; is the maximum depth of the tree.</li>
<li><strong>Space Complexity</strong>: O(bm), for a single path in the tree.</li>
</ul>
</li>
</ul>






<div class="highlight"><pre tabindex="0" style="color:#cdd6f4;background-color:#1e1e2e;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f849c"> 1</span><span><span style="color:#6c7086;font-style:italic"># Minimax Implementation</span>
</span></span><span style="display:flex;"><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f849c"> 2</span><span><span style="color:#cba6f7">def</span> <span style="color:#89b4fa">minimax</span>(state, depth, maximizing_player):
</span></span><span style="display:flex;"><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f849c"> 3</span><span>    <span style="color:#cba6f7">if</span> depth <span style="color:#89dceb;font-weight:bold">==</span> <span style="color:#fab387">0</span> <span style="color:#89dceb;font-weight:bold">or</span> is_terminal(state):
</span></span><span style="display:flex;"><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f849c"> 4</span><span>        <span style="color:#cba6f7">return</span> evaluate(state)
</span></span><span style="display:flex;"><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f849c"> 5</span><span>
</span></span><span style="display:flex;"><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f849c"> 6</span><span>    <span style="color:#cba6f7">if</span> maximizing_player:
</span></span><span style="display:flex;"><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f849c"> 7</span><span>        max_eval <span style="color:#89dceb;font-weight:bold">=</span> <span style="color:#89dceb;font-weight:bold">-</span>infinity
</span></span><span style="display:flex;"><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f849c"> 8</span><span>        <span style="color:#cba6f7">for</span> child <span style="color:#89dceb;font-weight:bold">in</span> get_children(state):
</span></span><span style="display:flex;"><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f849c"> 9</span><span>            <span style="color:#89dceb">eval</span> <span style="color:#89dceb;font-weight:bold">=</span> minimax(child, depth <span style="color:#89dceb;font-weight:bold">-</span> <span style="color:#fab387">1</span>, <span style="color:#fab387">False</span>)
</span></span><span style="display:flex;"><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f849c">10</span><span>            max_eval <span style="color:#89dceb;font-weight:bold">=</span> <span style="color:#89dceb">max</span>(max_eval, <span style="color:#89dceb">eval</span>)
</span></span><span style="display:flex;"><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f849c">11</span><span>        <span style="color:#cba6f7">return</span> max_eval
</span></span><span style="display:flex;"><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f849c">12</span><span>    <span style="color:#cba6f7">else</span>:
</span></span><span style="display:flex;"><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f849c">13</span><span>        min_eval <span style="color:#89dceb;font-weight:bold">=</span> <span style="color:#89dceb;font-weight:bold">+</span>infinity
</span></span><span style="display:flex;"><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f849c">14</span><span>        <span style="color:#cba6f7">for</span> child <span style="color:#89dceb;font-weight:bold">in</span> get_children(state):
</span></span><span style="display:flex;"><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f849c">15</span><span>            <span style="color:#89dceb">eval</span> <span style="color:#89dceb;font-weight:bold">=</span> minimax(child, depth <span style="color:#89dceb;font-weight:bold">-</span> <span style="color:#fab387">1</span>, <span style="color:#fab387">True</span>)
</span></span><span style="display:flex;"><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f849c">16</span><span>            min_eval <span style="color:#89dceb;font-weight:bold">=</span> <span style="color:#89dceb">min</span>(min_eval, <span style="color:#89dceb">eval</span>)
</span></span><span style="display:flex;"><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f849c">17</span><span>        <span style="color:#cba6f7">return</span> min_eval</span></span></code></pre></div>
<ul>
<li><strong>Resource Limits</strong>: In practice, searching the entire game tree is often infeasible due to its size.
<ul>
<li><strong>Solution</strong>: Use a depth-limited search and an evaluation function.</li>
</ul>
</li>
<li><strong>Evaluation Function</strong>: An evaluation function estimates the expected utility of a game state without searching to the end of the game.</li>
<li><strong>Game Tree Pruning</strong>: A technique to reduce the number of nodes evaluated in the search tree.
<ul>
<li><strong>Alpha-Beta Pruning</strong>: An optimization of the Minimax algorithm that prunes away branches of the search tree that cannot influence the final decision.
<ul>
<li><strong>Alpha</strong>: The best value (highest) found so far for the MAX player on the path from the root to the current node.</li>
<li><strong>Beta</strong>: The best value (lowest) found so far for the MIN player on the path from the root to the current node.</li>
<li><strong>Pruning Condition</strong>: Pruning occurs if <code>alpha &gt;= beta</code>.</li>
<li><strong>Time Complexity</strong>: In the best case, with perfect ordering of moves, the time complexity is O(b^(m/2)), effectively doubling the searchable depth. In the worst case, it&rsquo;s the same as Minimax, O(b^m).</li>
</ul>
</li>
</ul>
</li>
</ul>






<div class="highlight"><pre tabindex="0" style="color:#cdd6f4;background-color:#1e1e2e;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f849c"> 1</span><span><span style="color:#6c7086;font-style:italic"># Alpha-Beta Pruning Implementation</span>
</span></span><span style="display:flex;"><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f849c"> 2</span><span><span style="color:#cba6f7">def</span> <span style="color:#89b4fa">alpha_beta_pruning</span>(state, depth, alpha, beta, maximizing_player):
</span></span><span style="display:flex;"><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f849c"> 3</span><span>    <span style="color:#cba6f7">if</span> depth <span style="color:#89dceb;font-weight:bold">==</span> <span style="color:#fab387">0</span> <span style="color:#89dceb;font-weight:bold">or</span> is_terminal(state):
</span></span><span style="display:flex;"><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f849c"> 4</span><span>        <span style="color:#cba6f7">return</span> evaluate(state)
</span></span><span style="display:flex;"><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f849c"> 5</span><span>
</span></span><span style="display:flex;"><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f849c"> 6</span><span>    <span style="color:#cba6f7">if</span> maximizing_player:
</span></span><span style="display:flex;"><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f849c"> 7</span><span>        max_eval <span style="color:#89dceb;font-weight:bold">=</span> <span style="color:#89dceb;font-weight:bold">-</span>infinity
</span></span><span style="display:flex;"><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f849c"> 8</span><span>        <span style="color:#cba6f7">for</span> child <span style="color:#89dceb;font-weight:bold">in</span> get_children(state):
</span></span><span style="display:flex;"><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f849c"> 9</span><span>            <span style="color:#89dceb">eval</span> <span style="color:#89dceb;font-weight:bold">=</span> alpha_beta_pruning(child, depth <span style="color:#89dceb;font-weight:bold">-</span> <span style="color:#fab387">1</span>, alpha, beta, <span style="color:#fab387">False</span>)
</span></span><span style="display:flex;"><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f849c">10</span><span>            max_eval <span style="color:#89dceb;font-weight:bold">=</span> <span style="color:#89dceb">max</span>(max_eval, <span style="color:#89dceb">eval</span>)
</span></span><span style="display:flex;"><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f849c">11</span><span>            alpha <span style="color:#89dceb;font-weight:bold">=</span> <span style="color:#89dceb">max</span>(alpha, <span style="color:#89dceb">eval</span>)
</span></span><span style="display:flex;"><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f849c">12</span><span>            <span style="color:#cba6f7">if</span> beta <span style="color:#89dceb;font-weight:bold">&lt;=</span> alpha:
</span></span><span style="display:flex;"><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f849c">13</span><span>                <span style="color:#cba6f7">break</span>  <span style="color:#6c7086;font-style:italic"># Beta cut-off</span>
</span></span><span style="display:flex;"><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f849c">14</span><span>        <span style="color:#cba6f7">return</span> max_eval
</span></span><span style="display:flex;"><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f849c">15</span><span>    <span style="color:#cba6f7">else</span>:
</span></span><span style="display:flex;"><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f849c">16</span><span>        min_eval <span style="color:#89dceb;font-weight:bold">=</span> <span style="color:#89dceb;font-weight:bold">+</span>infinity
</span></span><span style="display:flex;"><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f849c">17</span><span>        <span style="color:#cba6f7">for</span> child <span style="color:#89dceb;font-weight:bold">in</span> get_children(state):
</span></span><span style="display:flex;"><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f849c">18</span><span>            <span style="color:#89dceb">eval</span> <span style="color:#89dceb;font-weight:bold">=</span> alpha_beta_pruning(child, depth <span style="color:#89dceb;font-weight:bold">-</span> <span style="color:#fab387">1</span>, alpha, beta, <span style="color:#fab387">True</span>)
</span></span><span style="display:flex;"><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f849c">19</span><span>            min_eval <span style="color:#89dceb;font-weight:bold">=</span> <span style="color:#89dceb">min</span>(min_eval, <span style="color:#89dceb">eval</span>)
</span></span><span style="display:flex;"><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f849c">20</span><span>            beta <span style="color:#89dceb;font-weight:bold">=</span> <span style="color:#89dceb">min</span>(beta, <span style="color:#89dceb">eval</span>)
</span></span><span style="display:flex;"><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f849c">21</span><span>            <span style="color:#cba6f7">if</span> beta <span style="color:#89dceb;font-weight:bold">&lt;=</span> alpha:
</span></span><span style="display:flex;"><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f849c">22</span><span>                <span style="color:#cba6f7">break</span>  <span style="color:#6c7086;font-style:italic"># Alpha cut-off</span>
</span></span><span style="display:flex;"><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f849c">23</span><span>        <span style="color:#cba6f7">return</span> min_eval</span></span></code></pre></div>
<ul>
<li><strong>Expectimax Search</strong>: A variation of Minimax for games with an element of chance (e.g., games with dice rolls).
<ul>
<li>It includes &ldquo;chance nodes&rdquo; in addition to MAX and MIN nodes.</li>
<li>Chance nodes calculate the expected value of the state, which is the average of the values of its children, weighted by their probabilities.</li>
<li>Expectimax Pruning: we cannot prune unless we have bounds on the values of the leaves.</li>
</ul>
</li>
</ul>






<div class="highlight"><pre tabindex="0" style="color:#cdd6f4;background-color:#1e1e2e;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f849c"> 1</span><span><span style="color:#6c7086;font-style:italic"># Expectimax Implementation</span>
</span></span><span style="display:flex;"><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f849c"> 2</span><span><span style="color:#cba6f7">def</span> <span style="color:#89b4fa">expectimax</span>(state, depth, agent):
</span></span><span style="display:flex;"><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f849c"> 3</span><span>    <span style="color:#cba6f7">if</span> depth <span style="color:#89dceb;font-weight:bold">==</span> <span style="color:#fab387">0</span> <span style="color:#89dceb;font-weight:bold">or</span> is_terminal(state):
</span></span><span style="display:flex;"><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f849c"> 4</span><span>        <span style="color:#cba6f7">return</span> evaluate(state)
</span></span><span style="display:flex;"><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f849c"> 5</span><span>
</span></span><span style="display:flex;"><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f849c"> 6</span><span>    <span style="color:#cba6f7">if</span> agent <span style="color:#89dceb;font-weight:bold">==</span> MAX:
</span></span><span style="display:flex;"><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f849c"> 7</span><span>        max_eval <span style="color:#89dceb;font-weight:bold">=</span> <span style="color:#89dceb;font-weight:bold">-</span>infinity
</span></span><span style="display:flex;"><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f849c"> 8</span><span>        <span style="color:#cba6f7">for</span> child <span style="color:#89dceb;font-weight:bold">in</span> get_children(state):
</span></span><span style="display:flex;"><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f849c"> 9</span><span>            <span style="color:#89dceb">eval</span> <span style="color:#89dceb;font-weight:bold">=</span> expectimax(child, depth <span style="color:#89dceb;font-weight:bold">-</span> <span style="color:#fab387">1</span>, MIN)
</span></span><span style="display:flex;"><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f849c">10</span><span>            max_eval <span style="color:#89dceb;font-weight:bold">=</span> <span style="color:#89dceb">max</span>(max_eval, <span style="color:#89dceb">eval</span>)
</span></span><span style="display:flex;"><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f849c">11</span><span>        <span style="color:#cba6f7">return</span> max_eval
</span></span><span style="display:flex;"><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f849c">12</span><span>    <span style="color:#cba6f7">elif</span> agent <span style="color:#89dceb;font-weight:bold">==</span> MIN:
</span></span><span style="display:flex;"><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f849c">13</span><span>        min_eval <span style="color:#89dceb;font-weight:bold">=</span> <span style="color:#89dceb;font-weight:bold">+</span>infinity
</span></span><span style="display:flex;"><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f849c">14</span><span>        <span style="color:#cba6f7">for</span> child <span style="color:#89dceb;font-weight:bold">in</span> get_children(state):
</span></span><span style="display:flex;"><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f849c">15</span><span>            <span style="color:#89dceb">eval</span> <span style="color:#89dceb;font-weight:bold">=</span> expectimax(child, depth <span style="color:#89dceb;font-weight:bold">-</span> <span style="color:#fab387">1</span>, CHANCE)
</span></span><span style="display:flex;"><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f849c">16</span><span>            min_eval <span style="color:#89dceb;font-weight:bold">=</span> <span style="color:#89dceb">min</span>(min_eval, <span style="color:#89dceb">eval</span>)
</span></span><span style="display:flex;"><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f849c">17</span><span>        <span style="color:#cba6f7">return</span> min_eval
</span></span><span style="display:flex;"><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f849c">18</span><span>    <span style="color:#cba6f7">else</span>: <span style="color:#6c7086;font-style:italic"># CHANCE node</span>
</span></span><span style="display:flex;"><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f849c">19</span><span>        expected_value <span style="color:#89dceb;font-weight:bold">=</span> <span style="color:#fab387">0</span>
</span></span><span style="display:flex;"><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f849c">20</span><span>        children <span style="color:#89dceb;font-weight:bold">=</span> get_children(state)
</span></span><span style="display:flex;"><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f849c">21</span><span>        num_children <span style="color:#89dceb;font-weight:bold">=</span> <span style="color:#89dceb">len</span>(children)
</span></span><span style="display:flex;"><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f849c">22</span><span>        <span style="color:#cba6f7">for</span> child <span style="color:#89dceb;font-weight:bold">in</span> children:
</span></span><span style="display:flex;"><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f849c">23</span><span>            <span style="color:#6c7086;font-style:italic"># Assuming equal probability for each outcome</span>
</span></span><span style="display:flex;"><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f849c">24</span><span>            prob <span style="color:#89dceb;font-weight:bold">=</span> <span style="color:#fab387">1.0</span> <span style="color:#89dceb;font-weight:bold">/</span> num_children
</span></span><span style="display:flex;"><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f849c">25</span><span>            expected_value <span style="color:#89dceb;font-weight:bold">+=</span> prob <span style="color:#89dceb;font-weight:bold">*</span> expectimax(child, depth <span style="color:#89dceb;font-weight:bold">-</span> <span style="color:#fab387">1</span>, MAX)
</span></span><span style="display:flex;"><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f849c">26</span><span>        <span style="color:#cba6f7">return</span> expected_value</span></span></code></pre></div>
<h4 id="probability">Probability</h4>
<ul>
<li><strong>Random Variables</strong>: A random variable represents an event whose outcome is unknown. It can take on a range of values, each with an associated probability.</li>
<li><strong>Probability Distribution</strong>: A function that describes the likelihood of all possible outcomes for a random variable. The sum of all probabilities in a distribution must equal 1.</li>
<li><strong>Joint Distributions</strong>: A joint probability distribution shows the probability of two or more random variables all happening at the same time.
<ul>
<li>For &rsquo;n&rsquo; variables, each with a domain size of &rsquo;d&rsquo;, the size of the joint distribution is d^n.</li>
</ul>
</li>
<li><strong>Probabilistic Model</strong>: A probabilistic model is a joint distribution over a set of random variables, which provides a complete description of the domain.</li>
<li><strong>Events</strong>: An event is a set of outcomes. In probability, we often care about partial assignments of variables.</li>
<li><strong>Marginal Distributions</strong>: A marginal distribution is a sub-table of a joint distribution where some variables have been eliminated.
<ul>
<li><strong>Marginalization</strong>: The process of calculating a marginal distribution by summing over the probabilities of the eliminated variables.</li>
</ul>
</li>
<li><strong>Conditional Probabilities</strong>: The probability of an event &lsquo;a&rsquo; occurring, given that another event &lsquo;b&rsquo; has already occurred. It is defined by the formula: <code>P(a|b) = P(a, b) / P(b)</code>.</li>
<li><strong>Probabilistic Inference</strong>: The process of computing a desired probability from a set of known probabilities.
<ul>
<li><strong>The Product Rule</strong>: A fundamental rule relating joint and conditional probabilities: <code>P(x, y) = P(x|y) * P(y)</code>.</li>
<li><strong>The Chain Rule</strong>: A generalization of the product rule that allows for the calculation of the joint probability of any number of random variables: <code>P(x1, x2, ..., xn) = P(x1) * P(x2|x1) * ... * P(xn|x1, ..., xn-1)</code>.</li>
<li><strong>Bayes&rsquo; Rule</strong>: A rule that describes the probability of an event, based on prior knowledge of conditions that might be related to the event: <code>P(x|y) = (P(y|x) * P(x)) / P(y)</code>.</li>
</ul>
</li>
<li><strong>Independence</strong>: Two random variables, X and Y, are independent if the occurrence of one does not affect the probability of the other. This is expressed as: <code>P(X, Y) = P(X) * P(Y)</code>.
<ul>
<li><strong>Conditional Independence</strong>: Two variables, X and Y, are conditionally independent given a third variable, Z, if <code>P(X, Y | Z) = P(X | Z) * P(Y | Z)</code> for all values of x, y, and z.</li>
</ul>
</li>
<li><strong>Normalization</strong>: The process of ensuring that a probability distribution sums to 1. This is often done by dividing each value by the sum of all values.</li>
<li><strong>Multi-Agent Utilities</strong>: In multi-agent systems, each agent has its own utility function that it aims to maximize. The overall utility of a state is a combination of the utilities of all agents.</li>
<li><strong>Maximum Expected Utility (MEU)</strong>: The principle of choosing an action that maximizes an agent&rsquo;s expected utility. The expected utility of an action is the sum of the utilities of all possible outcomes, weighted by their probabilities.</li>
</ul>
<h3 id="week-4">Week 4</h3>
<ul>
<li><strong>Markov Decision Processes (MDPs)</strong>: A framework for modeling decision-making in stochastic (non-deterministic) environments. MDPs are useful for problems where the outcomes of actions are uncertain. One way to solve MDPs is through a variation of expectimax search.
<ul>
<li><strong>Components of an MDP</strong>:
<ul>
<li>A set of states (S).</li>
<li>A set of actions (A).</li>
<li>A <strong>transition function</strong>, T(s, a, s&rsquo;), which gives the probability of reaching state s&rsquo; from state s after taking action a.</li>
<li>A <strong>reward function</strong>, R(s, a, s&rsquo;), which gives the reward received after transitioning from state s to s&rsquo; by taking action a.</li>
<li>A start state.</li>
<li>An optional terminal state.</li>
</ul>
</li>
</ul>
</li>
<li><strong>The Markov Property</strong>: The core assumption in an MDP is that the future is independent of the past, given the present. This means that the transition to the next state depends only on the current state and the chosen action, not on the sequence of states that led to the current state.</li>
<li><strong>Policies</strong>: In an MDP, the goal is to find an optimal <strong>policy</strong>, denoted as π*(s). A policy is a mapping from states to actions, which tells the agent what action to take in each state to maximize its expected utility.</li>
<li><strong>Discounting</strong>: A technique used to value immediate rewards more highly than future rewards. A discount factor, γ (gamma), between 0 and 1, is used to decay the value of future rewards exponentially.
<ul>
<li>Discounting also helps to ensure that the algorithms for solving MDPs converge, especially in infinite-horizon problems.</li>
</ul>
</li>
<li><strong>Dealing with Infinite Horizons</strong>: What if the game or process can last forever?
<ul>
<li><strong>Solution 1: Finite Horizon</strong>: Assume the process ends after a fixed number of steps.</li>
<li><strong>Solution 2: Discounting</strong>: Use a discount factor (0 &lt; γ &lt; 1) to make distant rewards less valuable.</li>
<li><strong>Solution 3: Absorbing States</strong>: Guarantee that a terminal state will eventually be reached, which ends the process.</li>
</ul>
</li>
<li><strong>Bellman Equations</strong>: A set of equations that describe the relationship between the value of a state and the values of its successor states. The Bellman equation for the optimal value function, V*(s), is:
<ul>
<li><code>V*(s) = max_a Σ_{s'} T(s, a, s') [R(s, a, s') + γV*(s')]</code></li>
<li>This equation states that the optimal value of a state is the maximum expected reward achievable from that state, which is calculated by considering all possible actions and their outcomes.</li>
</ul>
</li>
<li><strong>Value Iteration</strong>: A dynamic programming algorithm for finding the optimal value function, V*(s), for an MDP. It starts with an arbitrary value function, V0(s) = 0 for all states, and iteratively updates the values using the Bellman equation until they converge.
<ul>
<li><strong>Algorithm</strong>:
<ol>
<li>Initialize V0(s) = 0 for all s.</li>
<li>For k = 1, 2, &hellip; until convergence:
<ul>
<li>For each state s:
<ul>
<li><code>Vk(s) = max_a Σ_{s'} T(s, a, s') [R(s, a, s') + γV_{k-1}(s')]</code></li>
</ul>
</li>
</ul>
</li>
</ol>
</li>
<li><strong>Time Complexity</strong>: O(|S|^2 * |A|) per iteration, where |S| is the number of states and |A| is the number of actions.</li>
<li><strong>Convergence</strong>: The value iteration algorithm is guaranteed to converge to the optimal value function because the Bellman operator is a contraction mapping. With each iteration, the error between the current value function and the optimal one is reduced by a factor of γ.</li>
</ul>
</li>
</ul>
<h3 id="week-5">Week 5</h3>
<ul>
<li><strong>Policy Evaluation</strong>: For a fixed policy π, we can compute the utility of each state, V<sup>π</sup>(s). This is done by solving a system of linear equations derived from the Bellman equation.</li>
<li><strong>Policy Improvement</strong>: After evaluating the policy, we can improve it by making it &ldquo;greedy&rdquo; with respect to the current values. For each state, we choose the action that maximizes the expected utility.</li>
<li><strong>Policy Iteration</strong>: This is an algorithm that alternates between policy evaluation and policy improvement, starting with a random policy. It is guaranteed to converge to the optimal policy, often much faster than Value Iteration.</li>
<li><strong>Q-Values</strong>: A Q-value, Q(s, a), is the expected utility of taking action &lsquo;a&rsquo; in state &rsquo;s&rsquo; and then following the policy thereafter. It&rsquo;s often easier to select actions using Q-values than V-values, as you can simply pick the action with the highest Q-value.</li>
<li><strong>Monte Carlo Methods</strong>: A model-free RL technique that learns from complete &ldquo;episodes&rdquo; or trials. It estimates the value of a state by averaging the returns (total rewards) observed after visiting that state.</li>
</ul>

  </content>
  
  

    </main>
    <footer>
      
  <span>© 2024 Lee Wei</span>


  <span>
    |
    Made with
    <a href="https://github.com/maolonglong/hugo-simple/">Hugo ʕ•ᴥ•ʔ Simple</a>
  </span>


    </footer>

    
</body>
</html>
