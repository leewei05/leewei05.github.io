<!doctype html>
<html lang="en">
  <head>
    <meta http-equiv="X-Clacks-Overhead" content="GNU Terry Pratchett" />
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <link rel="shortcut icon" href="/images/favicon.png" />

<title>Spring 2025: ECE 6545 Deep Learning with Image Analysis&nbsp;|&nbsp; :)</title>
<meta
  name="title"
  content="Spring 2025: ECE 6545 Deep Learning with Image Analysis"
/>
<meta
  name="description"
  content="Week 1 Object detection: boundary of the object, what is the object, where is the object. Semantic Segmentation: labels different sections. Linear Classifier: draw a line in a space to classify different types of data. Overfitting: the model matches the training set too closely, resulting in the model failing to predict correctly on new data. Image Classification challenges: resolution of image variables It is common to have more training data than testing data."
/>
<meta
  name="keywords"
  content=""
/>

  <meta name="author" content="Lee Wei" />




<meta property="og:title" content="Spring 2025: ECE 6545 Deep Learning with Image Analysis" />
<meta property="og:description" content="Week 1 Object detection: boundary of the object, what is the object, where is the object. Semantic Segmentation: labels different sections. Linear Classifier: draw a line in a space to classify different types of data. Overfitting: the model matches the training set too closely, resulting in the model failing to predict correctly on new data. Image Classification challenges: resolution of image variables It is common to have more training data than testing data." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://leewei.co/ece6545/" /><meta property="article:section" content="blog" />
<meta property="article:published_time" content="2025-01-07T16:38:37-07:00" />
<meta property="article:modified_time" content="2025-01-07T16:38:37-07:00" />




<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Spring 2025: ECE 6545 Deep Learning with Image Analysis"/>
<meta name="twitter:description" content="Week 1 Object detection: boundary of the object, what is the object, where is the object. Semantic Segmentation: labels different sections. Linear Classifier: draw a line in a space to classify different types of data. Overfitting: the model matches the training set too closely, resulting in the model failing to predict correctly on new data. Image Classification challenges: resolution of image variables It is common to have more training data than testing data."/>




<meta itemprop="name" content="Spring 2025: ECE 6545 Deep Learning with Image Analysis">
<meta itemprop="description" content="Week 1 Object detection: boundary of the object, what is the object, where is the object. Semantic Segmentation: labels different sections. Linear Classifier: draw a line in a space to classify different types of data. Overfitting: the model matches the training set too closely, resulting in the model failing to predict correctly on new data. Image Classification challenges: resolution of image variables It is common to have more training data than testing data."><meta itemprop="datePublished" content="2025-01-07T16:38:37-07:00" />
<meta itemprop="dateModified" content="2025-01-07T16:38:37-07:00" />
<meta itemprop="wordCount" content="760">
<meta itemprop="keywords" content="" />
<meta name="referrer" content="no-referrer-when-downgrade" />

    
    <link href="/simple.min.css" rel="stylesheet" />

    
    <link href="/style.min.css" rel="stylesheet" />

    

    
</head>

  <body>
    <header>
      <nav>
  <a
    href="/"
    
    >Home</a
  >

  <a
    href="/blog/"
    
    >Blog</a
  >


  <a href="/index.xml">
    <svg xmlns="http://www.w3.org/2000/svg" class="icon" viewBox="0 0 448 512">
      
      <path
        d="M0 64C0 46.3 14.3 32 32 32c229.8 0 416 186.2 416 416c0 17.7-14.3 32-32 32s-32-14.3-32-32C384 253.6 226.4 96 32 96C14.3 96 0 81.7 0 64zM0 416a64 64 0 1 1 128 0A64 64 0 1 1 0 416zM32 160c159.1 0 288 128.9 288 288c0 17.7-14.3 32-32 32s-32-14.3-32-32c0-123.7-100.3-224-224-224c-17.7 0-32-14.3-32-32s14.3-32 32-32z"
      />
    </svg>
    RSS
  </a>

</nav>

<h1>Spring 2025: ECE 6545 Deep Learning with Image Analysis</h1>


    </header>
    <main>
      
  
    
      
      <p>
        <i>
          <time datetime="2025-01-07" pubdate>
            2025-01-07
          </time>
        </i>
      </p>
    
  
  
  <content>
    <h3 id="week-1">Week 1</h3>
<ul>
<li>Object detection: boundary of the object, what is the object, where is the object.</li>
<li>Semantic Segmentation: labels different sections.</li>
<li>Linear Classifier: draw a line in a space to classify different types of data.</li>
<li>Overfitting: the model matches the training set too closely, resulting in the model failing to predict correctly on new data.</li>
<li>Image Classification challenges:
<ul>
<li>resolution of image</li>
<li>variables</li>
</ul>
</li>
<li>It is common to have more training data than testing data.</li>
<li>Class Imbalance: certain class only has limited amount of data.</li>
<li>K nearest neighbor classifier: find closest resemblance.
<ul>
<li>It is never used due to <strong>slowness</strong>, <strong>overfitting</strong></li>
</ul>
</li>
<li>Hyperparameter: parameters that are fixed during training.
<ul>
<li><code>k</code> in K nearest neighbor classifier is a hyperparameter.</li>
<li>k is usually a odd number to avoid ties when it comes to voting.</li>
</ul>
</li>
<li>Linear Decision boundary: a straight line, plane, or hyperplane that separates different classes in a feature space.</li>
</ul>
<h3 id="week-2">Week 2</h3>
<ul>
<li>Linear Regression: a line that separates different types of data.
<ul>
<li>Under mild condition, linear regression has an optimal solution.</li>
</ul>
</li>
<li>Mean Squared Error (MSE): Average of the squared differences between observed and predicted values.
<ul>
<li>Good for linear regression.</li>
</ul>
</li>
<li>Supervised Learning: train model with training set and maps input to output while minimizing errors.</li>
<li>How to find the minimum with reference to <code>w</code>?
<ul>
<li>Differential MSE with w = 0</li>
</ul>
</li>
<li>Polynomial Regression: a curve line that separates different data.</li>
<li>Machine Learn Assumption: training set is drawn from the same probability distribution as test data.
<ul>
<li>Example: train a model based on the heights of 6 - 12 years olds, but the test data are the heights of 18 - 24 years olds. The model will not generalize well.</li>
<li><strong>Ultimate Goal:</strong> has as small errors as possible.</li>
</ul>
</li>
<li>Regularization is a technique used in machine learning to prevent overfitting by introducing additional constraints or penalties to the model&rsquo;s loss function.</li>
<li>Maximum Likelihood Estimation: find the parameter that maximizes the likelihood of the observed data under a given probabilistic model.
<ul>
<li>MLE estimates often converge to the expected value of the true parameter.</li>
<li>MLE is found by taking the derivative of the log-likelihood and solving for zero.</li>
<li>MLE is asymptotically unbiased but may be biased in small samples.</li>
<li>MLE has the lowest variance possible asymptotically (efficient estimator).</li>
<li>MLE is equivalent to minimizing KL divergence(minimize between 2 distributions).</li>
</ul>
</li>
<li>Binary Classification: predicting between two classes.</li>
<li>Cross-Entropy Loss: Measures the difference between predicted and actual labels. It ensures that high-confidence incorrect predictions get large gradients (forcing corrections).</li>
<li>Squash Function (Sigmoid): Converts raw scores to probabilities (0 to 1).
<ul>
<li>Divide each output by the sum of all outputs. What happens if the sum is negative? Exponential.</li>
</ul>
</li>
</ul>
<h3 id="week-3">Week 3</h3>
<ul>
<li>SoftMax: transforms outputs into probabilities, ensures probabilities sum is 1.</li>
<li>ReLU(Rectified Linear Unit): ReLU(x) = max(x, 0). It removes any negative values and keep positive values.</li>
<li>Goal: use neurual network to linear separate samples.
<ul>
<li>The more hidden layers you have, a much larger set of problems you can approximate.</li>
<li>Don&rsquo;t put sigmoid functions in the middent of the hidden layers, but it can be used on output layers.</li>
</ul>
</li>
<li>Loss functions:
<ul>
<li>MSE: regression</li>
<li>BCE(Binary Cross Entropy): binary classification</li>
<li>Cross Entropy: multi-class labels</li>
</ul>
</li>
<li>Several approach for training neurual networks:
<ul>
<li>Batch Descent</li>
<li>Stochastic gradient descent: one sample at a time(epoch one iteration), converge faster.</li>
<li>Mini Batch: each epoch is limited to B samples.</li>
</ul>
</li>
<li>Computational Graph</li>
</ul>
<h3 id="week-4">Week 4</h3>
<ul>
<li>Forward Pass: input data moves through data in a neurual network.</li>
<li>Backward Pass (Backpropagation): computing gradients using the chain rule.</li>
<li>Weight Updates: adjusting weights based on the gradients using optimization techniques like Stochastic Gradient Descent (SGD).</li>
<li>Activation Functions: Non-linearity in hidden layers (e.g., ReLU, sigmoid).</li>
<li>Batch Processing: concepts of minibatch to speed up the process.</li>
</ul>
<h3 id="week-5">Week 5</h3>
<ul>
<li>CNN</li>
<li>Cross-correlation v.s Convolution</li>
</ul>
<h3 id="week-6">Week 6</h3>
<ul>
<li>Max pooling</li>
<li>Stride</li>
<li>Conv -&gt; ReLU -&gt; Pooling</li>
<li>Regularization: L2 penalty</li>
<li>Global average pooling can replace flattening.</li>
<li>Backprop for CNNs
<ul>
<li>Local derivative for max pooling.</li>
<li>Local derivative for convolution layer. Similiar front convolutional operation, compute the downstream gradient and apply the filter downstream.</li>
</ul>
</li>
</ul>
<h3 id="week-7">Week 7</h3>
<ul>
<li>PyTorch</li>
<li>CNN</li>
</ul>
<h3 id="week-8">Week 8</h3>
<ul>
<li>Data preprocessing</li>
<li>Batch Normalization is a technique that improves speed(especially if training is deep) and stability for training neurual networks.</li>
<li>What is Normalization?
<ul>
<li>Re-centering and re-scaling layer&rsquo;s input.</li>
</ul>
</li>
<li>Parameter initialization</li>
<li>Regularization
<ul>
<li>L2 Regularization(weight decay)</li>
<li>Dropout</li>
</ul>
</li>
<li>Hyperparameter search
<ul>
<li>Grid search: train entire dataset with small epoch</li>
<li>Random search</li>
</ul>
</li>
</ul>
<h3 id="week-9">Week 9</h3>
<ul>
<li>Image Analysis</li>
<li>Object Localization
<ul>
<li>Bounding box</li>
<li>Height</li>
</ul>
</li>
<li>Model
<ul>
<li>CNN -&gt; flatten -&gt; softmax classification</li>
<li>
<pre><code>           -&gt; f.c linear activation
</code></pre>
</li>
<li>Add cross-entropy loss of classification and mse loss * lambda(to adjust the loss)</li>
<li>multitask learning</li>
</ul>
</li>
<li>Transfer Learning
<ul>
<li>Limitation</li>
</ul>
</li>
<li>What if there are mutliple objects?</li>
<li>R-CNN region based CNN</li>
<li>Fast R-CNN</li>
<li>Faster R-CNN</li>
</ul>
<h3 id="week-11">Week 11</h3>

  </content>
  <p>
    
  </p>

    </main>
    <footer>
      
  <span>© 2024 Lee Wei</span>


  <span>
    |
    Made with
    <a href="https://github.com/maolonglong/hugo-simple/">Hugo ʕ•ᴥ•ʔ Simple</a>
  </span>


    </footer>

    
</body>
</html>
