<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title> :)</title>
    <link>https://leewei.co/</link>
    <description>Recent content on  :)</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <managingEditor>lee10202013@gmail.com (Lee Wei)</managingEditor>
    <webMaster>lee10202013@gmail.com (Lee Wei)</webMaster>
    <copyright>Â© 2024 Lee Wei</copyright>
    <lastBuildDate>Tue, 07 Jan 2025 16:38:37 -0700</lastBuildDate>
    <atom:link href="https://leewei.co/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Spring 2025: CS 6460 Operating Systems</title>
      <link>https://leewei.co/blog/cs6460/</link>
      <pubDate>Tue, 07 Jan 2025 16:38:37 -0700</pubDate><author>lee10202013@gmail.com (Lee Wei)</author>
      <guid>https://leewei.co/blog/cs6460/</guid>
      <description>Week 1 Time sharing: a policy for processes to take turn to use the CPU. Hardware has a timer to send interrupts to the OS. Scheduling: choose process to run. Isolation: avoid process access other processes&amp;rsquo; data. File descriptor: an integer that maps to a file. Unix philosophy: everything is a file. Kernel maintains a file descriptor table. 0: stdin, 1: stdout, 2: stderr Week 2 fork(): create a new process.</description>
      <content:encoded><![CDATA[<h3 id="week-1">Week 1</h3>
<ul>
<li>Time sharing: a policy for processes to take turn to use the CPU.
<ul>
<li>Hardware has a timer to send interrupts to the OS.</li>
</ul>
</li>
<li>Scheduling: choose process to run.</li>
<li>Isolation: avoid process access other processes&rsquo; data.</li>
<li>File descriptor: an integer that maps to a file.
<ul>
<li>Unix philosophy: <em>everything is a file</em>.</li>
<li>Kernel maintains a file descriptor table.</li>
<li><code>0: stdin, 1: stdout, 2: stderr</code></li>
</ul>
</li>
</ul>
<h3 id="week-2">Week 2</h3>
<ul>
<li><code>fork()</code>: create a new process.</li>
<li><code>exec()</code>: replace memory of the current process.
<ul>
<li>It doesn&rsquo;t clear file descriptor.</li>
</ul>
</li>
<li>Pipe: redirect one process&rsquo; output into another process input.
<ul>
<li>parent: write to <code>p[1]</code>, close <code>p[0], p[1]</code></li>
<li>child: close stdin, duplicate <code>p[0]</code> close <code>p[0], p[1]</code></li>
</ul>
</li>
</ul>
<h3 id="week-3">Week 3</h3>
<ul>
<li><code>leave</code>: special instruction in x86, which return the old <code>ebp</code>.</li>
<li>Why do we need stack frames? They are not strictly required, but it is good to have them.
<ul>
<li>Stack contains return addresses of caller function.</li>
</ul>
</li>
<li><code>eax, edx</code>: the return value.</li>
<li><code>ebp</code>(frame pointer): points to the base of the frame.</li>
<li>variables:
<ul>
<li>Global variables: initialized(data section), uninitialized(BSS).</li>
<li>Dynamic variables: allocated on Heap memory.</li>
<li>Local variables: stack.</li>
</ul>
</li>
</ul>
<h3 id="week-4">Week 4</h3>
<ul>
<li>Linking: combines multiple object files into an executable or a library.
<ul>
<li>Pros</li>
<li>We can write our programs in modules.</li>
<li>Faster code compilation, since we only need to re-compile changed source files and link them to the final target.</li>
<li>Space efficient, since we can share common code.</li>
</ul>
</li>
<li>Loading: load executable into memory.</li>
<li>Relocation: merge sections of each object files into multiple sections in the final executable. Resolve any unknown memory addresses.</li>
<li>ELF format
<ul>
<li>Program header table: used by loader to load each segments into memory.</li>
<li>Section header table: used by linker to link code and data sections together.</li>
</ul>
</li>
<li>Statically linked: library is linked into the executable, which makes the size of the file larger.</li>
<li>Dynamically linked: library is loaded at runtime, which makes the size of the file smaller.</li>
<li>Position independent code(PIC): generate code in such a way that it can work no
matter where it is located in the address space.
<ul>
<li>Add additional layer of indirection for all references to global data, imported functions.</li>
<li>Global Offset Table(GOT): a table, which maintains by the linker, that stores the addresses of variables.</li>
</ul>
</li>
</ul>
<h3 id="week-5">Week 5</h3>
<ul>
<li>How to share one memory across multiple processes?
<ul>
<li>Relocation: process 1 starts from 0x00, process 2 starts from 0x1100.</li>
<li>This works but it lacks isolation. One process can easily access other processes&rsquo; memory.</li>
</ul>
</li>
<li>How can we enforce isolation?
<ul>
<li>Software: SFI(Software Fault Isolation) works, but it has performance overhead.</li>
<li>Hardware: segmentations add base addresses, which are maintained by the hardware, for each process. Hardware has a special register to keep an index into the table.
<ul>
<li>Global Descriptor Table: an array of segments(base and size) and access control flags.</li>
<li>GDT register points to the address of GDT in physical address.</li>
<li>Linear address(named by Intel): physical address = base(logical address) + offset(effective address)</li>
<li>Each process has a private GDT.</li>
</ul>
</li>
</ul>
</li>
<li>What if one process needs more memory and the increased memory section overlaps with another process?
<ul>
<li>Move the other process to another memory address, or swap it to disk. Both solutions are inefficient.</li>
</ul>
</li>
<li>Paging is an alternative solution for segmentation.
<ul>
<li>Instead of seeing memory as a contiguous area, OS treats them as multiple pages that map to frames on physical memory.</li>
<li>Each process has its own page table(page table directory and page tables).</li>
</ul>
</li>
<li>Implementation for Paging
<ul>
<li>Array</li>
<li>Array of arrays(Page table)</li>
</ul>
</li>
</ul>
<h3 id="week-6">Week 6</h3>
<ul>
<li>copy-on-write: OS only copies page tables only when one of parent, child writes.</li>
<li>What kind of services might disable page table?
<ul>
<li>Databases</li>
<li>In-memory key-value stores</li>
</ul>
</li>
<li>Does OS flush TLB after context-switching?
<ul>
<li>A tagged TLB can tag process id to avoid flushing TLB. Greater performance.</li>
</ul>
</li>
<li>System boot</li>
</ul>
<h3 id="week-7">Week 7</h3>
<ul>
<li>System boot
<ul>
<li>Intel ME powers first and reads initialization code from BIOS chip.</li>
<li>One of the logical processor is chosen as Bootstrap processor(BSP). Others will become application processors.</li>
<li>BSP starts reading instructions in the BIOS chip.</li>
<li>BSP starts without DRAM. Custom assembly code that uses no stack.</li>
</ul>
</li>
<li>System Management Mode: OS cannot access this region of memory. No way to disable this.</li>
<li>BIOS ends by loading a boot loader.</li>
<li>(xv6) BIOS starts executing instructions at address <code>0x7c00</code>.</li>
<li>Outline of the boot sequence:
<ul>
<li>Setup segmentation(data and code)</li>
<li>Switch to protection mode(16 to 32 bits)</li>
<li>Load GDT(global descriptor table)</li>
<li>Setup stack(part of C runtime)</li>
<li>Load kernel from disk(ELF)</li>
<li>Jump to kernel entry(set page as 4KB and setup page directory)</li>
<li>Setup page table</li>
<li>Setup high address stack <code>0x7c00</code> grows towards <code>0x0000</code></li>
<li>Jump to main</li>
</ul>
</li>
<li>Page table has two entries to map to the kernl
<ul>
<li>#1: 0x0:0x4MB</li>
<li>#2: 0x80000000:0x8040000</li>
</ul>
</li>
<li>Hardware wants the page table directory in register cr3.</li>
</ul>
<h3 id="week-8">Week 8</h3>
<ul>
<li>Why do we need the first page table entry? 0x0:0x4MB
<ul>
<li>After enabling page table, the kernel will continue executing on virtual address.</li>
<li>If we don&rsquo;t map the first page table, it will crash after enabling paging.</li>
</ul>
</li>
<li>Linker script specifies the memory address of each section.</li>
<li>Enforce isolation so that user processes cannot access each other and the kernel.</li>
<li>(xv6) Each process will have a 2GB user memory and 2GB kernel memory.</li>
<li>How to implement a memory allocator?
<ul>
<li>A bitmap that refers to pages. 0: available page, 1: not available page.</li>
<li>Maintain a free page lists.</li>
</ul>
</li>
<li>(xv6) There is an area of free memory after the kernel end where we can use to allocate kernel page table.</li>
<li>Map a region of virtual memory into page tables.
<ul>
<li>Start at 2GB, iterate memory by page, allocate page directory and pages, map pte with repected physcial address.</li>
</ul>
</li>
<li>Lowest 12 bits of the page table entry are used as modes.</li>
<li>Why do we need interrupt?
<ul>
<li>Timer</li>
<li>Hardware notification</li>
</ul>
</li>
<li>What do we save before handling interrupts?
<ul>
<li>CS(code segment registers)</li>
<li>EFLAG</li>
<li>EIP</li>
</ul>
</li>
</ul>
<h3 id="week-9">Week 9</h3>
<ul>
<li>Midterm recap
<ul>
<li>No: AI, Google Search</li>
<li>Yes: homework, quiz, compiler explorer</li>
<li>Lecture 1 - 7</li>
</ul>
</li>
<li>Be familiar with Unix system calls
<ul>
<li>read, write, open, dup, close</li>
</ul>
</li>
<li>Be familiar with x86 calling convention</li>
<li>Stack, BSS, data, heap</li>
<li>Relocation</li>
<li>Page Tables</li>
</ul>
<h3 id="week-11">Week 11</h3>
]]></content:encoded>
    </item>
    <item>
      <title>Spring 2025: ECE 6545 Deep Learning with Image Analysis</title>
      <link>https://leewei.co/blog/ece6545/</link>
      <pubDate>Tue, 07 Jan 2025 16:38:37 -0700</pubDate><author>lee10202013@gmail.com (Lee Wei)</author>
      <guid>https://leewei.co/blog/ece6545/</guid>
      <description>Week 1 Object detection: boundary of the object, what is the object, where is the object. Semantic Segmentation: labels different sections. Linear Classifier: draw a line in a space to classify different types of data. Overfitting: the model matches the training set too closely, resulting in the model failing to predict correctly on new data. Image Classification challenges: resolution of image variables It is common to have more training data than testing data.</description>
      <content:encoded><![CDATA[<h3 id="week-1">Week 1</h3>
<ul>
<li>Object detection: boundary of the object, what is the object, where is the object.</li>
<li>Semantic Segmentation: labels different sections.</li>
<li>Linear Classifier: draw a line in a space to classify different types of data.</li>
<li>Overfitting: the model matches the training set too closely, resulting in the model failing to predict correctly on new data.</li>
<li>Image Classification challenges:
<ul>
<li>resolution of image</li>
<li>variables</li>
</ul>
</li>
<li>It is common to have more training data than testing data.</li>
<li>Class Imbalance: certain class only has limited amount of data.</li>
<li>K nearest neighbor classifier: find closest resemblance.
<ul>
<li>It is never used due to <strong>slowness</strong>, <strong>overfitting</strong></li>
</ul>
</li>
<li>Hyperparameter: parameters that are fixed during training.
<ul>
<li><code>k</code> in K nearest neighbor classifier is a hyperparameter.</li>
<li>k is usually a odd number to avoid ties when it comes to voting.</li>
</ul>
</li>
<li>Linear Decision boundary: a straight line, plane, or hyperplane that separates different classes in a feature space.</li>
</ul>
<h3 id="week-2">Week 2</h3>
<ul>
<li>Linear Regression: a line that separates different types of data.
<ul>
<li>Under mild condition, linear regression has an optimal solution.</li>
</ul>
</li>
<li>Mean Squared Error (MSE): Average of the squared differences between observed and predicted values.
<ul>
<li>Good for linear regression.</li>
</ul>
</li>
<li>Supervised Learning: train model with training set and maps input to output while minimizing errors.</li>
<li>How to find the minimum with reference to <code>w</code>?
<ul>
<li>Differential MSE with w = 0</li>
</ul>
</li>
<li>Polynomial Regression: a curve line that separates different data.</li>
<li>Machine Learn Assumption: training set is drawn from the same probability distribution as test data.
<ul>
<li>Example: train a model based on the heights of 6 - 12 years olds, but the test data are the heights of 18 - 24 years olds. The model will not generalize well.</li>
<li><strong>Ultimate Goal:</strong> has as small errors as possible.</li>
</ul>
</li>
<li>Regularization is a technique used in machine learning to prevent overfitting by introducing additional constraints or penalties to the model&rsquo;s loss function.</li>
<li>Maximum Likelihood Estimation: find the parameter that maximizes the likelihood of the observed data under a given probabilistic model.
<ul>
<li>MLE estimates often converge to the expected value of the true parameter.</li>
<li>MLE is found by taking the derivative of the log-likelihood and solving for zero.</li>
<li>MLE is asymptotically unbiased but may be biased in small samples.</li>
<li>MLE has the lowest variance possible asymptotically (efficient estimator).</li>
<li>MLE is equivalent to minimizing KL divergence(minimize between 2 distributions).</li>
</ul>
</li>
<li>Binary Classification: predicting between two classes.</li>
<li>Cross-Entropy Loss: Measures the difference between predicted and actual labels. It ensures that high-confidence incorrect predictions get large gradients (forcing corrections).</li>
<li>Squash Function (Sigmoid): Converts raw scores to probabilities (0 to 1).
<ul>
<li>Divide each output by the sum of all outputs. What happens if the sum is negative? Exponential.</li>
</ul>
</li>
</ul>
<h3 id="week-3">Week 3</h3>
<ul>
<li>SoftMax: transforms outputs into probabilities, ensures probabilities sum is 1.</li>
<li>ReLU(Rectified Linear Unit): ReLU(x) = max(x, 0). It removes any negative values and keep positive values.</li>
<li>Goal: use neurual network to linear separate samples.
<ul>
<li>The more hidden layers you have, a much larger set of problems you can approximate.</li>
<li>Don&rsquo;t put sigmoid functions in the middent of the hidden layers, but it can be used on output layers.</li>
</ul>
</li>
<li>Loss functions:
<ul>
<li>MSE: regression</li>
<li>BCE(Binary Cross Entropy): binary classification</li>
<li>Cross Entropy: multi-class labels</li>
</ul>
</li>
<li>Several approach for training neurual networks:
<ul>
<li>Batch Descent</li>
<li>Stochastic gradient descent: one sample at a time(epoch one iteration), converge faster.</li>
<li>Mini Batch: each epoch is limited to B samples.</li>
</ul>
</li>
<li>Computational Graph</li>
</ul>
<h3 id="week-4">Week 4</h3>
<ul>
<li>Forward Pass: input data moves through data in a neurual network.</li>
<li>Backward Pass (Backpropagation): computing gradients using the chain rule.</li>
<li>Weight Updates: adjusting weights based on the gradients using optimization techniques like Stochastic Gradient Descent (SGD).</li>
<li>Activation Functions: Non-linearity in hidden layers (e.g., ReLU, sigmoid).</li>
<li>Batch Processing: concepts of minibatch to speed up the process.</li>
</ul>
<h3 id="week-5">Week 5</h3>
<ul>
<li>CNN</li>
<li>Cross-correlation v.s Convolution</li>
</ul>
<h3 id="week-6">Week 6</h3>
<ul>
<li>Max pooling</li>
<li>Stride</li>
<li>Conv -&gt; ReLU -&gt; Pooling</li>
<li>Regularization: L2 penalty</li>
<li>Global average pooling can replace flattening.</li>
<li>Backprop for CNNs
<ul>
<li>Local derivative for max pooling.</li>
<li>Local derivative for convolution layer. Similiar front convolutional operation, compute the downstream gradient and apply the filter downstream.</li>
</ul>
</li>
</ul>
<h3 id="week-7">Week 7</h3>
<ul>
<li>PyTorch</li>
<li>CNN</li>
</ul>
<h3 id="week-8">Week 8</h3>
<ul>
<li>Data preprocessing</li>
<li>Batch Normalization is a technique that improves speed(especially if training is deep) and stability for training neurual networks.</li>
<li>What is Normalization?
<ul>
<li>Re-centering and re-scaling layer&rsquo;s input.</li>
</ul>
</li>
<li>Parameter initialization</li>
<li>Regularization
<ul>
<li>L2 Regularization(weight decay)</li>
<li>Dropout</li>
</ul>
</li>
<li>Hyperparameter search
<ul>
<li>Grid search: train entire dataset with small epoch</li>
<li>Random search</li>
</ul>
</li>
</ul>
<h3 id="week-9">Week 9</h3>
<ul>
<li>Image Analysis</li>
<li>Object Localization
<ul>
<li>Bounding box</li>
<li>Height</li>
</ul>
</li>
<li>Model
<ul>
<li>CNN -&gt; flatten -&gt; softmax classification</li>
<li>
<pre><code>           -&gt; f.c linear activation
</code></pre>
</li>
<li>Add cross-entropy loss of classification and mse loss * lambda(to adjust the loss)</li>
<li>multitask learning</li>
</ul>
</li>
<li>Transfer Learning
<ul>
<li>Limitation</li>
</ul>
</li>
<li>What if there are mutliple objects?</li>
<li>R-CNN region based CNN</li>
<li>Fast R-CNN</li>
<li>Faster R-CNN</li>
</ul>
<h3 id="week-11">Week 11</h3>
]]></content:encoded>
    </item>
    <item>
      <title>Fall 2024: CS 6810 Computer Architecture</title>
      <link>https://leewei.co/blog/cs6810/</link>
      <pubDate>Thu, 05 Dec 2024 20:47:54 -0600</pubDate><author>lee10202013@gmail.com (Lee Wei)</author>
      <guid>https://leewei.co/blog/cs6810/</guid>
      <description>Week 1 Introduction and metrics
Week 2 Metrics and ISA
Week 3 To improve the performance of a processor, we introduce a technique called Pipelining. Pipelining splits instructions into multiple stages. Ideally, Throughput increases by a factor of # of the stages.
Pipeling are usually 5 stages: IF, ID, EXE, MEM, WB.
Instruction Fetch(IF): fetch instruction from memory and PC = PC + 4. Instruction Decode(ID): read registers from register file and sign extension for immediate value.</description>
      <content:encoded><![CDATA[<h3 id="week-1">Week 1</h3>
<p>Introduction and metrics</p>
<h3 id="week-2">Week 2</h3>
<p>Metrics and ISA</p>
<h3 id="week-3">Week 3</h3>
<p>To improve the performance of a processor, we introduce a technique called <code>Pipelining</code>.
<code>Pipelining</code> splits instructions into multiple stages. Ideally, Throughput increases by a factor of # of the stages.</p>
<p>Pipeling are usually 5 stages: IF, ID, EXE, MEM, WB.</p>
<ol>
<li>Instruction Fetch(IF): fetch instruction from memory and PC = PC + 4.</li>
<li>Instruction Decode(ID): read registers from register file and sign extension for immediate value.</li>
<li>Execution(EXE): execute the instruction with one input register 0 and either register 1 or immediate value. Computing branch can also be in this stage.</li>
<li>Access Memory(MEM): execute load or store instruction.</li>
<li>Write Back(WB): write value back to register file.</li>
</ol>
<p>Each stage has a buffer that passes information to the next stage. These buffers are controlled by controlling signals.</p>
<p>One problem for pipelining is to balance the clock period of each stage since the lowest circuit delay determines the clock cycle.</p>
<h3 id="week-4">Week 4</h3>
<p>Pipeline Hazards are events that restrict the pipeline flow.</p>
<ol>
<li>Structural Hazard: resource conflicts. For instance, processor with one memory unit could have structural hazard when fetching instruction and executing memory instruction at the same time.</li>
<li>Data Hazard:</li>
<li>Control Hazard:</li>
</ol>
<p>Static branch predictor: fixed prediction.</p>
<h3 id="week-5">Week 5</h3>
<p>Scoreboarding is a technique for allowing instructions to execute out of order when there are sufficient resources and no data dependences. It utilizes in-order issues.</p>
<p>Scoreboarding limitation</p>
<ul>
<li>Structural hazard: functional units are busy for the current instruction.</li>
<li>Resolving WAW, RAW, WAR with stalls.</li>
<li>Registers are only read when they are both available.</li>
</ul>
<p>Main idea:</p>
<ul>
<li>Split ID into two stages:
<ul>
<li>Issue: decode instruction, check for structural hazard and WAW.</li>
<li>Read operands: wait until no RAW hazard, read data from registers.</li>
</ul>
</li>
<li>Execution</li>
<li>Write Back: check for WAR</li>
</ul>
<h3 id="week-6">Week 6</h3>
<p>Tomasulo&rsquo;s Algorithm</p>
<ul>
<li>Reservation stations: a buffer infront of every function unit, so that processor doesn&rsquo;t stall when there&rsquo;s structural hazard.</li>
<li>Register renaming: read value for operands without reading from register file. These renaming data are stored in the rename table.</li>
<li>A common data bus(CDB) connects every reservation station.</li>
</ul>
<p>Tomasulo limitation</p>
<ul>
<li>Branch stall execution. Tomasulo doesn&rsquo;t allow branch prediction. It waits until branch is resolved.</li>
<li>Loads and stores are performed in order.</li>
</ul>
<p>How can we support branch prediction - speculation execution</p>
<p>Multi-issue processors</p>
<ul>
<li>Superscalar: instructions are chosen dynamically by the hardware.</li>
<li>VLIW: instructions are chosen statically by the compiler. Intel Itanium</li>
</ul>
<p>For Tomasulo algorithm, we cannot tell which instructions are after branch instruction due to out-of-order execution.</p>
<ol>
<li>Identify instructions after the branch.</li>
<li>Exception in specualtive code should be buffered before actually raising the exception.</li>
<li>Precise exception: when a exception is raised, all instructions after the exception are squashed.</li>
</ol>
<p>Add a reorder buffer to keep track the original order when issuing instructions.</p>
<p>Issue inorder -&gt; Execute out-of-order -&gt; Commit inorder</p>
<h3 id="week-7">Week 7</h3>
<p>Instruction can only be fetched when a branch is resolved.</p>
<p>Why do we need reservation station when we have reorder buffer?</p>
<p>reorder buffer holds output
reservation station buffers input</p>
<p>Tomasulo with Hardware Speculation</p>
<p>Issue -&gt; Execute -&gt; Write Result(ROB) -&gt; Commit</p>
<p>Trace cache</p>
<p>Midterm review: all until superscalars</p>
<p>Macro-op fusion: Fuses simple instruction combinations to reduce instruction count, kind of like Peephole optimization.</p>
<p>Practical limitations to ILP: programs can only have a certain level of concurrency</p>
<h3 id="week-9">Week 9</h3>
<p>Midterm review + Midterm</p>
<h3 id="week-10">Week 10</h3>
<p>Temporal Locality: recent memory access will have higher chances to be accessed again.
Spatial Locality: locations near the cenet memory access will have higher chances to be accessed.</p>
<p>SRAM: cache
DRAM: Memory</p>
<p>Cache Block placement</p>
<ul>
<li>Fully Associative(one set): block can go any where.
<ul>
<li>Have lower miss rate.</li>
<li>Must search the whole cache to find the block.</li>
</ul>
</li>
<li>Direct Mapped: block can only go to location <code>mod blocksize</code>.
<ul>
<li>Simplest approach.</li>
<li>Blocks map to the same location, resulting in higher miss rate.</li>
<li>Only have one replacement policy.</li>
</ul>
</li>
<li>Set Associative: n-way Associative, each set can have at most n blocks.
<ul>
<li>Higher level caches: 2- or 4-way common (faster search).</li>
<li>Lower level caches: 8- to 32-way common.</li>
</ul>
</li>
</ul>
<p>Cache Block Identification: Tag - Index - Block Offset</p>
<ul>
<li>Example: Cache 32 KBytes, 2-way, 64 Bytes per line, Address 32 bits
<ul>
<li>0x000249F0 = (0000 0000 0000 0010 0100 1001 1111 0000)_2</li>
<li>Block offset = log_2 64 = 6 bits</li>
<li>Index = log_2(32K / 64 / 2 (2-way)) = 15 - 6 - 1 = 8 bits</li>
<li>Tag = 32 - 8 - 6 = 18 bits</li>
</ul>
</li>
</ul>
<p>Eviction Methods: which cache block to evict?</p>
<ul>
<li>Random</li>
<li>Least-recently-used(LRU)</li>
<li>Not-recently-used(NRU): any cache block other than most-recently-used.</li>
</ul>
<p>Inclusive cache</p>
<ul>
<li>lower level cache has a copy of every block in higher-level caches.
<ul>
<li>pros: in parallel systems, if lower-level cache is not presented, system doesn&rsquo;t need to search higher level cache.</li>
<li>cons: need to evict each level&rsquo;s cache block if a cache block is evicted.
Exclusive cache</li>
</ul>
</li>
<li>each level has dintict cache blocks.
<ul>
<li>pros: efficient use of space since there is no duplicate cache block.</li>
<li>cons: cache coherence across different processors.</li>
</ul>
</li>
</ul>
<p>Average Memory Access Time(AMAT) = Hit time + Miss rate * Miss penalty = Hit rate * Hit time + Miss rate * Miss time</p>
<ul>
<li>Hit time is always there because whether the block we&rsquo;re trying to access is in the cache, we will need to check the cache.</li>
</ul>
<p>Techniques for reducing hit time</p>
<ul>
<li>Victim Cache: stores blocks that are evicted from L1. (Also reduces Miss rate or Miss penalty)</li>
</ul>
<p>Techniques for reducing miss penalty</p>
<ul>
<li>Early restart: request words in normal orde and send requested word to processor as soon as it arrives, not wait until cache line is filled.</li>
<li>Critical word first: request the exact word from memory and sends requested word to processor as soon as it arrives.</li>
<li>Merging write buffer: CPU only stalls on write when write buffer full. (Write may overtake early write)</li>
</ul>
<p>Cache Miss Types</p>
<ul>
<li>Compulsory(Cold) miss: when a block is accessed for the first time.</li>
<li>Capacity miss: cache was evicted due to capacity.</li>
<li>Conflict miss: cache was evicted due to capacity of set. (Fully associative does not have this miss type)</li>
</ul>
<p>Reducing Cold Miss Rates</p>
<ul>
<li>Large block size</li>
<li>Prefetch: speculate future instr/data accesses and fetch them into cache. Prefetch should not be late or too early.
<ul>
<li>Hardware prefetch: sequential and strided prefetching. Stream buffer(works well for instruction caches) to prevent cache pollution.</li>
<li>Software prefetch: explicit prefetch instructions. Software prefetch with loop unrolling or software pipeline.
<ul>
<li>Restricted to loops with array accesses and it&rsquo;s hard to get right.</li>
</ul>
</li>
</ul>
</li>
<li>High associativity caches</li>
</ul>
<p>Basic Cache Optimization(+ improvement, - worse)</p>
<ul>
<li>Larger block size: + Miss rate, - Miss penalty. Doesn&rsquo;t affect Hit time and power consumption.</li>
<li>Bigger cache: + Miss rate(improves capacity misses), - Hit time, - Power.</li>
<li>High associativity: + Miss rate(improves conflict misses), - Hit time, - Power.</li>
<li>Multilevel caches: + Miss penalty(data might be found in L2 cache).</li>
<li>Give priority to read misses: read misses and there are write misses in the write buffer, read misses are handled first. + Miss penalty.</li>
<li>Avoid virtual to physical address translation lookup: + Hit time.</li>
</ul>
<p>Compiler Optimizations:</p>
<ul>
<li>Instruction reordering: reduce conflict misses</li>
<li>Data reordering: 2 arrays v.s. struct, loop interchange(swap nested loops to access memory in sequential ), loop fusion, blocking(access blocks of data)</li>
</ul>
<h3 id="week-11">Week 11</h3>
<p>Virtual Memory</p>
<ul>
<li>Programs are too large to fit in physical memory. A method to share physical memory for a large amount of processes.</li>
<li>Each process has its own, full, address space.
<ul>
<li>Virtual Memory: pages</li>
<li>Physical Memory: frames</li>
</ul>
</li>
<li>Recently not used pages may be swapped to disk. (Swap disk)</li>
<li>Virtual Memory miss: page fault. Page not in memory, so OS needs to retrieve pages from disk(very slow).</li>
</ul>
<p>Page Table</p>
<ul>
<li>OS maintains a table that maps all virtual pages to physical page frames.</li>
<li>One PT per process(page table register points to the Page table of the current process) and one for the OS.</li>
<li>Memory is fully associative.</li>
<li>OS maintains a list of free frames.</li>
</ul>
<p>Page Table stores info for translating virtual page number to physical page number.</p>
<p>Methods to make Page Tables space-efficient</p>
<ul>
<li>Inverted page table: a hash table that maps physical and virtual pages.</li>
<li>Hierarchical page table: n-level page table. Only the N-th level page table has the value of physical frames.</li>
</ul>
<p>Paging means that every memory access involves 2 memory accesses: 1. get physical address 2. get data from physical address.</p>
<p>What can we do to make paging faster?</p>
<p>Translation Lookaside Buffer</p>
<ul>
<li>A full-associative(Content Addressable Memory, CAM) cache of Page Table entries. Parallel lookup.</li>
<li>Every entry has many bits(Valid, R/W, User/Supervisor, Dirty, Access), a tag, a data(physcial page number).</li>
</ul>
<p>Virtually tagged problems</p>
<ul>
<li>Synonyms(alias problem): different virtual addresses points to the same physical address.
<ul>
<li>Write to copy 1 would not be reflected in copy 2.</li>
</ul>
</li>
<li>Homonyms: same virtual addresses different physical address due to process switching.
<ul>
<li>Possible solution: 1. flush cache on context switch(increase miss rate) 2. add PID to cache tag.</li>
</ul>
</li>
</ul>
<p>Methods to address a cache in a virtual-memory system</p>
<ul>
<li>Physically Indexed, physically tagged: translation first, increasing L1 hit time.</li>
<li>Virtually Indexed, virtually tagged: cannot distinguish synonyms/homonyms in cache.</li>
<li>Virtually Indexed, physically tagged: L1 cache indexed virtual address, tags can be checked after translation.</li>
<li>Physically Indexed, virtually tagged: not practical.</li>
</ul>
<p>Does physically indexed, physically tagged mean TLB and cache have to be accessed sequentially? Not if PageSize &gt; #Sets * BlockSize</p>
<h3 id="week-12">Week 12</h3>
<p>Motivation for multicores</p>
<ul>
<li>Power wall, ILP wall</li>
</ul>
<p>Parallel architecture = computing model + communication model</p>
<ul>
<li>Computing model: organization of cores and how data is processed</li>
<li>Communication model: how cores communication?
<ul>
<li>Shared memory: explicit synchronization(via loads and stores)</li>
<li>Message passing: implicit synchronization(via messages)</li>
</ul>
</li>
</ul>
<p>Multicore processors</p>
<ul>
<li>Uniform memory access (UMA): physically centralized memory -&gt; Symmetric Multiprocessor(SMP)</li>
<li>Non-Uniform memory access (NUMA): physically distributed memory -&gt; Distributed Shared-Memory</li>
</ul>
<p>Communication Model</p>
<ul>
<li>Threads communication is done through shared memory variables</li>
<li>Explicit data synchronization, done by the developers</li>
</ul>
<p>The main goal for Cache Coherence is to make caches invisible.</p>
<p>Single Write Multiple Reader</p>
<ul>
<li>Write Propagation: writes are eventually visible in all processors.</li>
<li>Write Serialization: writes are in the same order in all processors.</li>
</ul>
<p>Cache Coherence Protocol: keep track of what processors have copies of what data.</p>
<ul>
<li>Invalidate protocols: get rid of data with old values, usually used with write-back caches.
<ul>
<li>+: multiple writes to the cache block only require one invalidation.</li>
<li>+: less bandwidth since it doesn&rsquo;t need to send new value of the data.</li>
<li>-: write-back data to memory when evicting a modified block.</li>
</ul>
</li>
<li>Update protocols: update every caches&rsquo; copy of data, usually used with write-through caches.
<ul>
<li>+: new value can be re-used without the need to ask for it again.</li>
<li>+: data can always be read from memory.</li>
<li>-: possible multiple useless updates.</li>
</ul>
</li>
</ul>
<p>How can cache coherence protocols be implemented?</p>
<ul>
<li>Software coherence: programmer or compiler-controlled</li>
<li>Hardware coherence: add state bits to cache lines to track state of the line.
<ul>
<li>Exclusive state: block is cached only in this cache, has not been modified, but can be modified without permission. Freely modify and upgrade modified state.</li>
</ul>
</li>
</ul>
<h3 id="week-13">Week 13</h3>
<p>Cache Coherence Protocol Implementations</p>
<ul>
<li>Snooping: all cache controllers monitor all other caches&rsquo; activites and maintain the state of their lines.
<ul>
<li>Information is shared in a common bus. Bus does not scale well.</li>
<li>Each cache has a bus-side controller that monitors all transactions.</li>
<li>Two types of snooping: 1. write invalidation 2. write update</li>
</ul>
</li>
<li>Directory: a central control device directly handles all cache activies.
<ul>
<li>Directory acts as a serialization to provide ordering.</li>
</ul>
</li>
</ul>
<p>MSI Protocol</p>
<ul>
<li>Invalid: block is not present. Need to fetch it from memory or other cache.</li>
<li>Shared: in &gt; 1 caches.</li>
<li>Modified: in 1 cache. Processor can read/write directly.</li>
</ul>
<p>MESI Protocol has one more Exclusive state.</p>
<p>Coherence misses: when a block is not in the cache because it was invalidated by a write from another processor.</p>
<ul>
<li>Hard to reduce due to communication and sharing of data in parallel application.</li>
<li>False sharing: processor modify different words of the cache block but end up invalidating the complete block.
<ul>
<li>False sharing coherence misses increase with larger cache line size.</li>
</ul>
</li>
</ul>
<p>Problems for snooping on a common shared bus</p>
<ul>
<li>When should memory provide data?</li>
<li>What if we need to Write-back?</li>
<li>Conflict when processor and bus-side controller check the cache</li>
<li>State transitions may require several steps</li>
<li>What to do if there are conflicting requests(race conditions) on the bus?
<ul>
<li>Transient states</li>
</ul>
</li>
</ul>
<p>Problems for snooping with multi-level hierarchies</p>
<ul>
<li>Processor interacts with L1 while bus-side controller interacts with L2.
<ul>
<li>Inclusive cache and M state caches in L1 must also be in L2</li>
<li>Propagate all transactions to L1</li>
</ul>
</li>
</ul>
<h3 id="week-14">Week 14</h3>
<p>Snooping implementation has bottleneck at the common data bus. Thus we introduce snooping with split-transaction buses.</p>
<ul>
<li>Create buffer for each cache to hold pending transactions.</li>
<li>Send <code>negative acknowledgement (NACK)</code> when buffers are full.</li>
<li>Snooping with Ring can enforce write serialization with home node. If there are multiple racing writes, ties are broken via the home node.</li>
</ul>
<p>Directory contains a line state and sharing bit-vector.</p>
<ul>
<li>Line state: invalid(00), shared(01), modified(10)</li>
<li>Sharing vector: not cached(00), shared(01)</li>
</ul>
<p>Directory operation</p>
<ul>
<li>It is necessary to collect all acknowledgements(ACK) with write that has multiple sharers.</li>
<li>Complex state changes, directory must also receive ACK.</li>
</ul>
<p>Implementation difficulties for direcotry operation</p>
<ul>
<li>Operations have to be serialized locally.</li>
<li>Operations have to be serialized at directory.</li>
</ul>
<p>Directory Overhead grows with number of cores.</p>
<ul>
<li>Baseline overhead: (number of cores + 1 dirty bit / cache block size * 8 bits)</li>
<li>Cached Directories</li>
<li>Limited Pointer Directories</li>
</ul>
<p>Distributed Directories</p>
<ul>
<li>Local, Home, Remote nodes.</li>
</ul>
<p>Memory Consistency is a specification, which specifies the order of loads and stores.</p>
<ul>
<li>Memory Consistency model is governed by 1. Core pipeline(memory reorder) 2. Coherence Protocol</li>
</ul>
<p>Sequential Consistency(SC): 1. Result should be the same in a time-shared multiprocessor 2. Relative order should be maintained in one thread</p>
<ul>
<li>Sequential Consistency is what should load really happens.
<ol>
<li>Threads issue memory operations in program order</li>
<li>Before issuing next memory operation threads wait until last issued memory operation completes (i.e., performs w.r.t. all other processors)</li>
<li>A read is allowed to complete only if the matching write (i.e., the one whose value is returned to the read) also completes</li>
</ol>
</li>
</ul>
<p>Issue: memory operation leaves the processor and becomes visible to the memory subsystem.
Performed: memory operation appears to have taken place.</p>
<ul>
<li>Performed with reference to processor X: performed to processor X.</li>
<li>Globally performed or complete: performed to all processors.</li>
</ul>
<p>Merging write buffer executes memory operations in the following sequence:</p>
<ul>
<li>write foo(200)</li>
<li>write A(400)</li>
<li>write flag(204)</li>
<li>write bar(404)</li>
</ul>
<p>foo and flag will be written to memory before A and bar.</p>
<p>Write-serialization: per variable. Write to same location by different processors are seen in same order by all processors.
Write-atomicity: across threads</p>
<p>In-window Speculation:</p>
<ul>
<li>Speculation: read from cache before commiting. If no change, then commit; If there&rsquo;s a change, then squash and replay.</li>
<li>Write-prefetcing: obtain read-exclusive out-of-order or in parallel. However, the write should be in program.</li>
</ul>
<h3 id="week-15">Week 15</h3>
<p>Relaxed Memory Consistency Models:</p>
<ul>
<li>Total Store Ordering (TSO): relaxes W -&gt; R. Not guarantee because there could be a store buffer.</li>
<li>Partial Store Ordering (PSO): relaxes W -&gt; R and W -&gt; W.</li>
<li>Relaxes Memory Ordering (RMO): relaxes all four memory orders.</li>
<li>Release Consistency (RC): relaxes all four memory orders but provides release store and acquire load.
<ul>
<li>Reads and writes are allowed to bypass both reads and writes.</li>
<li>Previous reads and writes must complete before release completes.</li>
<li>No reads and writes can complete before acquire completes.</li>
</ul>
</li>
<li>IBM Power: relaxes all four memory orders and write atomicity. Provides 2 types of barriers.</li>
</ul>
<p>Every relaxed consistency model ensures single thread dependencies.</p>
<p>Release Consistency:</p>
<ul>
<li>Writer-initiated invalidation</li>
<li>Without Writer-initiated invalidation</li>
</ul>
<p>Out-of-thin-air problem</p>
<p>Progress Axiom: a store should be eventually visible for all processors.</p>
<p>Synchronization is necessary to ensure that operations in a parallel program happen in the correct order.</p>
<ul>
<li>Conditional Variable: signal</li>
<li>Mutual exclusion</li>
</ul>
<p>Without memory consistency model, we cannot implement different types of synchronization.</p>
<p>Can Sequential Consistency implement mutually exclusion?</p>
<ul>
<li>Yes. Peterson algorithm, but it is not practical for multiple processors.</li>
<li>For Relaxes models need to use fences.</li>
</ul>
<p>Building blocks for synchronization. Special instructions(RMW atomic) of the hardware to implement locks.</p>
<ul>
<li>Test &amp; set: reads a memory location and sets it to value 1.</li>
<li>Compare &amp; swap: it is used more frequently. Check the value and swap if the value is equal.</li>
<li>Fetch &amp; add: fetch value from memory location and atomically increment it.</li>
<li>Load Link(or Load Locked)/Store Conditional(LL/SC): don&rsquo;t need to retain exclusive state.</li>
</ul>
<h3 id="week-16">Week 16</h3>
<p>Implementation of Read Modified Write(RMW) instructions:</p>
<ul>
<li>Lock the bus: disallows other threads</li>
<li>Cache line blocking: to obtain read-exclusive state, invalidates other processors&rsquo; cache. Once a processor gains exclusive state,
other processors will receive NACK from the processor that has exclusive access.</li>
</ul>
<p>Exclusive Access</p>
<ul>
<li>Directory retries when receives NACK</li>
<li>With Snooping, processors retires when receives NACK</li>
</ul>
<p>RMW acts like a memory fence(flush write buffer before RMW).</p>
<p>Techniques for reducing test and set traffic:</p>
<ul>
<li>Test and test-&amp;-set relies on cache coherece. It grabs a lock one time.</li>
<li>Test and set with exponential back-off, retry test and set after pause.</li>
</ul>
]]></content:encoded>
    </item>
    <item>
      <title>Fall 2024: CS 6520 Programming Languages</title>
      <link>https://leewei.co/blog/cs6520/</link>
      <pubDate>Thu, 05 Dec 2024 20:47:48 -0600</pubDate><author>lee10202013@gmail.com (Lee Wei)</author>
      <guid>https://leewei.co/blog/cs6520/</guid>
      <description>Week 1 Overview of this course. Shplait programming language basic overview.
Development template
Types: data representation Tests: write function signature and tests Template: write the layout of the function body Body: finish function body case-by-case Homework: Familiar with Shplait.
Week 2 Introduction of Shplait lists.
Homework: Natural recursion implementation with template.
Week 3 Binding and environment
Binding: bind an expression to an variable. Environment: store the bindings. Homework: Write a interpreter for Moe programming language using Shplait.</description>
      <content:encoded><![CDATA[<h3 id="week-1">Week 1</h3>
<p>Overview of this course. <strong>Shplait</strong> programming language basic overview.</p>
<p>Development template</p>
<ol>
<li>Types: data representation</li>
<li>Tests: write function signature and tests</li>
<li>Template: write the layout of the function body</li>
<li>Body: finish function body case-by-case</li>
</ol>
<p>Homework: Familiar with <strong>Shplait</strong>.</p>
<h3 id="week-2">Week 2</h3>
<p>Introduction of <strong>Shplait</strong> lists.</p>
<p>Homework: Natural recursion implementation with template.</p>
<h3 id="week-3">Week 3</h3>
<p>Binding and environment</p>
<ul>
<li>Binding: bind an expression to an variable.</li>
<li>Environment: store the bindings.</li>
</ul>
<p>Homework: Write a interpreter for <strong>Moe</strong> programming language using <strong>Shplait</strong>.</p>
<ul>
<li>Parse: parse <strong>Moe</strong> to <strong>Shplait</strong> data representation (expression).</li>
<li>Subst: substitude identifiers with expressions.</li>
<li>Interp: interpret expressions to integer.</li>
</ul>
<h3 id="week-4">Week 4</h3>
<p>Lambda: function as values. We can also bind anonymous function to variable.
Mutable states and stores.</p>
<ul>
<li>Box: store location of the value.</li>
<li>Unbox: get the value of a given location.</li>
<li>Setbox: update the value of a given location.</li>
</ul>
<p>Homework: Implement <strong>Moe</strong> conditions and boolean. Also, implement thunks and force.</p>
<ul>
<li>Thunk: delay a computation, until it is called by force.</li>
<li>Force: evaluate a thunk expression.</li>
</ul>
<h3 id="week-5">Week 5</h3>
<p>Record: similar to <strong>C</strong>&rsquo;s struct. A record can have a list of fields and values.
Variable: mutable variable.
Fluid let: syntax sugar. Instead of changing the interpreter, we can change the parser to genereate a let form that is matching fluid let.</p>
<p>Homework: Implement <strong>Moe</strong> begin and record initialization, access and mutation.</p>
<ul>
<li>Record: implement with <strong>Moe</strong>&rsquo;s box expression.</li>
</ul>
<h3 id="week-6">Week 6</h3>
<p>Syntax sugar and encoding
Currying: a function takes an argument that returns another function takes another argument.
Midterm: The concept of box is important! Think of box as a pointer, then everything is clear.</p>






<pre tabindex="0"><code>#true = fun(x) : fun(y) : x
#false = fun(x) : fun(y) : y</code></pre>
<h3 id="week-7">Week 7</h3>
<p><code>letrec</code>, <code>mk_rec</code> for encoding recursion. However, the problem with <code>mk_rec</code> is that we assuem the right hand side as a procedure.
Use <code>fun (): ...</code> as a delay for implementing recursion.
<code>Optionof</code> has two variants: <code>none</code> and <code>some</code>.</p>
<p>Homework: Implement syntax sugar for recursive bindings, recursive function and two arguments function.</p>
<h3 id="week-9">Week 9</h3>
<p>Lazy evaluation
Continuation</p>
<h3 id="week-10">Week 10</h3>
<p>Trace continuation
Garbage Collection
Compiler</p>
<p>Homework: Implement <code>neg</code>, <code>avg</code> and support zero or multiple arguments for function call with continuation.</p>
<h3 id="week-11">Week 11</h3>
<p>Compiler
Midterm</p>
<p>Homework: Implement a compiler that translate Moe with garbage collection.</p>
<h3 id="week-12">Week 12</h3>
<p>Class
Inheritance</p>
<p>Homework: Implement instantiation, instanceof, select.</p>
<h3 id="week-13">Week 13</h3>
<p>Type checker checks types of the program before interpreting it.</p>
<p>Homework: Typechecker.</p>
<h3 id="week-14">Week 14</h3>
<p>Type checker with unify.</p>
<p>Homework: Typecheck if0 and list.</p>
<h3 id="week-15">Week 15</h3>
<p>Polymorphism</p>
<p>Homework: Parameterized over types and functions.</p>
<h3 id="week-16">Week 16</h3>
<p>Macro
Programming Language research</p>
<p>Homework: final project</p>
]]></content:encoded>
    </item>
    <item>
      <title>Fall 2024: CS 6475 Advanced Compilers</title>
      <link>https://leewei.co/blog/cs6475/</link>
      <pubDate>Thu, 05 Dec 2024 20:03:46 -0600</pubDate><author>lee10202013@gmail.com (Lee Wei)</author>
      <guid>https://leewei.co/blog/cs6475/</guid>
      <description>Week 1 Overview of this course.
Readings:
The death of optimizing compilers On Proebstingâs Law Impact of Economics on Compiler Optimization Why Do Peephole Optimizations Work? (option) Compiler Optimization Catalog Assignment:
Find a missing optimization in LLVM using this. Prove it with Alive2. Week 2 Discussions:
Is it really the dealth of optimizing compilers? No. On Proebsting&amp;rsquo;s Law. Probably cannot use -O0 as a 18 years old compiler. Impact of Economics on Compiler Optimization.</description>
      <content:encoded><![CDATA[<h3 id="week-1">Week 1</h3>
<p>Overview of this course.</p>
<p>Readings:</p>
<ul>
<li><a href="https://cr.yp.to/talks/2015.04.16/slides-djb-20150416-a4.pdf">The death of optimizing compilers</a></li>
<li><a href="https://gwern.net/doc/cs/algorithm/2001-scott.pdf">On Proebstingâs Law</a></li>
<li><a href="https://dl.acm.org/doi/pdf/10.1145/376656.376751">Impact of Economics on Compiler Optimization</a></li>
<li><a href="https://blog.regehr.org/archives/2485">Why Do Peephole Optimizations Work?</a></li>
<li><a href="https://www.clear.rice.edu/comp512/Lectures/Papers/1971-allen-catalog.pdf">(option) Compiler Optimization Catalog</a></li>
</ul>
<p>Assignment:</p>
<ul>
<li>Find a missing optimization in LLVM using <a href="https://gcc.godbolt.org/">this</a>.</li>
<li>Prove it with <a href="https://alive2.llvm.org/ce/">Alive2</a>.</li>
</ul>
<h3 id="week-2">Week 2</h3>
<p>Discussions:</p>
<ul>
<li>Is it really the dealth of optimizing compilers? No.</li>
<li>On Proebsting&rsquo;s Law. Probably cannot use <code>-O0</code> as a 18 years old compiler.</li>
<li>Impact of Economics on Compiler Optimization. Look for where the money goes.</li>
<li>Refinement is very important!</li>
</ul>
<p>Assignment:</p>
<ul>
<li>Pick a missing optimization in LLVM to implement.</li>
<li>Build LLVM and Alive2 locally.</li>
</ul>
<h3 id="week-3">Week 3</h3>
<p>Discussions:</p>
<ul>
<li>How LLVM is tested?
<ul>
<li>unittest: LLVM API tests</li>
<li>tests: regression tests</li>
<li>llvm-test-suite: benchmark tests</li>
<li>libFuzzer</li>
<li>test by users</li>
</ul>
</li>
<li>AVX512 ternary logic. It&rsquo;s difficult to decode due to its required bit space.</li>
<li>Superoptimization: To generate optimized code, superoptimizer searches for certain pattern, does refinement check and assess with its cost model.</li>
</ul>
<p>Readings:</p>
<ul>
<li><a href="https://www.cs.princeton.edu/~appel/papers/ssafun.pdf">SSA is Functional Programming</a></li>
<li><a href="https://www.cs.cmu.edu/~rjsimmon/15411-f15/lec/10-ssa.pdf">Lecture Notes on Static Single Assignment Form</a></li>
<li><a href="https://lowlevelbits.org/system-under-test-llvm/">System Under Test: LLVM</a></li>
<li><a href="https://blog.regehr.org/archives/1450">Testing LLVM</a></li>
</ul>
<p>Assignment:</p>
<ul>
<li>Implement a missing optimization in LLVM and add tests to it. <a href="https://github.com/regehr/llvm-project/pull/60">Github PR</a></li>
</ul>
<h3 id="week-4">Week 4</h3>
<p>Discussions:</p>
<ul>
<li>Compiler without SSA gets harder to get in right, and it messes up the code base.</li>
<li>Brainfuck language
<ul>
<li>Game of Life in BF</li>
</ul>
</li>
<li>Speedup interpreter with <a href="https://eli.thegreenplace.net/2012/07/12/computed-goto-for-efficient-dispatch-tables">computed goto</a>.</li>
<li>Need to be cautious when using <code>APInt</code>. There are use cases in LLVM that use 80, 320 bits.</li>
<li>Intro for <a href="https://en.wikipedia.org/wiki/Partial_evaluation">Partial Evaluation</a> and its relation to staged computation.
<ul>
<li>Abstract interpreter, approximate computations, Halting problem, Tainted cell.</li>
</ul>
</li>
<li>General optimization approach for BF.
<ul>
<li>Creating virtual instructions for BF is similar with x86-64 processors having virtual instructions that are not exposed to developers.</li>
</ul>
</li>
<li>It&rsquo;s hard to pass alias information to compilers, like using <code>restrict</code> correctly in C.</li>
</ul>
<p>Readings:</p>
<ul>
<li><a href="https://c9x.me/compile/bib/braun13cc.pdf">Simple and Efficient Construction of Static Single Assignment Form</a>: Great paper that solves real problems!</li>
<li><a href="https://arxiv.org/pdf/2305.13241">Whose baseline is it anyway?</a></li>
<li><a href="https://www.npopov.com/2023/10/22/How-to-reduce-LLVM-crashes.html">llvm-reduce</a></li>
</ul>
<p>Assignment:</p>
<ul>
<li>Implement a <a href="https://github.com/leewei05/bf">BF interpreter</a>.</li>
</ul>
<h3 id="week-5">Week 5</h3>
<p>Discussions:</p>
<ul>
<li>Always optimize with a profiler (data). Recursive to a close form.</li>
<li>C++ downcast optimization probably couldn&rsquo;t catch by the profiler.</li>
<li>ABI doc tells developer how procedures communicate. Which register to store argument.</li>
<li>If a compiler doesn&rsquo;t know a fact, sometimes we can rewrite code to teach compiler to optimize.</li>
<li>How does compiler recognize certain pattern, such as popcount, to optimize? Hardcode and do some canonicalization before finding certain patterns.</li>
<li>Approximate: tracking info accurately is hard. Instead, tools try to shrink the area of approximation.</li>
</ul>
<p>Readings:</p>
<ul>
<li><a href="https://cs.au.dk/~amoeller/spa/spa.pdf">Static Program Analysis: Ch 1</a></li>
<li><a href="https://www.agner.org/optimize/">Agner Fog</a></li>
<li><a href="https://www.corsix.org/content/whirlwind-tour-aarch64-vector-instructions">ARM SIMD</a></li>
</ul>
<p>Assignment:</p>
<ul>
<li>Implement a profiler on top of the <a href="https://github.com/leewei05/bf">BF interpreter</a>.
<ul>
<li>Simple loops: no i/o, no pointer changes, either +1, -1</li>
</ul>
</li>
<li>Implement a <a href="(https://github.com/leewei05/bf)">compiler</a> for BF that emits x86-64 assembly.</li>
</ul>
<h3 id="week-6">Week 6</h3>
<p>Discussions:</p>
<ul>
<li>Is it okay to remove infinite loop?</li>
<li>Debugging: Optimization fuel decrement one until fuel run out, binary search the commit.</li>
<li>Scan memory tricks</li>
<li>Fast program tries to handle aligned memory.</li>
<li>Lattice and semilattice are abstract values that live in the compiler.
<ul>
<li>Design goal tight enough to not run forever. In other words, having enough information for the compiler to run fast.</li>
</ul>
</li>
<li>Dataflow Analysis
<ul>
<li>Top is universal set(imprecise info). Start from the top, we will reach a point that has enough information.</li>
<li>Bottom is empty set(precise info). Start from the bottom, we will reach a fixed point.</li>
<li>Least fixed point is the point we get the most information.</li>
</ul>
</li>
</ul>
<p>Examples:</p>
<p>Dead Code Elimination(Lattice with a height of 2):</p>
<ul>
<li>Top is maybe reachable.</li>
<li>Bottom is provably unreachable.</li>
</ul>
<p>Constant Propagation(Lattice with a height of 3):</p>
<ul>
<li>Top is probably not constant.</li>
<li>Integers</li>
<li>Bottom is unreachable.</li>
</ul>
<p>Readings:</p>
<ul>
<li><a href="https://cs.au.dk/~amoeller/spa/spa.pdf">Static Program Analysis: Ch 4</a></li>
<li><a href="https://docs.oracle.com/javase/specs/jls/se23/html/jls-17.html#jls-17.4.9">Infinite Loop in Java</a></li>
<li><a href="https://www.open-std.org/jtc1/sc22/wg14/www/docs/n1528.htm">Undefined Bahvior for infinite loops</a></li>
</ul>
<p>Assignment:</p>
<ul>
<li>Optimize simple and non-simple loops in BF, fast vector implementation.
<ul>
<li>Starts with simple cases: make a tape with bunch of 1s with a 0 in it, and print the pointer of the 0.</li>
<li>Write a tiny BF program</li>
</ul>
</li>
</ul>
<h3 id="week-7">Week 7</h3>
<p>Discussions:</p>
<ul>
<li>GCC IR gimple</li>
<li>LLVM IR flat</li>
<li>In practice, we use worklist algorithm a lot. Which node inside a worklist to choose first? A node that is SCC since it can affect other nodes.</li>
<li>Transfer function table SPA</li>
<li>How can we run better benchmarks?</li>
<li>Setting up the conditions for the compilers to know more details.</li>
<li>Be a better programmer by moving redundant and loop invariant operations outside of loop.</li>
<li>Math is good for low-level programming.</li>
</ul>
<p>References for writing x86-64 vector instructions:</p>
<ul>
<li><a href="https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#ig_expand=5738,440,778,5762,5872,4606,5744,5762,5872,4606">Intel Official Intrinsic manual</a></li>
<li><a href="https://stackoverflow.com/questions/40032906/is-there-an-efficient-way-to-get-the-first-non-zero-element-in-an-simd-register">Peter Cordes on Stackoverflow</a></li>
<li><a href="https://www.felixcloutier.com/x86/">Felix Cloutier&rsquo;s x86-64 manual</a></li>
<li><a href="https://gcc.godbolt.org/z/vqd9K4rqT">One of my working examples</a></li>
</ul>
<p>Readings:</p>
<ul>
<li><a href="https://gcc.gnu.org/onlinedocs/gccint/GIMPLE.html">GIMPLE</a></li>
<li><a href="https://www.cse.wustl.edu/~jain/iucee/index.html">Computer Systems Performance Analysis</a></li>
<li><a href="https://emeryberger.com/research/stabilizer/">Sabilizer</a></li>
</ul>
<p>Videos:</p>
<ul>
<li><a href="https://www.youtube.com/watch?v=vrkR7jKcFpw">Alive2 like tool for GCC</a></li>
</ul>
<h3 id="week-9">Week 9</h3>
<p>Discussions:</p>
<ul>
<li>JIT compiler, how to encode instructions to JIT.</li>
<li>Partial evaluation</li>
<li>Dead store elimination</li>
<li>calloc</li>
<li>global, stack, heap(sticking point, people uninitialized on purpose)</li>
<li>Common subexpression elimination -&gt; Available</li>
<li>Rematerialzation</li>
<li>Very busy -&gt; LICM</li>
<li>Autotuning try a lot of things, PGO run a once and tell the compiler some information next the build.</li>
</ul>
<h3 id="week-11">Week 11</h3>
<p>Discussions:</p>
<ul>
<li>Linker script: tells the linker where memory section maps on a dev board.</li>
<li>Demanded Bits: you cannot prove that bits are not necessary.</li>
<li>LLVM Known bits.</li>
<li>Trace based JIT</li>
<li>Precompiled headers</li>
<li>Timeout is a real problem, AWS IAM Z3 solver.</li>
<li>Flaky tests: sometimes fail and success.</li>
<li>LLVM IR verifier</li>
</ul>
<p>Readings:</p>
<ul>
<li><a href="https://www.amazon.com/Move-Semantics-Complete-Guide-First/dp/3967309002/">C++ Move</a></li>
<li><a href="https://github.com/llvm/llvm-project/blob/main/llvm/include/llvm/Support/KnownBits.h">LLVM KnownBits</a></li>
</ul>
<h3 id="week-12">Week 12</h3>
<p>Discussions:</p>
<ul>
<li>Bugpoint was used in LLVM for reducing tests. It defaults use some instructions result in Undefined Behavior.</li>
<li>Trusting trust attacks.</li>
<li>Diverse Double compiling</li>
</ul>
<p>Readings:</p>
<ul>
<li><a href="https://users.cs.utah.edu/~regehr/papers/undef-pldi17.pdf">Taming Undefined Behavior in LLVM</a></li>
<li><a href="https://dwheeler.com/trusting-trust/">Diverse Double compiling</a></li>
<li><a href="https://en.wikipedia.org/wiki/Reproducible_builds">Reproducible builds</a></li>
<li><a href="https://www.cs.cmu.edu/~rdriley/487/papers/Thompson_1984_ReflectionsonTrustingTrust.pdf">Trusting trust</a></li>
</ul>
<h3 id="week-13">Week 13</h3>
<p>Discussions:</p>
<ul>
<li>Undefined Behavior is also in the hardware chips.</li>
<li>Register pair for multiplication.</li>
<li><code>undef</code> breaks SSA.</li>
<li>Trusting trust really worths reading it!</li>
<li>Java type concurrency</li>
<li>Type system is trying to help you not break the program.</li>
<li>Union find</li>
<li>HCI for expressing types</li>
<li>bitblit</li>
</ul>
<p>How to implement fast inverse square root on GCC and Clang:</p>
<ul>
<li>union</li>
<li>memcpy fast</li>
<li>char *</li>
<li>(C++)reinterpret cast</li>
</ul>
<p>Readings:</p>
<ul>
<li><a href="http://lucacardelli.name/papers/typesystems.pdf">Type Systems</a></li>
<li><a href="http://mazsola.iit.uni-miskolc.hu/~drdani/docs_arm/36_Elsevier-ARM_Sy.pdf">ARM System</a></li>
<li><a href="https://arxiv.org/pdf/2305.13241">Whose Baseline is it anyway?</a></li>
<li><a href="https://devblogs.microsoft.com/oldnewthing/20180209-00/?p=97995">BitBlip</a></li>
<li><a href="https://pdos.csail.mit.edu/~rsc/pike84bitblt.pdf">BitMap</a></li>
</ul>
<h3 id="week-14">Week 14</h3>
<p>Discussions:</p>
<ul>
<li>SMT solvers: provide them constraints to solve. If there are solutions, it will return one solution(model).
<ul>
<li>Practice: z3 sudoku solver(Int, BitVector).</li>
</ul>
</li>
<li>Use a solver for unbounded Loop is hard because we cannot unroll it.</li>
<li>People compile language to Javascript -&gt; asm.js -&gt; WebAssembly(Stack machine)</li>
<li>Baseline compiler</li>
<li>Everyone needs to know at least one language in System Programming, Scripting, Parallel Programming, Math.</li>
<li>Autovectorization for large legacy can improve significant performance.</li>
</ul>
<p>Readings:</p>
<ul>
<li><a href="https://webkit.org/blog/3362/introducing-the-webkit-ftl-jit/">Webkit JIT</a></li>
<li><a href="https://ericpony.github.io/z3py-tutorial/guide-examples.htm">Z3 Python tutorial</a></li>
<li><a href="https://www.sciencedirect.com/science/article/pii/S1877050912003304">CUDA: Compiling and optimizing for a GPU platform</a></li>
</ul>
<h3 id="week-15">Week 15</h3>
<p>Discussions:</p>
<ul>
<li>GPU compilers</li>
<li>What CPU compilers optimizations don&rsquo;t work on GPU compilers? (GPU has limited registers for each thread)
<ul>
<li>Inline</li>
<li>Loop unrolling</li>
<li>LICM</li>
<li>InstCombine</li>
</ul>
</li>
<li>Divergence problem is critical. Divergence &gt; Latency</li>
<li>Rematerialization</li>
</ul>
<p>Readings:</p>
<ul>
<li><a href="https://dl.acm.org/doi/epdf/10.1145/3528416.3530249">Compilation On The GPU? A Feasibility Study</a></li>
</ul>
<h3 id="week-16">Week 16</h3>
<p>Discussions:</p>
<ul>
<li>Compilation on the GPU</li>
<li>Go&rsquo;s generic rules compares with LLVM&rsquo;s large C++ code base.</li>
<li>Go&rsquo;s generic rules should be checked by something like Z3.</li>
<li>e-graph is good but it is probably not practical to adopt to existing compilers like LLVM or GCC. Applying rewrites might blow up memory usage.</li>
<li>Quine is fun.</li>
</ul>
<p>Readings:</p>
<ul>
<li><a href="https://go.dev/src/cmd/compile/internal/ssa/_gen/generic.rules">Go generic rules</a></li>
<li><a href="https://dl.acm.org/doi/epdf/10.1145/3434304">egg: Fast and Extensible Equality Saturation</a></li>
</ul>
]]></content:encoded>
    </item>
    <item>
      <title>2024 Fall CS ç ç©¶æè½é åç³è«</title>
      <link>https://leewei.co/blog/mscs-2024/</link>
      <pubDate>Thu, 21 Mar 2024 18:34:44 +0800</pubDate><author>lee10202013@gmail.com (Lee Wei)</author>
      <guid>https://leewei.co/blog/mscs-2024/</guid>
      <description>ç³è«çµæ Admission
Virginia Tech MS CS (2/20) Pennsylvania State University MS CSE (3/20) The University of Utah MS CS (3/26) Rejection
Indiana University Bloomington MS CS (2/23) University of Illinois Urbana-Champaign MCS (4/1) University of Michigan MS CSE (4/4) University of Wisconsin-Madison PMP CS (4/6) Decision
The University of Utah MS CS èæ¯ Education
NTUST Electronic and Computer Engineering (2013 - 2017) GPA: 3.02/4.0 ð«¢ NTUST + NTUT é¨ç­éè® 3 å­¸æ (2022/09 - 2024/01) GAP: 4.</description>
      <content:encoded><![CDATA[<h4 id="ç³è«çµæ">ç³è«çµæ</h4>
<p><strong>Admission</strong></p>
<ul>
<li>Virginia Tech MS CS (2/20)</li>
<li>Pennsylvania State University MS CSE (3/20)</li>
<li>The University of Utah MS CS (3/26)</li>
</ul>
<p><strong>Rejection</strong></p>
<ul>
<li>Indiana University Bloomington MS CS (2/23)</li>
<li>University of Illinois Urbana-Champaign MCS (4/1)</li>
<li>University of Michigan MS CSE (4/4)</li>
<li>University of Wisconsin-Madison PMP CS (4/6)</li>
</ul>
<p><strong>Decision</strong></p>
<ul>
<li>The University of Utah MS CS</li>
</ul>
<h4 id="èæ¯">èæ¯</h4>
<p><strong>Education</strong></p>
<ul>
<li>NTUST Electronic and Computer Engineering (2013 - 2017)</li>
<li>GPA: 3.02/4.0 ð«¢</li>
<li>NTUST + NTUT é¨ç­éè® 3 å­¸æ (2022/09 - 2024/01)</li>
<li>GAP: 4.0/4.0 (21 credits)</li>
<li>CS related courses: è³æçµæ§ãé¢æ£æ¸å­¸ãè¨ç®æ©ç¶²è·¯æ¦è«ãæ¼ç®æ³ãç·¨è­¯å¨è¨­è¨ãä½æ¥­ç³»çµ±ãè¨ç®æ©çµæ§</li>
</ul>
<p><strong>Test Scores</strong></p>
<ul>
<li>GRE: 320 (V: 152, Q: 168, AWA: 3.5) (2019/10)</li>
<li>TOEFL 107 (R: 29, L: 30, S: 24, W: 24) (2023/06)</li>
</ul>
<p><strong>Award</strong></p>
<ul>
<li>N/A</li>
</ul>
<p><strong>Research Experience</strong></p>
<ul>
<li>N/A</li>
</ul>
<p><strong>Work Experience</strong></p>
<ul>
<li>Opennet, DevOps Engineer (2021/07 - 2022/08)</li>
<li>17 Live, Site Reliability Engineer (2020/10 - 2021/07)</li>
<li>iKala, Senior Customer Engineer (2018/03 - 2020/10)</li>
</ul>
<p><strong>Recommendation Letter</strong></p>
<ul>
<li>NTUST é¨ç­éè® ææ * 3</li>
<li>å·¥ä½åä¸»ç®¡ * 1</li>
</ul>
<h4 id="ç³è«ç¶é©">ç³è«ç¶é©</h4>
<p><strong>æ¨è¦ä¿¡</strong></p>
<p><strong>TOEFL &amp; GRE</strong></p>
<p>ä»¥ä¸ç¥ï¼å¯ä»¥åèæå¤§å­¸é·ç<a href="https://medium.com/@tau_chang/2022-fall-%E5%9C%8B%E5%A4%96-cs-%E7%A0%94%E7%A9%B6%E6%89%80%E8%BD%89%E9%A0%98%E5%9F%9F%E7%94%B3%E8%AB%8B-a6cae9c8df0e">ç³è«å¿å¾</a> ðã</p>
<p><strong>CV &amp; SoP</strong></p>
<p>ä¹åæä¸éåµåææ Alex éçå­¸è¡å¯«ä½è·å£èªªèª²ç¨ï¼åäººè¦ºå¾ä¸èª²é«é©å¾æ£ï¼ä¹å­¸å°å¾å¤ï¼æä»¥å¾ä¾ Statement of Purpose æå¯«äºä¸ååçä¹å¾ææ¾åµåææå¹«å¿æ½¤ç¨¿ãåäººè¦ºå¾è »ä¸é¯çï¼å çºå¨å¹«å¿æ½¤ç¨¿ä¹åï¼éæè· Alex ä¸æ¬¡ç·ä¸ 30 åéçè¨è«ãæä¸é»æè¦ºå¾ç¹å¥éè¦ï¼ã Statement of Purpose æ¯ç¨ä¾åè¨´å°æ¹çºä»éº¼éå program å¯ä»¥å¹«ä½ è£è¶³ä½ ç knowledge gapï¼ä¸¦ä¸å¹«å©ä½ éæä½ çæªä¾ç®æ¨ãèéç¨ä¾è¨´èªªä½ çæå°±çï¼é£æ¯ CV çç¨éãã</p>
<p><strong>ç³è«å­¸æ ¡é¸æ</strong></p>
<p>éç¶æ¯å·¥ä½ä¸æ®µæéä¹å¾æåºä¾è®æ¸ï¼ä½éæ¯æ³è¦å¨ç¢©å£«éæ®µæ¥è§¸å­¸è¡ç ç©¶ãè½äººå®¶é½èªªå¯«è«æãåç ç©¶å¾çè¦ï¼ä¸è©¦è©¦çæéº¼æç¥éãæä»¥æé¸çå¤§é¨å program é½æ¯æè«æé¸é çï¼å°é¨åæ¯ä¸ç¨å¯«è«æçã</p>
<p>å°æ¼å­¸æ ¡ program çåªåé åºï¼</p>
<p>ç¸½è±è²»ä¾¿å® or æ TA/RA æ©æ(å¯ä»¥æ¸åå­¸è²») &gt; æ systems é åçææä»¥åèª²ç¨ &gt; å°é»ãå¤©æ°£ &gt; æ ¡å</p>
<p>é¤éç ç©¶æ©æå¤ + systems é åææå¤ + æ ¡åé½éå¸¸å¥½ï¼ææä¸èæ®è±è²»ï¼æä»¥æææ UMich (ä½ä¹æ²ä¸ ð)ãå çºæå¾æç¢ºæ¯æ³è¦è·èå²å®³çææåç ç©¶çï¼æä»¥æè¦ºå¾æ¯å¦æææå°ç²¾æ¼ systems é åå¤§æ¼å­¸æ ¡çæ ¡åãæèªç¥ç¡æ³è·ç³è« top 50 å­¸æ ¡çç³è«èç«¶ç­ï¼ç¢ç«æä¸æ¯è³å·¥ç³»ç¢æ¥­ï¼ç¶æ­·ä¹ä¸æ¯ç¹å¥æ¼äº®ï¼æä»¥é¸æ ¡æ¯è¼ä¿å®ä¸äºãä½æå¾éæ¯æé¸ä¸äºæ¯è¼åé¢çå­¸æ ¡ï¼æ³èªªç¢°ç¢°éæ°£ã</p>
<h4 id="è½é åå¿è·¯æ­·ç¨">è½é åå¿è·¯æ­·ç¨</h4>
<p>å¶å¯¦ 2019 å¹´åæä¹æç³è«éåå¤ç ç©¶æï¼ä½ç¶ææ¯è¼åæ¯è·é¢¨ãçå°ç¶²è·¯ä¸ä¸äºçå­¸åäº«ï¼éæå¤§ç¥åäºåäº«éä¸äºçå­¸ç¶æ­·å ä¸æäºåäºé¢è·å»çå­¸ï¼å°±èçäºåºåå¸æ¸çæ³æ³ãååå¿å¿å°èäº GRE, TOEFLï¼é¨ææºåç³è«æä»¶éææ¨è¦ä¿¡ãåé ­çï¼ç¶ææ²æè¢«ä»»ä½ä¸éå­¸æ ¡éåç®æ¯ç¶é ­æ£åãé¨ä¾¿çæºåç¶ç¶æ¯ææä¾é¨ä¾¿ççµæãå¾ä¾ covid ä¹çç¼äºï¼æè¨±æ²æå»çå­¸ä¹æ¯å silver liningãç¶ééæ¬¡ç³è«å¤±æçç¶é©ï¼æéå§æ´ç©æ¥µå°å°å¾æçå°æ¥­ãæ´åªåå­¸ç¿ã</p>
<p>2020 å¹´æå» 17 å·¥ä½ï¼å¸æè½ç´¯ç©æ´å¤å¯¦åä¸çç¶é©ã2021 å¹´åå¥½åæä¸åæ©ææå»äºå¦ä¸éè·¨åå¬å¸ãå¨éå©éå¬å¸ç´¯ç©äºè¨±å¤å°æ¥­ç¥è­ï¼ä½ä¹è®æèªç¥å°æéæå¾å¤ä¸è¶³çå°æ¹ãä¾å¦ï¼è³æåº«ãä½æ¥­ç³»çµ±çåºå±¤æ¶æ§ãéäºèªæèªç¥ä¿ä½¿æéå§å°æ¾ç·ä¸çå­¸ç¿è³æºï¼ç¡æéç¼æäº jserv ç Linux æ ¸å¿å¯¦ä½èª²ç¨ã2020 å¹´æ¥å­£æè©¦èä¸ç­å¾è·èä¸äº 8 é±ï¼ä½å çºæè¨ç®æ©çåºç¤ç¥è­ä¸è¶³ï¼æä»¥å­¸èµ·ä¾éå¸¸çè¦ï¼å¾ä¾å°±æ¾æ£äºãä¸å¥è©±ï¼æççç ð¢ãç¶éåä¸æ¬¡çæ«æï¼æåè¨´èªå·±æ¢æ¢ä¾æ¯è¼å¿«ã2022 å¹´ 8 æå­å°ä¸äºé¢è®ææåæ°£è£¸è¾­ææ©æºåç³è«ï¼ä¸¦ä¸åå°å°ç§å¤§è·å¤§å­¸çä¸èµ·ä¸èª²ï¼ä»¥è£è¶³ç¼ºå°çåºç¤ç¥è­ã</p>
<p>2022 å¹´ 9 æï¼ç¢æ¥­äºå¹´ä¹å¾åæ­¸æ ¡åãä»¥åå¤§å­¸æé½æ¯åå¨æå¾ä¸ææ»ææ©ææ¯ç¡è¦ºï¼ä½éåæ ¡åå¾ï¼æé½åå¨åä¸æãä¸é¨åæ¯ççæ³è¦èªçå­¸ç¿ï¼ä¸é¨åä¹æ¯æ³è¦è®ææå¤èªè­æï¼ä¹å¾è©¢åæ¨è¦ä¿¡ææ¯è¼é å©ã2023 å¹´ 2 æï¼åéå®å¹´ï¼éå§æºå TOEFL èè©¦ãéæ¬¡ä¹æ²æè£ç¿ï¼ä½æ¯æ¯è¼èªççå»ç·´ç¿é±è®ãå£èªªåå¯«ä½çæå·§ãèè©¦ç¶å¤©ï¼æä¸éåºçå°é±è® 29 è½å 30ï¼æ³èªªæè©²ç©©äºï¼èªªä¸å®å¯ä»¥ç ´ 110ãå çºæå£èªªèªè¦ºè¬å¾å¾é ï¼å¯«ä½æä¹åèªçè· Amazing Talker çèå¸«èªçæª¢è¨ï¼èä¸ç·´ç¿ææå¯«éé¡ä¼¼çé¡ç®ãèª°ç¥éçµæåºä¾ä¸å¦èªå·±é æï¼ä½ä¹å¤ æä»ç³è«äºï¼å°±ä¸åèäº ð¥´ã</p>
<p>èå® TOEFL ä¹å¾çæå 7, 8 æéå§èæ SoP åçãé¸æ ¡é¸ programï¼éæè©¢åæ¨è¦ä¿¡ãééç¨ä¸­æä¹éå§åæ³ä¸¦æ´çï¼æç©¶ç«åæ­¡ Computer Science çä»éº¼ï¼çºä»éº¼æ³è®ç ç©¶æï¼å¯è½å çºæ²æå·¥ä½ï¼æä»¥æç¹å¥å¤çæéå¯ä»¥è¡æäºæ³ãä¹å çºææéæ³ï¼æä»¥æç¢ºç¥éèªå·±æ¯åæ­¡ç ç©¶åºå±¤ç³»çµ±ç¥è­çï¼å°¤å¶æ¯ç·¨è­¯å¨ãä½æ¥­ç³»çµ±ãè¨ç®æ©çµæ§ï¼ä¸»è¦æ¯å çºä¸ååå ãç¬¬ä¸ï¼è½å¤ äºè§£å¾å¤ä»¥åç¨çæç¨ç¨å¼çåºå±¤éè¼¯ï¼å°æä¾èªªæ¯éå¸¸éå¿çãç¬¬äºï¼å çºè¦ºå¾ç¾å¨å¾å¤åè²»ç·ä¸èª²ç¨ï¼å¾å¤äººé½å¯ä»¥èªå­¸ç¶²é åå¾ç«¯ï¼æä»¥ç«¶ç­æ¿çãå¾åºå±¤ç³»çµ±èµ°ï¼æè¨±å¯ä»¥è·å¤§å¤æ¸äººååºå·®ç°æ§ãç®åæ¯éå¾å°è½å°æäººè½é åæ¯æç´æ¥è½ä¾å¯«ç³»çµ±è»é«çãç¬¬ä¸ï¼æªä¾å¹¾å¹´æè¨± AI éæ¯æå¾å¤¯ï¼ç¼å± AI ææå¾å¤ç¡¬é« + è»é«çç¡¬éæ±ãå¤§å¬å¸é½æéè¦æç·¨è­¯å¨ãä½æ¥­ç³»çµ±ãè¨ç®æ©çµæ§ç¸éå°æ¥­ç¥è­çäººæä¾ç ç¼èªå®¶ç AI å¹³å°ãç¡¬é«ãè»é«ç­ç­ãæå¾ï¼åè¨­å­¸äºéäºæ±è¥¿éæ¯æ¾ä¸å°å·¥ä½ï¼éæ¯ææ©æåä¾è½å¯«ç¶²é åå¾ç«¯çï¼ç®æ¯æçååæ¡ãç±ä¸èä¸æï¼ç±ä¸èä¸é£ ð§ã</p>
<p>2023 å¹´ 12 ææå°±å·²ç¶éåºææå­¸æ ¡çç³è«ï¼å çºç¶æéæä¿®å©éèª²ï¼æä»¥ææ«æ³è¦çæ´å¤æéèªçè®æ¸ãåé ­çééç¨ï¼ææ¾ç¶ä¹ææ³éå¦ææçå¨ä¸ä¸éå¬å¸ï¼èªªä¸å®æå¯ä»¥å­å°å¾å¤é¢ï¼è²·è»è²·æ¿ä¹é¡çãä½éæ¨£çäººçå¤ç¡è¶£ï¼é¢åè³ºå°±è¡ï¼å¤ ç¨å°±å¥½ãéæ¬¡æºåéç¨çäºå¾å¤ PTT, Dcard, Reddit, ä¸çä¸åå°å¿å¾æç« ãå¾å¸¸è¦çå°æäººèªªï¼ã CS æ«ç­è»éèµ°äºï¼å·¥ä½ä¸å¥½æ¾äºãç¾å¨ä¾ CP å¼ä¸é«äºããéç¶çç¢ºçèµ·ä¾å°±æ¥­å¸å ´ä¸å¥½ï¼ä½æ²è©¦éæéº¼ç¥éï¼å¥ä¸å¥æèç¸å¸¸è¬çï¼Just try your best! éææèåª½å¸¸èªªçï¼ãä¸åé½æ¯æå¥½çå®æãã</p>
<h4 id="æè¬">æè¬</h4>
<p>æè¬ä¸ä½ææ Prof. C, Prof. Y, Prog. F é¡æç¶æçæ¨è¦äººãéæ¨è¦ä¿¡ä¹éå¸¸è¿éï¼ä¸ä¸å°±éåºäºãä¹è¬è¬ç¶åä¸ä½ææç¶æé¡æåæè®æé¨ç­éè®ã</p>
<p>æè¬åä¸»ç®¡ Ted é¡æç¶æçæ¨è¦äººï¼ä¸éå§è©¢åçæåï¼å¾ç½å¿«å°±ç­æäºï¼éèªªè¦å¤å°å°é½æ²åé¡ï¼</p>
<p>æè¬ç³è«æéè¢«æè©¢åéçåå­¸éæå­¸é·å§åï¼</p>
<p>æè¬æååçæ¯æï¼è½å°ä½ åèªªæ¯ææåºåè®æ¸ï¼éé¡æä¹å¾ä¾æ¾æç©ï¼ççè¶éå¿çï¼å­äºï¼</p>
<p>æè¬åµåææç Alexï¼å¨æä¸èª²æéçµ¦äºå¾å¤ç¶é©åäº«ï¼éæå¾ä¾ SoP çæ½¤ç¨¿æåã</p>
<p>æè¬å­æ¿¤å¤§å­¸é·ï¼æèº«éå¹¾ä¹å¾å°æäººæåºåçå­¸çç¶é©ï¼æè¬ä½ é¡æçµ¦æå¾å¤å»ºè­°éæç¶é©åäº«ãç¥å¤§å­¸é·è® PhD é å©ï¼ä¹æè¬å­¸é· Medium çå¿å¾åäº«ï¼æç´æ¥æäºå­¸é·çæ¨¡æ¿ä¾æ¹ ðã</p>
<p>æè¬å®¶äººä¸ç´ä»¥ä¾çæ¯æï¼</p>
<p>æè¬å¥³åï¼</p>
<p>æè¬èªå·±ï¼</p>
]]></content:encoded>
    </item>
  </channel>
</rss>
