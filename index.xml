<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title> :)</title>
    <link>https://leewei.co/</link>
    <description>Recent content on  :)</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <managingEditor>lee10202013@gmail.com (Lee Wei)</managingEditor>
    <webMaster>lee10202013@gmail.com (Lee Wei)</webMaster>
    <copyright>Â© 2024 Lee Wei</copyright>
    <lastBuildDate>Tue, 07 Jan 2025 16:38:37 -0700</lastBuildDate>
    <atom:link href="https://leewei.co/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Course Review: CS 6460 Operating Systems</title>
      <link>https://leewei.co/cs6460/</link>
      <pubDate>Tue, 07 Jan 2025 16:38:37 -0700</pubDate><author>lee10202013@gmail.com (Lee Wei)</author>
      <guid>https://leewei.co/cs6460/</guid>
      <description>Week 1 Time sharing: a policy for processes to take turn to use the CPU. Scheduling: choose process to run. Isolation: avoid process access other processes&amp;rsquo; data. File descriptor: an integer that maps to a file. Unix philosophy: everything is a file. Kernel maintains a file descriptor table. 0: stdin, 1: stdout, 2: stderr Week 2 fork(): create a new process. exec(): replace memory of the current process. It doesn&amp;rsquo;t clear file descriptor.</description>
      <content:encoded><![CDATA[<h3 id="week-1">Week 1</h3>
<ul>
<li>Time sharing: a policy for processes to take turn to use the CPU.</li>
<li>Scheduling: choose process to run.</li>
<li>Isolation: avoid process access other processes&rsquo; data.</li>
<li>File descriptor: an integer that maps to a file.
<ul>
<li>Unix philosophy: <em>everything is a file</em>.</li>
<li>Kernel maintains a file descriptor table.</li>
<li><code>0: stdin, 1: stdout, 2: stderr</code></li>
</ul>
</li>
</ul>
<h3 id="week-2">Week 2</h3>
<ul>
<li><code>fork()</code>: create a new process.</li>
<li><code>exec()</code>: replace memory of the current process.
<ul>
<li>It doesn&rsquo;t clear file descriptor.</li>
</ul>
</li>
<li>Pipe: redirect one process&rsquo; output into another process input.
<ul>
<li>parent: write to <code>p[1]</code>, close <code>p[0], p[1]</code></li>
<li>child: close stdin, duplicate <code>p[0]</code> close <code>p[0], p[1]</code></li>
</ul>
</li>
</ul>
<h3 id="week-3">Week 3</h3>
<ul>
<li><code>leave</code>: special instruction in x86, which return the old <code>ebp</code>.</li>
<li>Why do we need stack frames? They are not strictly required, but it is good to have them.
<ul>
<li>Stack contains return addresses of caller function.</li>
</ul>
</li>
<li><code>eax, edx</code>: the return value.</li>
<li><code>ebp</code>(frame pointer): points to the base of the frame.</li>
<li>variables:
<ul>
<li>Global variables: initialized(data section), uninitialized(BSS).</li>
<li>Dynamic variables: allocated on Heap memory.</li>
<li>Local variables: stack.</li>
</ul>
</li>
</ul>
<h3 id="week-4">Week 4</h3>
<ul>
<li>Linking: combines multiple object files into an executable or a library.
<ul>
<li>Pros</li>
<li>We can write our programs in modules.</li>
<li>Faster code compilation, since we only need to re-compile changed source files and link them to the final target.</li>
<li>Space efficient, since we can share common code.</li>
</ul>
</li>
<li>Loading: load executable into memory.</li>
<li>Relocation: merge sections of each object files into multiple sections in the final executable. Resolve any unknown memory addresses.</li>
<li>ELF format
<ul>
<li>Program header table: used by loader to load each segments into memory.</li>
<li>Section header table: used by linker to link code and data sections together.</li>
</ul>
</li>
<li>Statically linked: library is linked into the executable, which makes the size of the file larger.</li>
<li>Dynamically linked: library is loaded at runtime, which makes the size of the file smaller.</li>
<li>Position independent code(PIC): generate code in such a way that it can work no
matter where it is located in the address space.
<ul>
<li>Add additional layer of indirection for all references to global data, imported functions.</li>
<li>Global Offset Table(GOT): a table, which maintains by the linker, that stores the addresses of variables.</li>
</ul>
</li>
</ul>
<h3 id="week-5">Week 5</h3>
]]></content:encoded>
    </item>
    <item>
      <title>Course Review: ECE 6545 Deep Learning with Image Analysis</title>
      <link>https://leewei.co/ece6545/</link>
      <pubDate>Tue, 07 Jan 2025 16:38:37 -0700</pubDate><author>lee10202013@gmail.com (Lee Wei)</author>
      <guid>https://leewei.co/ece6545/</guid>
      <description>Week 1 Object detection: boundary of the object, what is the object, where is the object. Semantic Segmentation: labels different sections. Linear Classifier: draw a line in a space to classify different types of data. Overfitting: the model matches the training set too closely, resulting in the model failing to predict correctly on new data. Image Classification challenges: resolution of image variables It is common to have more training data than testing data.</description>
      <content:encoded><![CDATA[<h3 id="week-1">Week 1</h3>
<ul>
<li>Object detection: boundary of the object, what is the object, where is the object.</li>
<li>Semantic Segmentation: labels different sections.</li>
<li>Linear Classifier: draw a line in a space to classify different types of data.</li>
<li>Overfitting: the model matches the training set too closely, resulting in the model failing to predict correctly on new data.</li>
<li>Image Classification challenges:
<ul>
<li>resolution of image</li>
<li>variables</li>
</ul>
</li>
<li>It is common to have more training data than testing data.</li>
<li>Class Imbalance: certain class only has limited amount of data.</li>
<li>K nearest neighbor classifier: find closest resemblance.
<ul>
<li>It is never used due to <strong>slowness</strong>, <strong>overfitting</strong></li>
</ul>
</li>
<li>Hyperparameter: parameters that are fixed during training.
<ul>
<li><code>k</code> in K nearest neighbor classifier is a hyperparameter.</li>
<li>k is usually a odd number to avoid ties when it comes to voting.</li>
</ul>
</li>
<li>Linear Decision boundary: a straight line, plane, or hyperplane that separates different classes in a feature space.</li>
</ul>
<h3 id="week-2">Week 2</h3>
<ul>
<li>Linear Regression: a line that separates different types of data.
<ul>
<li>Under mild condition, linear regression has an optimal solution.</li>
</ul>
</li>
<li>Mean Squared Error (MSE): Average of the squared differences between observed and predicted values.
<ul>
<li>Good for linear regression.</li>
</ul>
</li>
<li>Supervised Learning: train model with training set and maps input to output while minimizing errors.</li>
<li>How to find the minimum with reference to <code>w</code>?
<ul>
<li>Differential MSE with w = 0</li>
</ul>
</li>
<li>Polynomial Regression: a curve line that separates different data.</li>
<li>Machine Learn Assumption: training set is drawn from the same probability distribution as test data.
<ul>
<li>Example: train a model based on the heights of 6 - 12 years olds, but the test data are the heights of 18 - 24 years olds. The model will not generalize well.</li>
<li><strong>Ultimate Goal:</strong> has as small errors as possible.</li>
</ul>
</li>
<li>Regularization is a technique used in machine learning to prevent overfitting by introducing additional constraints or penalties to the model&rsquo;s loss function.</li>
<li>Maximum Likelihood Estimation: find the parameter that maximizes the likelihood of the observed data under a given probabilistic model.
<ul>
<li>MLE estimates often converge to the expected value of the true parameter.</li>
<li>MLE is found by taking the derivative of the log-likelihood and solving for zero.</li>
<li>MLE is asymptotically unbiased but may be biased in small samples.</li>
<li>MLE has the lowest variance possible asymptotically (efficient estimator).</li>
<li>MLE is equivalent to minimizing KL divergence(minimize between 2 distributions).</li>
</ul>
</li>
<li>Binary Classification: predicting between two classes.</li>
<li>Cross-Entropy Loss: Measures the difference between predicted and actual labels. It ensures that high-confidence incorrect predictions get large gradients (forcing corrections).</li>
<li>Squash Function (Sigmoid): Converts raw scores to probabilities (0 to 1).
<ul>
<li>Divide each output by the sum of all outputs. What happens if the sum is negative? Exponential.</li>
</ul>
</li>
</ul>
<h3 id="week-3">Week 3</h3>
<ul>
<li>SoftMax: transforms outputs into probabilities, ensures probabilities sum is 1.</li>
<li>ReLU(Rectified Linear Unit): ReLU(x) = max(x, 0). It removes any negative values and keep positive values.</li>
<li>Goal: use neurual network to linear separate samples.
<ul>
<li>The more hidden layers you have, a much larger set of problems you can approximate.</li>
<li>Don&rsquo;t put sigmoid functions in the middent of the hidden layers, but it can be used on output layers.</li>
</ul>
</li>
<li>Loss functions:
<ul>
<li>MSE: regression</li>
<li>BCE(Binary Cross Entropy): binary classification</li>
<li>Cross Entropy: multi-class labels</li>
</ul>
</li>
<li>Several approach for training neurual networks:
<ul>
<li>Batch Descent</li>
<li>Stochastic gradient descent: one sample at a time(epoch one iteration), converge faster.</li>
<li>Mini Batch: each epoch is limited to B samples.</li>
</ul>
</li>
<li>Computational Graph</li>
</ul>
<h3 id="week-4">Week 4</h3>
<ul>
<li>Forward Pass: input data moves through data in a neurual network.</li>
<li>Backward Pass (Backpropagation): computing gradients using the chain rule.</li>
<li>Weight Updates: adjusting weights based on the gradients using optimization techniques like Stochastic Gradient Descent (SGD).</li>
<li>Activation Functions: Non-linearity in hidden layers (e.g., ReLU, sigmoid).</li>
<li>Batch Processing: concepts of minibatch to speed up the process.</li>
</ul>
]]></content:encoded>
    </item>
    <item>
      <title>Course Review: CS 6810 Computer Architecture</title>
      <link>https://leewei.co/cs6810/</link>
      <pubDate>Thu, 05 Dec 2024 20:47:54 -0600</pubDate><author>lee10202013@gmail.com (Lee Wei)</author>
      <guid>https://leewei.co/cs6810/</guid>
      <description>Week 1 Introduction and metrics
Week 2 Metrics and ISA
Week 3 To improve the performance of a processor, we introduce a technique called Pipelining. Pipelining splits instructions into multiple stages. Ideally, Throughput increases by a factor of # of the stages.
Pipeling are usually 5 stages: IF, ID, EXE, MEM, WB.
Instruction Fetch(IF): fetch instruction from memory and PC = PC + 4. Instruction Decode(ID): read registers from register file and sign extension for immediate value.</description>
      <content:encoded><![CDATA[<h3 id="week-1">Week 1</h3>
<p>Introduction and metrics</p>
<h3 id="week-2">Week 2</h3>
<p>Metrics and ISA</p>
<h3 id="week-3">Week 3</h3>
<p>To improve the performance of a processor, we introduce a technique called <code>Pipelining</code>.
<code>Pipelining</code> splits instructions into multiple stages. Ideally, Throughput increases by a factor of # of the stages.</p>
<p>Pipeling are usually 5 stages: IF, ID, EXE, MEM, WB.</p>
<ol>
<li>Instruction Fetch(IF): fetch instruction from memory and PC = PC + 4.</li>
<li>Instruction Decode(ID): read registers from register file and sign extension for immediate value.</li>
<li>Execution(EXE): execute the instruction with one input register 0 and either register 1 or immediate value. Computing branch can also be in this stage.</li>
<li>Access Memory(MEM): execute load or store instruction.</li>
<li>Write Back(WB): write value back to register file.</li>
</ol>
<p>Each stage has a buffer that passes information to the next stage. These buffers are controlled by controlling signals.</p>
<p>One problem for pipelining is to balance the clock period of each stage since the lowest circuit delay determines the clock cycle.</p>
<h3 id="week-4">Week 4</h3>
<p>Pipeline Hazards are events that restrict the pipeline flow.</p>
<ol>
<li>Structural Hazard: resource conflicts. For instance, processor with one memory unit could have structural hazard when fetching instruction and executing memory instruction at the same time.</li>
<li>Data Hazard:</li>
<li>Control Hazard:</li>
</ol>
<p>Static branch predictor: fixed prediction.</p>
<h3 id="week-5">Week 5</h3>
<p>Scoreboarding is a technique for allowing instructions to execute out of order when there are sufficient resources and no data dependences. It utilizes in-order issues.</p>
<p>Scoreboarding limitation</p>
<ul>
<li>Structural hazard: functional units are busy for the current instruction.</li>
<li>Resolving WAW, RAW, WAR with stalls.</li>
<li>Registers are only read when they are both available.</li>
</ul>
<p>Main idea:</p>
<ul>
<li>Split ID into two stages:
<ul>
<li>Issue: decode instruction, check for structural hazard and WAW.</li>
<li>Read operands: wait until no RAW hazard, read data from registers.</li>
</ul>
</li>
<li>Execution</li>
<li>Write Back: check for WAR</li>
</ul>
<h3 id="week-6">Week 6</h3>
<p>Tomasulo&rsquo;s Algorithm</p>
<ul>
<li>Reservation stations: a buffer infront of every function unit, so that processor doesn&rsquo;t stall when there&rsquo;s structural hazard.</li>
<li>Register renaming: read value for operands without reading from register file. These renaming data are stored in the rename table.</li>
<li>A common data bus(CDB) connects every reservation station.</li>
</ul>
<p>Tomasulo limitation</p>
<ul>
<li>Branch stall execution. Tomasulo doesn&rsquo;t allow branch prediction. It waits until branch is resolved.</li>
<li>Loads and stores are performed in order.</li>
</ul>
<p>How can we support branch prediction - speculation execution</p>
<p>Multi-issue processors</p>
<ul>
<li>Superscalar: instructions are chosen dynamically by the hardware.</li>
<li>VLIW: instructions are chosen statically by the compiler. Intel Itanium</li>
</ul>
<p>For Tomasulo algorithm, we cannot tell which instructions are after branch instruction due to out-of-order execution.</p>
<ol>
<li>Identify instructions after the branch.</li>
<li>Exception in specualtive code should be buffered before actually raising the exception.</li>
<li>Precise exception: when a exception is raised, all instructions after the exception are squashed.</li>
</ol>
<p>Add a reorder buffer to keep track the original order when issuing instructions.</p>
<p>Issue inorder -&gt; Execute out-of-order -&gt; Commit inorder</p>
<h3 id="week-7">Week 7</h3>
<p>Instruction can only be fetched when a branch is resolved.</p>
<p>Why do we need reservation station when we have reorder buffer?</p>
<p>reorder buffer holds output
reservation station buffers input</p>
<p>Tomasulo with Hardware Speculation</p>
<p>Issue -&gt; Execute -&gt; Write Result(ROB) -&gt; Commit</p>
<p>Trace cache</p>
<p>Midterm review: all until superscalars</p>
<p>Macro-op fusion: Fuses simple instruction combinations to reduce instruction count, kind of like Peephole optimization.</p>
<p>Practical limitations to ILP: programs can only have a certain level of concurrency</p>
<h3 id="week-9">Week 9</h3>
<p>Midterm review + Midterm</p>
<h3 id="week-10">Week 10</h3>
<p>Temporal Locality: recent memory access will have higher chances to be accessed again.
Spatial Locality: locations near the cenet memory access will have higher chances to be accessed.</p>
<p>SRAM: cache
DRAM: Memory</p>
<p>Cache Block placement</p>
<ul>
<li>Fully Associative(one set): block can go any where.
<ul>
<li>Have lower miss rate.</li>
<li>Must search the whole cache to find the block.</li>
</ul>
</li>
<li>Direct Mapped: block can only go to location <code>mod blocksize</code>.
<ul>
<li>Simplest approach.</li>
<li>Blocks map to the same location, resulting in higher miss rate.</li>
<li>Only have one replacement policy.</li>
</ul>
</li>
<li>Set Associative: n-way Associative, each set can have at most n blocks.
<ul>
<li>Higher level caches: 2- or 4-way common (faster search).</li>
<li>Lower level caches: 8- to 32-way common.</li>
</ul>
</li>
</ul>
<p>Cache Block Identification: Tag - Index - Block Offset</p>
<ul>
<li>Example: Cache 32 KBytes, 2-way, 64 Bytes per line, Address 32 bits
<ul>
<li>0x000249F0 = (0000 0000 0000 0010 0100 1001 1111 0000)_2</li>
<li>Block offset = log_2 64 = 6 bits</li>
<li>Index = log_2(32K / 64 / 2 (2-way)) = 15 - 6 - 1 = 8 bits</li>
<li>Tag = 32 - 8 - 6 = 18 bits</li>
</ul>
</li>
</ul>
<p>Eviction Methods: which cache block to evict?</p>
<ul>
<li>Random</li>
<li>Least-recently-used(LRU)</li>
<li>Not-recently-used(NRU): any cache block other than most-recently-used.</li>
</ul>
<p>Inclusive cache</p>
<ul>
<li>lower level cache has a copy of every block in higher-level caches.
<ul>
<li>pros: in parallel systems, if lower-level cache is not presented, system doesn&rsquo;t need to search higher level cache.</li>
<li>cons: need to evict each level&rsquo;s cache block if a cache block is evicted.
Exclusive cache</li>
</ul>
</li>
<li>each level has dintict cache blocks.
<ul>
<li>pros: efficient use of space since there is no duplicate cache block.</li>
<li>cons: cache coherence across different processors.</li>
</ul>
</li>
</ul>
<p>Average Memory Access Time(AMAT) = Hit time + Miss rate * Miss penalty = Hit rate * Hit time + Miss rate * Miss time</p>
<ul>
<li>Hit time is always there because whether the block we&rsquo;re trying to access is in the cache, we will need to check the cache.</li>
</ul>
<p>Techniques for reducing hit time</p>
<ul>
<li>Victim Cache: stores blocks that are evicted from L1. (Also reduces Miss rate or Miss penalty)</li>
</ul>
<p>Techniques for reducing miss penalty</p>
<ul>
<li>Early restart: request words in normal orde and send requested word to processor as soon as it arrives, not wait until cache line is filled.</li>
<li>Critical word first: request the exact word from memory and sends requested word to processor as soon as it arrives.</li>
<li>Merging write buffer: CPU only stalls on write when write buffer full. (Write may overtake early write)</li>
</ul>
<p>Cache Miss Types</p>
<ul>
<li>Compulsory(Cold) miss: when a block is accessed for the first time.</li>
<li>Capacity miss: cache was evicted due to capacity.</li>
<li>Conflict miss: cache was evicted due to capacity of set. (Fully associative does not have this miss type)</li>
</ul>
<p>Reducing Cold Miss Rates</p>
<ul>
<li>Large block size</li>
<li>Prefetch: speculate future instr/data accesses and fetch them into cache. Prefetch should not be late or too early.
<ul>
<li>Hardware prefetch: sequential and strided prefetching. Stream buffer(works well for instruction caches) to prevent cache pollution.</li>
<li>Software prefetch: explicit prefetch instructions. Software prefetch with loop unrolling or software pipeline.
<ul>
<li>Restricted to loops with array accesses and it&rsquo;s hard to get right.</li>
</ul>
</li>
</ul>
</li>
<li>High associativity caches</li>
</ul>
<p>Basic Cache Optimization(+ improvement, - worse)</p>
<ul>
<li>Larger block size: + Miss rate, - Miss penalty. Doesn&rsquo;t affect Hit time and power consumption.</li>
<li>Bigger cache: + Miss rate(improves capacity misses), - Hit time, - Power.</li>
<li>High associativity: + Miss rate(improves conflict misses), - Hit time, - Power.</li>
<li>Multilevel caches: + Miss penalty(data might be found in L2 cache).</li>
<li>Give priority to read misses: read misses and there are write misses in the write buffer, read misses are handled first. + Miss penalty.</li>
<li>Avoid virtual to physical address translation lookup: + Hit time.</li>
</ul>
<p>Compiler Optimizations:</p>
<ul>
<li>Instruction reordering: reduce conflict misses</li>
<li>Data reordering: 2 arrays v.s. struct, loop interchange(swap nested loops to access memory in sequential ), loop fusion, blocking(access blocks of data)</li>
</ul>
<h3 id="week-11">Week 11</h3>
<p>Virtual Memory</p>
<ul>
<li>Programs are too large to fit in physical memory. A method to share physical memory for a large amount of processes.</li>
<li>Each process has its own, full, address space.
<ul>
<li>Virtual Memory: pages</li>
<li>Physical Memory: frames</li>
</ul>
</li>
<li>Recently not used pages may be swapped to disk. (Swap disk)</li>
<li>Virtual Memory miss: page fault. Page not in memory, so OS needs to retrieve pages from disk(very slow).</li>
</ul>
<p>Page Table</p>
<ul>
<li>OS maintains a table that maps all virtual pages to physical page frames.</li>
<li>One PT per process(page table register points to the Page table of the current process) and one for the OS.</li>
<li>Memory is fully associative.</li>
<li>OS maintains a list of free frames.</li>
</ul>
<p>Page Table stores info for translating virtual page number to physical page number.</p>
<p>Methods to make Page Tables space-efficient</p>
<ul>
<li>Inverted page table: a hash table that maps physical and virtual pages.</li>
<li>Hierarchical page table: n-level page table. Only the N-th level page table has the value of physical frames.</li>
</ul>
<p>Paging means that every memory access involves 2 memory accesses: 1. get physical address 2. get data from physical address.</p>
<p>What can we do to make paging faster?</p>
<p>Translation Lookaside Buffer</p>
<ul>
<li>A full-associative(Content Addressable Memory, CAM) cache of Page Table entries. Parallel lookup.</li>
<li>Every entry has many bits(Valid, R/W, User/Supervisor, Dirty, Access), a tag, a data(physcial page number).</li>
</ul>
<p>Virtually tagged problems</p>
<ul>
<li>Synonyms(alias problem): different virtual addresses points to the same physical address.
<ul>
<li>Write to copy 1 would not be reflected in copy 2.</li>
</ul>
</li>
<li>Homonyms: same virtual addresses different physical address due to process switching.
<ul>
<li>Possible solution: 1. flush cache on context switch(increase miss rate) 2. add PID to cache tag.</li>
</ul>
</li>
</ul>
<p>Methods to address a cache in a virtual-memory system</p>
<ul>
<li>Physically Indexed, physically tagged: translation first, increasing L1 hit time.</li>
<li>Virtually Indexed, virtually tagged: cannot distinguish synonyms/homonyms in cache.</li>
<li>Virtually Indexed, physically tagged: L1 cache indexed virtual address, tags can be checked after translation.</li>
<li>Physically Indexed, virtually tagged: not practical.</li>
</ul>
<p>Does physically indexed, physically tagged mean TLB and cache have to be accessed sequentially? Not if PageSize &gt; #Sets * BlockSize</p>
<h3 id="week-12">Week 12</h3>
<p>Motivation for multicores</p>
<ul>
<li>Power wall, ILP wall</li>
</ul>
<p>Parallel architecture = computing model + communication model</p>
<ul>
<li>Computing model: organization of cores and how data is processed</li>
<li>Communication model: how cores communication?
<ul>
<li>Shared memory: explicit synchronization(via loads and stores)</li>
<li>Message passing: implicit synchronization(via messages)</li>
</ul>
</li>
</ul>
<p>Multicore processors</p>
<ul>
<li>Uniform memory access (UMA): physically centralized memory -&gt; Symmetric Multiprocessor(SMP)</li>
<li>Non-Uniform memory access (NUMA): physically distributed memory -&gt; Distributed Shared-Memory</li>
</ul>
<p>Communication Model</p>
<ul>
<li>Threads communication is done through shared memory variables</li>
<li>Explicit data synchronization, done by the developers</li>
</ul>
<p>The main goal for Cache Coherence is to make caches invisible.</p>
<p>Single Write Multiple Reader</p>
<ul>
<li>Write Propagation: writes are eventually visible in all processors.</li>
<li>Write Serialization: writes are in the same order in all processors.</li>
</ul>
<p>Cache Coherence Protocol: keep track of what processors have copies of what data.</p>
<ul>
<li>Invalidate protocols: get rid of data with old values, usually used with write-back caches.
<ul>
<li>+: multiple writes to the cache block only require one invalidation.</li>
<li>+: less bandwidth since it doesn&rsquo;t need to send new value of the data.</li>
<li>-: write-back data to memory when evicting a modified block.</li>
</ul>
</li>
<li>Update protocols: update every caches&rsquo; copy of data, usually used with write-through caches.
<ul>
<li>+: new value can be re-used without the need to ask for it again.</li>
<li>+: data can always be read from memory.</li>
<li>-: possible multiple useless updates.</li>
</ul>
</li>
</ul>
<p>How can cache coherence protocols be implemented?</p>
<ul>
<li>Software coherence: programmer or compiler-controlled</li>
<li>Hardware coherence: add state bits to cache lines to track state of the line.
<ul>
<li>Exclusive state: block is cached only in this cache, has not been modified, but can be modified without permission. Freely modify and upgrade modified state.</li>
</ul>
</li>
</ul>
<h3 id="week-13">Week 13</h3>
<p>Cache Coherence Protocol Implementations</p>
<ul>
<li>Snooping: all cache controllers monitor all other caches&rsquo; activites and maintain the state of their lines.
<ul>
<li>Information is shared in a common bus. Bus does not scale well.</li>
<li>Each cache has a bus-side controller that monitors all transactions.</li>
<li>Two types of snooping: 1. write invalidation 2. write update</li>
</ul>
</li>
<li>Directory: a central control device directly handles all cache activies.
<ul>
<li>Directory acts as a serialization to provide ordering.</li>
</ul>
</li>
</ul>
<p>MSI Protocol</p>
<ul>
<li>Invalid: block is not present. Need to fetch it from memory or other cache.</li>
<li>Shared: in &gt; 1 caches.</li>
<li>Modified: in 1 cache. Processor can read/write directly.</li>
</ul>
<p>MESI Protocol has one more Exclusive state.</p>
<p>Coherence misses: when a block is not in the cache because it was invalidated by a write from another processor.</p>
<ul>
<li>Hard to reduce due to communication and sharing of data in parallel application.</li>
<li>False sharing: processor modify different words of the cache block but end up invalidating the complete block.
<ul>
<li>False sharing coherence misses increase with larger cache line size.</li>
</ul>
</li>
</ul>
<p>Problems for snooping on a common shared bus</p>
<ul>
<li>When should memory provide data?</li>
<li>What if we need to Write-back?</li>
<li>Conflict when processor and bus-side controller check the cache</li>
<li>State transitions may require several steps</li>
<li>What to do if there are conflicting requests(race conditions) on the bus?
<ul>
<li>Transient states</li>
</ul>
</li>
</ul>
<p>Problems for snooping with multi-level hierarchies</p>
<ul>
<li>Processor interacts with L1 while bus-side controller interacts with L2.
<ul>
<li>Inclusive cache and M state caches in L1 must also be in L2</li>
<li>Propagate all transactions to L1</li>
</ul>
</li>
</ul>
<h3 id="week-14">Week 14</h3>
<p>Snooping implementation has bottleneck at the common data bus. Thus we introduce snooping with split-transaction buses.</p>
<ul>
<li>Create buffer for each cache to hold pending transactions.</li>
<li>Send <code>negative acknowledgement (NACK)</code> when buffers are full.</li>
<li>Snooping with Ring can enforce write serialization with home node. If there are multiple racing writes, ties are broken via the home node.</li>
</ul>
<p>Directory contains a line state and sharing bit-vector.</p>
<ul>
<li>Line state: invalid(00), shared(01), modified(10)</li>
<li>Sharing vector: not cached(00), shared(01)</li>
</ul>
<p>Directory operation</p>
<ul>
<li>It is necessary to collect all acknowledgements(ACK) with write that has multiple sharers.</li>
<li>Complex state changes, directory must also receive ACK.</li>
</ul>
<p>Implementation difficulties for direcotry operation</p>
<ul>
<li>Operations have to be serialized locally.</li>
<li>Operations have to be serialized at directory.</li>
</ul>
<p>Directory Overhead grows with number of cores.</p>
<ul>
<li>Baseline overhead: (number of cores + 1 dirty bit / cache block size * 8 bits)</li>
<li>Cached Directories</li>
<li>Limited Pointer Directories</li>
</ul>
<p>Distributed Directories</p>
<ul>
<li>Local, Home, Remote nodes.</li>
</ul>
<p>Memory Consistency is a specification, which specifies the order of loads and stores.</p>
<ul>
<li>Memory Consistency model is governed by 1. Core pipeline(memory reorder) 2. Coherence Protocol</li>
</ul>
<p>Sequential Consistency(SC): 1. Result should be the same in a time-shared multiprocessor 2. Relative order should be maintained in one thread</p>
<ul>
<li>Sequential Consistency is what should load really happens.
<ol>
<li>Threads issue memory operations in program order</li>
<li>Before issuing next memory operation threads wait until last issued memory operation completes (i.e., performs w.r.t. all other processors)</li>
<li>A read is allowed to complete only if the matching write (i.e., the one whose value is returned to the read) also completes</li>
</ol>
</li>
</ul>
<p>Issue: memory operation leaves the processor and becomes visible to the memory subsystem.
Performed: memory operation appears to have taken place.</p>
<ul>
<li>Performed with reference to processor X: performed to processor X.</li>
<li>Globally performed or complete: performed to all processors.</li>
</ul>
<p>Merging write buffer executes memory operations in the following sequence:</p>
<ul>
<li>write foo(200)</li>
<li>write A(400)</li>
<li>write flag(204)</li>
<li>write bar(404)</li>
</ul>
<p>foo and flag will be written to memory before A and bar.</p>
<p>Write-serialization: per variable. Write to same location by different processors are seen in same order by all processors.
Write-atomicity: across threads</p>
<p>In-window Speculation:</p>
<ul>
<li>Speculation: read from cache before commiting. If no change, then commit; If there&rsquo;s a change, then squash and replay.</li>
<li>Write-prefetcing: obtain read-exclusive out-of-order or in parallel. However, the write should be in program.</li>
</ul>
<h3 id="week-15">Week 15</h3>
<p>Relaxed Memory Consistency Models:</p>
<ul>
<li>Total Store Ordering (TSO): relaxes W -&gt; R. Not guarantee because there could be a store buffer.</li>
<li>Partial Store Ordering (PSO): relaxes W -&gt; R and W -&gt; W.</li>
<li>Relaxes Memory Ordering (RMO): relaxes all four memory orders.</li>
<li>Release Consistency (RC): relaxes all four memory orders but provides release store and acquire load.
<ul>
<li>Reads and writes are allowed to bypass both reads and writes.</li>
<li>Previous reads and writes must complete before release completes.</li>
<li>No reads and writes can complete before acquire completes.</li>
</ul>
</li>
<li>IBM Power: relaxes all four memory orders and write atomicity. Provides 2 types of barriers.</li>
</ul>
<p>Every relaxed consistency model ensures single thread dependencies.</p>
<p>Release Consistency:</p>
<ul>
<li>Writer-initiated invalidation</li>
<li>Without Writer-initiated invalidation</li>
</ul>
<p>Out-of-thin-air problem</p>
<p>Progress Axiom: a store should be eventually visible for all processors.</p>
<p>Synchronization is necessary to ensure that operations in a parallel program happen in the correct order.</p>
<ul>
<li>Conditional Variable: signal</li>
<li>Mutual exclusion</li>
</ul>
<p>Without memory consistency model, we cannot implement different types of synchronization.</p>
<p>Can Sequential Consistency implement mutually exclusion?</p>
<ul>
<li>Yes. Peterson algorithm, but it is not practical for multiple processors.</li>
<li>For Relaxes models need to use fences.</li>
</ul>
<p>Building blocks for synchronization. Special instructions(RMW atomic) of the hardware to implement locks.</p>
<ul>
<li>Test &amp; set: reads a memory location and sets it to value 1.</li>
<li>Compare &amp; swap: it is used more frequently. Check the value and swap if the value is equal.</li>
<li>Fetch &amp; add: fetch value from memory location and atomically increment it.</li>
<li>Load Link(or Load Locked)/Store Conditional(LL/SC): don&rsquo;t need to retain exclusive state.</li>
</ul>
<h3 id="week-16">Week 16</h3>
<p>Implementation of Read Modified Write(RMW) instructions:</p>
<ul>
<li>Lock the bus: disallows other threads</li>
<li>Cache line blocking: to obtain read-exclusive state, invalidates other processors&rsquo; cache. Once a processor gains exclusive state,
other processors will receive NACK from the processor that has exclusive access.</li>
</ul>
<p>Exclusive Access</p>
<ul>
<li>Directory retries when receives NACK</li>
<li>With Snooping, processors retires when receives NACK</li>
</ul>
<p>RMW acts like a memory fence(flush write buffer before RMW).</p>
<p>Techniques for reducing test and set traffic:</p>
<ul>
<li>Test and test-&amp;-set relies on cache coherece. It grabs a lock one time.</li>
<li>Test and set with exponential back-off, retry test and set after pause.</li>
</ul>
]]></content:encoded>
    </item>
    <item>
      <title>Course Review: CS 6520 Programming Languages</title>
      <link>https://leewei.co/cs6520/</link>
      <pubDate>Thu, 05 Dec 2024 20:47:48 -0600</pubDate><author>lee10202013@gmail.com (Lee Wei)</author>
      <guid>https://leewei.co/cs6520/</guid>
      <description>Week 1 Overview of this course. Shplait programming language basic overview.
Development template
Types: data representation Tests: write function signature and tests Template: write the layout of the function body Body: finish function body case-by-case Homework: Familiar with Shplait.
Week 2 Introduction of Shplait lists.
Homework: Natural recursion implementation with template.
Week 3 Binding and environment
Binding: bind an expression to an variable. Environment: store the bindings. Homework: Write a interpreter for Moe programming language using Shplait.</description>
      <content:encoded><![CDATA[<h3 id="week-1">Week 1</h3>
<p>Overview of this course. <strong>Shplait</strong> programming language basic overview.</p>
<p>Development template</p>
<ol>
<li>Types: data representation</li>
<li>Tests: write function signature and tests</li>
<li>Template: write the layout of the function body</li>
<li>Body: finish function body case-by-case</li>
</ol>
<p>Homework: Familiar with <strong>Shplait</strong>.</p>
<h3 id="week-2">Week 2</h3>
<p>Introduction of <strong>Shplait</strong> lists.</p>
<p>Homework: Natural recursion implementation with template.</p>
<h3 id="week-3">Week 3</h3>
<p>Binding and environment</p>
<ul>
<li>Binding: bind an expression to an variable.</li>
<li>Environment: store the bindings.</li>
</ul>
<p>Homework: Write a interpreter for <strong>Moe</strong> programming language using <strong>Shplait</strong>.</p>
<ul>
<li>Parse: parse <strong>Moe</strong> to <strong>Shplait</strong> data representation (expression).</li>
<li>Subst: substitude identifiers with expressions.</li>
<li>Interp: interpret expressions to integer.</li>
</ul>
<h3 id="week-4">Week 4</h3>
<p>Lambda: function as values. We can also bind anonymous function to variable.
Mutable states and stores.</p>
<ul>
<li>Box: store location of the value.</li>
<li>Unbox: get the value of a given location.</li>
<li>Setbox: update the value of a given location.</li>
</ul>
<p>Homework: Implement <strong>Moe</strong> conditions and boolean. Also, implement thunks and force.</p>
<ul>
<li>Thunk: delay a computation, until it is called by force.</li>
<li>Force: evaluate a thunk expression.</li>
</ul>
<h3 id="week-5">Week 5</h3>
<p>Record: similar to <strong>C</strong>&rsquo;s struct. A record can have a list of fields and values.
Variable: mutable variable.
Fluid let: syntax sugar. Instead of changing the interpreter, we can change the parser to genereate a let form that is matching fluid let.</p>
<p>Homework: Implement <strong>Moe</strong> begin and record initialization, access and mutation.</p>
<ul>
<li>Record: implement with <strong>Moe</strong>&rsquo;s box expression.</li>
</ul>
<h3 id="week-6">Week 6</h3>
<p>Syntax sugar and encoding
Currying: a function takes an argument that returns another function takes another argument.
Midterm: The concept of box is important! Think of box as a pointer, then everything is clear.</p>






<pre tabindex="0"><code>#true = fun(x) : fun(y) : x
#false = fun(x) : fun(y) : y</code></pre>
<h3 id="week-7">Week 7</h3>
<p><code>letrec</code>, <code>mk_rec</code> for encoding recursion. However, the problem with <code>mk_rec</code> is that we assuem the right hand side as a procedure.
Use <code>fun (): ...</code> as a delay for implementing recursion.
<code>Optionof</code> has two variants: <code>none</code> and <code>some</code>.</p>
<p>Homework: Implement syntax sugar for recursive bindings, recursive function and two arguments function.</p>
<h3 id="week-9">Week 9</h3>
<p>Lazy evaluation
Continuation</p>
<h3 id="week-10">Week 10</h3>
<p>Trace continuation
Garbage Collection
Compiler</p>
<p>Homework: Implement <code>neg</code>, <code>avg</code> and support zero or multiple arguments for function call with continuation.</p>
<h3 id="week-11">Week 11</h3>
<p>Compiler
Midterm</p>
<p>Homework: Implement a compiler that translate Moe with garbage collection.</p>
<h3 id="week-12">Week 12</h3>
<p>Class
Inheritance</p>
<p>Homework: Implement instantiation, instanceof, select.</p>
<h3 id="week-13">Week 13</h3>
<p>Type checker checks types of the program before interpreting it.</p>
<p>Homework: Typechecker.</p>
<h3 id="week-14">Week 14</h3>
<p>Type checker with unify.</p>
<p>Homework: Typecheck if0 and list.</p>
<h3 id="week-15">Week 15</h3>
<p>Polymorphism</p>
<p>Homework: Parameterized over types and functions.</p>
<h3 id="week-16">Week 16</h3>
<p>Macro
Programming Language research</p>
<p>Homework: final project</p>
]]></content:encoded>
    </item>
    <item>
      <title>Course Review: CS 6475 Advanced Compilers</title>
      <link>https://leewei.co/cs6475/</link>
      <pubDate>Thu, 05 Dec 2024 20:03:46 -0600</pubDate><author>lee10202013@gmail.com (Lee Wei)</author>
      <guid>https://leewei.co/cs6475/</guid>
      <description>Week 1 Overview of this course.
Readings:
The death of optimizing compilers On Proebstingâs Law Impact of Economics on Compiler Optimization Why Do Peephole Optimizations Work? (option) Compiler Optimization Catalog Assignment:
Find a missing optimization in LLVM using this. Prove it with Alive2. Week 2 Discussions:
Is it really the dealth of optimizing compilers? No. On Proebsting&amp;rsquo;s Law. Probably cannot use -O0 as a 18 years old compiler. Impact of Economics on Compiler Optimization.</description>
      <content:encoded><![CDATA[<h3 id="week-1">Week 1</h3>
<p>Overview of this course.</p>
<p>Readings:</p>
<ul>
<li><a href="https://cr.yp.to/talks/2015.04.16/slides-djb-20150416-a4.pdf">The death of optimizing compilers</a></li>
<li><a href="https://gwern.net/doc/cs/algorithm/2001-scott.pdf">On Proebstingâs Law</a></li>
<li><a href="https://dl.acm.org/doi/pdf/10.1145/376656.376751">Impact of Economics on Compiler Optimization</a></li>
<li><a href="https://blog.regehr.org/archives/2485">Why Do Peephole Optimizations Work?</a></li>
<li><a href="https://www.clear.rice.edu/comp512/Lectures/Papers/1971-allen-catalog.pdf">(option) Compiler Optimization Catalog</a></li>
</ul>
<p>Assignment:</p>
<ul>
<li>Find a missing optimization in LLVM using <a href="https://gcc.godbolt.org/">this</a>.</li>
<li>Prove it with <a href="https://alive2.llvm.org/ce/">Alive2</a>.</li>
</ul>
<h3 id="week-2">Week 2</h3>
<p>Discussions:</p>
<ul>
<li>Is it really the dealth of optimizing compilers? No.</li>
<li>On Proebsting&rsquo;s Law. Probably cannot use <code>-O0</code> as a 18 years old compiler.</li>
<li>Impact of Economics on Compiler Optimization. Look for where the money goes.</li>
<li>Refinement is very important!</li>
</ul>
<p>Assignment:</p>
<ul>
<li>Pick a missing optimization in LLVM to implement.</li>
<li>Build LLVM and Alive2 locally.</li>
</ul>
<h3 id="week-3">Week 3</h3>
<p>Discussions:</p>
<ul>
<li>How LLVM is tested?
<ul>
<li>unittest: LLVM API tests</li>
<li>tests: regression tests</li>
<li>llvm-test-suite: benchmark tests</li>
<li>libFuzzer</li>
<li>test by users</li>
</ul>
</li>
<li>AVX512 ternary logic. It&rsquo;s difficult to decode due to its required bit space.</li>
<li>Superoptimization: To generate optimized code, superoptimizer searches for certain pattern, does refinement check and assess with its cost model.</li>
</ul>
<p>Readings:</p>
<ul>
<li><a href="https://www.cs.princeton.edu/~appel/papers/ssafun.pdf">SSA is Functional Programming</a></li>
<li><a href="https://www.cs.cmu.edu/~rjsimmon/15411-f15/lec/10-ssa.pdf">Lecture Notes on Static Single Assignment Form</a></li>
<li><a href="https://lowlevelbits.org/system-under-test-llvm/">System Under Test: LLVM</a></li>
<li><a href="https://blog.regehr.org/archives/1450">Testing LLVM</a></li>
</ul>
<p>Assignment:</p>
<ul>
<li>Implement a missing optimization in LLVM and add tests to it. <a href="https://github.com/regehr/llvm-project/pull/60">Github PR</a></li>
</ul>
<h3 id="week-4">Week 4</h3>
<p>Discussions:</p>
<ul>
<li>Compiler without SSA gets harder to get in right, and it messes up the code base.</li>
<li>Brainfuck language
<ul>
<li>Game of Life in BF</li>
</ul>
</li>
<li>Speedup interpreter with <a href="https://eli.thegreenplace.net/2012/07/12/computed-goto-for-efficient-dispatch-tables">computed goto</a>.</li>
<li>Need to be cautious when using <code>APInt</code>. There are use cases in LLVM that use 80, 320 bits.</li>
<li>Intro for <a href="https://en.wikipedia.org/wiki/Partial_evaluation">Partial Evaluation</a> and its relation to staged computation.
<ul>
<li>Abstract interpreter, approximate computations, Halting problem, Tainted cell.</li>
</ul>
</li>
<li>General optimization approach for BF.
<ul>
<li>Creating virtual instructions for BF is similar with x86-64 processors having virtual instructions that are not exposed to developers.</li>
</ul>
</li>
<li>It&rsquo;s hard to pass alias information to compilers, like using <code>restrict</code> correctly in C.</li>
</ul>
<p>Readings:</p>
<ul>
<li><a href="https://c9x.me/compile/bib/braun13cc.pdf">Simple and Efficient Construction of Static Single Assignment Form</a>: Great paper that solves real problems!</li>
<li><a href="https://arxiv.org/pdf/2305.13241">Whose baseline is it anyway?</a></li>
<li><a href="https://www.npopov.com/2023/10/22/How-to-reduce-LLVM-crashes.html">llvm-reduce</a></li>
</ul>
<p>Assignment:</p>
<ul>
<li>Implement a <a href="https://github.com/leewei05/bf">BF interpreter</a>.</li>
</ul>
<h3 id="week-5">Week 5</h3>
<p>Discussions:</p>
<ul>
<li>Always optimize with a profiler (data). Recursive to a close form.</li>
<li>C++ downcast optimization probably couldn&rsquo;t catch by the profiler.</li>
<li>ABI doc tells developer how procedures communicate. Which register to store argument.</li>
<li>If a compiler doesn&rsquo;t know a fact, sometimes we can rewrite code to teach compiler to optimize.</li>
<li>How does compiler recognize certain pattern, such as popcount, to optimize? Hardcode and do some canonicalization before finding certain patterns.</li>
<li>Approximate: tracking info accurately is hard. Instead, tools try to shrink the area of approximation.</li>
</ul>
<p>Readings:</p>
<ul>
<li><a href="https://cs.au.dk/~amoeller/spa/spa.pdf">Static Program Analysis: Ch 1</a></li>
<li><a href="https://www.agner.org/optimize/">Agner Fog</a></li>
<li><a href="https://www.corsix.org/content/whirlwind-tour-aarch64-vector-instructions">ARM SIMD</a></li>
</ul>
<p>Assignment:</p>
<ul>
<li>Implement a profiler on top of the <a href="https://github.com/leewei05/bf">BF interpreter</a>.
<ul>
<li>Simple loops: no i/o, no pointer changes, either +1, -1</li>
</ul>
</li>
<li>Implement a <a href="(https://github.com/leewei05/bf)">compiler</a> for BF that emits x86-64 assembly.</li>
</ul>
<h3 id="week-6">Week 6</h3>
<p>Discussions:</p>
<ul>
<li>Is it okay to remove infinite loop?</li>
<li>Debugging: Optimization fuel decrement one until fuel run out, binary search the commit.</li>
<li>Scan memory tricks</li>
<li>Fast program tries to handle aligned memory.</li>
<li>Lattice and semilattice are abstract values that live in the compiler.
<ul>
<li>Design goal tight enough to not run forever. In other words, having enough information for the compiler to run fast.</li>
</ul>
</li>
<li>Dataflow Analysis
<ul>
<li>Top is universal set(imprecise info). Start from the top, we will reach a point that has enough information.</li>
<li>Bottom is empty set(precise info). Start from the bottom, we will reach a fixed point.</li>
<li>Least fixed point is the point we get the most information.</li>
</ul>
</li>
</ul>
<p>Examples:</p>
<p>Dead Code Elimination(Lattice with a height of 2):</p>
<ul>
<li>Top is maybe reachable.</li>
<li>Bottom is provably unreachable.</li>
</ul>
<p>Constant Propagation(Lattice with a height of 3):</p>
<ul>
<li>Top is probably not constant.</li>
<li>Integers</li>
<li>Bottom is unreachable.</li>
</ul>
<p>Readings:</p>
<ul>
<li><a href="https://cs.au.dk/~amoeller/spa/spa.pdf">Static Program Analysis: Ch 4</a></li>
<li><a href="https://docs.oracle.com/javase/specs/jls/se23/html/jls-17.html#jls-17.4.9">Infinite Loop in Java</a></li>
<li><a href="https://www.open-std.org/jtc1/sc22/wg14/www/docs/n1528.htm">Undefined Bahvior for infinite loops</a></li>
</ul>
<p>Assignment:</p>
<ul>
<li>Optimize simple and non-simple loops in BF, fast vector implementation.
<ul>
<li>Starts with simple cases: make a tape with bunch of 1s with a 0 in it, and print the pointer of the 0.</li>
<li>Write a tiny BF program</li>
</ul>
</li>
</ul>
<h3 id="week-7">Week 7</h3>
<p>Discussions:</p>
<ul>
<li>GCC IR gimple</li>
<li>LLVM IR flat</li>
<li>In practice, we use worklist algorithm a lot. Which node inside a worklist to choose first? A node that is SCC since it can affect other nodes.</li>
<li>Transfer function table SPA</li>
<li>How can we run better benchmarks?</li>
<li>Setting up the conditions for the compilers to know more details.</li>
<li>Be a better programmer by moving redundant and loop invariant operations outside of loop.</li>
<li>Math is good for low-level programming.</li>
</ul>
<p>References for writing x86-64 vector instructions:</p>
<ul>
<li><a href="https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#ig_expand=5738,440,778,5762,5872,4606,5744,5762,5872,4606">Intel Official Intrinsic manual</a></li>
<li><a href="https://stackoverflow.com/questions/40032906/is-there-an-efficient-way-to-get-the-first-non-zero-element-in-an-simd-register">Peter Cordes on Stackoverflow</a></li>
<li><a href="https://www.felixcloutier.com/x86/">Felix Cloutier&rsquo;s x86-64 manual</a></li>
<li><a href="https://gcc.godbolt.org/z/vqd9K4rqT">One of my working examples</a></li>
</ul>
<p>Readings:</p>
<ul>
<li><a href="https://gcc.gnu.org/onlinedocs/gccint/GIMPLE.html">GIMPLE</a></li>
<li><a href="https://www.cse.wustl.edu/~jain/iucee/index.html">Computer Systems Performance Analysis</a></li>
<li><a href="https://emeryberger.com/research/stabilizer/">Sabilizer</a></li>
</ul>
<p>Videos:</p>
<ul>
<li><a href="https://www.youtube.com/watch?v=vrkR7jKcFpw">Alive2 like tool for GCC</a></li>
</ul>
<h3 id="week-9">Week 9</h3>
<p>Discussions:</p>
<ul>
<li>JIT compiler, how to encode instructions to JIT.</li>
<li>Partial evaluation</li>
<li>Dead store elimination</li>
<li>calloc</li>
<li>global, stack, heap(sticking point, people uninitialized on purpose)</li>
<li>Common subexpression elimination -&gt; Available</li>
<li>Rematerialzation</li>
<li>Very busy -&gt; LICM</li>
<li>Autotuning try a lot of things, PGO run a once and tell the compiler some information next the build.</li>
</ul>
<h3 id="week-11">Week 11</h3>
<p>Discussions:</p>
<ul>
<li>Linker script: tells the linker where memory section maps on a dev board.</li>
<li>Demanded Bits: you cannot prove that bits are not necessary.</li>
<li>LLVM Known bits.</li>
<li>Trace based JIT</li>
<li>Precompiled headers</li>
<li>Timeout is a real problem, AWS IAM Z3 solver.</li>
<li>Flaky tests: sometimes fail and success.</li>
<li>LLVM IR verifier</li>
</ul>
<p>Readings:</p>
<ul>
<li><a href="https://www.amazon.com/Move-Semantics-Complete-Guide-First/dp/3967309002/">C++ Move</a></li>
<li><a href="https://github.com/llvm/llvm-project/blob/main/llvm/include/llvm/Support/KnownBits.h">LLVM KnownBits</a></li>
</ul>
<h3 id="week-12">Week 12</h3>
<p>Discussions:</p>
<ul>
<li>Bugpoint was used in LLVM for reducing tests. It defaults use some instructions result in Undefined Behavior.</li>
<li>Trusting trust attacks.</li>
<li>Diverse Double compiling</li>
</ul>
<p>Readings:</p>
<ul>
<li><a href="https://users.cs.utah.edu/~regehr/papers/undef-pldi17.pdf">Taming Undefined Behavior in LLVM</a></li>
<li><a href="https://dwheeler.com/trusting-trust/">Diverse Double compiling</a></li>
<li><a href="https://en.wikipedia.org/wiki/Reproducible_builds">Reproducible builds</a></li>
<li><a href="https://www.cs.cmu.edu/~rdriley/487/papers/Thompson_1984_ReflectionsonTrustingTrust.pdf">Trusting trust</a></li>
</ul>
<h3 id="week-13">Week 13</h3>
<p>Discussions:</p>
<ul>
<li>Undefined Behavior is also in the hardware chips.</li>
<li>Register pair for multiplication.</li>
<li><code>undef</code> breaks SSA.</li>
<li>Trusting trust really worths reading it!</li>
<li>Java type concurrency</li>
<li>Type system is trying to help you not break the program.</li>
<li>Union find</li>
<li>HCI for expressing types</li>
<li>bitblit</li>
</ul>
<p>How to implement fast inverse square root on GCC and Clang:</p>
<ul>
<li>union</li>
<li>memcpy fast</li>
<li>char *</li>
<li>(C++)reinterpret cast</li>
</ul>
<p>Readings:</p>
<ul>
<li><a href="http://lucacardelli.name/papers/typesystems.pdf">Type Systems</a></li>
<li><a href="http://mazsola.iit.uni-miskolc.hu/~drdani/docs_arm/36_Elsevier-ARM_Sy.pdf">ARM System</a></li>
<li><a href="https://arxiv.org/pdf/2305.13241">Whose Baseline is it anyway?</a></li>
<li><a href="https://devblogs.microsoft.com/oldnewthing/20180209-00/?p=97995">BitBlip</a></li>
<li><a href="https://pdos.csail.mit.edu/~rsc/pike84bitblt.pdf">BitMap</a></li>
</ul>
<h3 id="week-14">Week 14</h3>
<p>Discussions:</p>
<ul>
<li>SMT solvers: provide them constraints to solve. If there are solutions, it will return one solution(model).
<ul>
<li>Practice: z3 sudoku solver(Int, BitVector).</li>
</ul>
</li>
<li>Use a solver for unbounded Loop is hard because we cannot unroll it.</li>
<li>People compile language to Javascript -&gt; asm.js -&gt; WebAssembly(Stack machine)</li>
<li>Baseline compiler</li>
<li>Everyone needs to know at least one language in System Programming, Scripting, Parallel Programming, Math.</li>
<li>Autovectorization for large legacy can improve significant performance.</li>
</ul>
<p>Readings:</p>
<ul>
<li><a href="https://webkit.org/blog/3362/introducing-the-webkit-ftl-jit/">Webkit JIT</a></li>
<li><a href="https://ericpony.github.io/z3py-tutorial/guide-examples.htm">Z3 Python tutorial</a></li>
<li><a href="https://www.sciencedirect.com/science/article/pii/S1877050912003304">CUDA: Compiling and optimizing for a GPU platform</a></li>
</ul>
<h3 id="week-15">Week 15</h3>
<p>Discussions:</p>
<ul>
<li>GPU compilers</li>
<li>What CPU compilers optimizations don&rsquo;t work on GPU compilers? (GPU has limited registers for each thread)
<ul>
<li>Inline</li>
<li>Loop unrolling</li>
<li>LICM</li>
<li>InstCombine</li>
</ul>
</li>
<li>Divergence problem is critical. Divergence &gt; Latency</li>
<li>Rematerialization</li>
</ul>
<p>Readings:</p>
<ul>
<li><a href="https://dl.acm.org/doi/epdf/10.1145/3528416.3530249">Compilation On The GPU? A Feasibility Study</a></li>
</ul>
<h3 id="week-16">Week 16</h3>
<p>Discussions:</p>
<ul>
<li>Compilation on the GPU</li>
<li>Go&rsquo;s generic rules compares with LLVM&rsquo;s large C++ code base.</li>
<li>Go&rsquo;s generic rules should be checked by something like Z3.</li>
<li>e-graph is good but it is probably not practical to adopt to existing compilers like LLVM or GCC. Applying rewrites might blow up memory usage.</li>
<li>Quine is fun.</li>
</ul>
<p>Readings:</p>
<ul>
<li><a href="https://go.dev/src/cmd/compile/internal/ssa/_gen/generic.rules">Go generic rules</a></li>
<li><a href="https://dl.acm.org/doi/epdf/10.1145/3434304">egg: Fast and Extensible Equality Saturation</a></li>
</ul>
]]></content:encoded>
    </item>
    <item>
      <title>Deep Learning</title>
      <link>https://leewei.co/deep-learning/</link>
      <pubDate>Sat, 21 Sep 2024 00:20:31 -0600</pubDate><author>lee10202013@gmail.com (Lee Wei)</author>
      <guid>https://leewei.co/deep-learning/</guid>
      <description>Day 1 What is Neural Network?
Neuron: holds a number between 0 and 1. The number is also called &amp;ldquo;Activation&amp;rdquo;. Network: multiple layers of neurons connected together. A layer is a series of neurons. Each layer&amp;rsquo;s output affect the neurons of the next layer.
Why the layers?
Each layer construct some parts of information for the next layer until the output layer. Each layer can be represented as a vector. Each neuron in a layer has a weight that connects to the next layer&amp;rsquo;s neuron.</description>
      <content:encoded><![CDATA[<h3 id="day-1">Day 1</h3>
<p>What is Neural Network?</p>
<ul>
<li>Neuron: holds a number between 0 and 1. The number is also called &ldquo;Activation&rdquo;.</li>
<li>Network: multiple layers of neurons connected together.</li>
</ul>
<p>A layer is a series of neurons. Each layer&rsquo;s output affect the neurons of the next layer.</p>
<p>Why the layers?</p>
<ul>
<li>Each layer construct some parts of information for the next layer until the output layer.</li>
<li>Each layer can be represented as a vector.</li>
</ul>
<p>Each neuron in a layer has a weight that connects to the next layer&rsquo;s neuron.
This weight can be seen as a parameter that help detect a certain pattern of our input.</p>
<p>A weighted sum could be any number, but we want this sum value to fit between 0 and 1.
We can utilize a Sigmoid function(old school) or Rectified linear unit.</p>
<p>Bias for inactivity, which can be subtracted to the weighted sum before inputing to Sigmoid or ReLU.</p>
<p><strong>Learning: finding the right weights and biases.</strong></p>
<h3 id="day-2">Day 2</h3>
<p>What is Gradient descent?</p>
<ul>
<li>Finding a minimum of a certain function.</li>
</ul>
<p>Cost function</p>
<ul>
<li>Find the difference between output and the desired result.</li>
<li>If cost is small, then the model produces result that is closed to the desired result.</li>
<li>Input: weights and biases, Output: 1 number(the cost).</li>
</ul>
<p>Gradient, the direction of steepest increase. The opposite of Gradient is the steepest decrease.
The learning process is to adjust the weights to get the minimum cost.</p>
<p>Local minimum is relatively easier to find than Global minimum.</p>
<p><strong>Network Learning: minimizing the cost function.</strong></p>
<h3 id="day-3">Day 3</h3>
<p>What is backpropagation?</p>
<ul>
<li>An algorithm for determining a single training example for changing the weights in the previous layers.</li>
</ul>
<p>Which weight changes can rapidly decrease the cost? Some weight changes have bigger affect on the desired output.</p>
<p>A true gradient descent will involve calculating all the data set, which may take long to compute.</p>
<ul>
<li>Solution: Mini-batches can divide data set into multiple batches and calculate each batch&rsquo;s gradient descent step. After recursively applying each gradient descent step, we will eventually get the local minimum of our data set.</li>
</ul>
<p><strong>Backpropagation: tune weights to lean towards the desired output.</strong></p>
<h3 id="day-4">Day 4</h3>
<p>What is GPT(Generative Pre-trained Transformer)?</p>
<ul>
<li>Transformer basically takes an input and generate a output. For instance, text-to-speech, text-to-image, Google translate are different kinds of transformers.</li>
</ul>
<p>How does a Transformer work?</p>
<ul>
<li>A sentence is fed into a transformer. Transformer splits a sentence into multiple <strong>tokens</strong> as a word or a part of a word. Each token is represented as a vector. Then, this sequence of vectors are passed to an <strong>Attention block</strong> for vectors to communicate and pass values with each other. After Attention block, these vectors are passed to MLP(Multilayer Perceptron). MLP consists of many matrix multiplication. The process of Attention and MLP are repeated multiple times until the last vectors have a vector of probability of outcome.</li>
</ul>
<h3 id="day-5">Day 5</h3>
<p>In the last example, a sentence is spilt into multiple tokens. Each token is represented as a vector to embedd its meaning in a multi-dimension space(GPT has 12***).</p>
<p>An Embedded matrix is a matrix that represent the meaning of each token using vector. In the Attention block, this matrix is changed as it is learning context from these tokens.</p>
<p>Unembedding a matrix is a step where we get the last vector and multiply with a weight to get a vector which includes each token in the vocabulary. Then we feed this vector(Logits) to a function called <strong>softmax</strong>(sometimes with T(temperature), the higher the T is, the less it&rsquo;s going to resemble the input distribution) to normalize the vector into a probability distribution.</p>
<h3 id="day-6">Day 6</h3>
<p>What is Attention supposed to do?</p>
<ul>
<li>Refine the meaning of a token, which is to understand the context.</li>
</ul>
<p>Query is represented as a vector(weighted sum), which is kind of like asking questions to a token.
Key is also represented as a vector, which is kind of like answering queries with dot product of query.
Also, we don&rsquo;t want later tokens affected the toke appeared before them in an attention matrix. This process is called masking.</p>
<p>Value is associated with keys, and we applied changes of weighted sum to refine our embeddings.</p>
]]></content:encoded>
    </item>
  </channel>
</rss>
