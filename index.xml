<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title> :)</title>
    <link>https://leewei.co/</link>
    <description>Recent content on  :)</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <managingEditor>lee10202013@gmail.com (Lee Wei)</managingEditor>
    <webMaster>lee10202013@gmail.com (Lee Wei)</webMaster>
    <copyright>Â© 2024 Lee Wei</copyright>
    <lastBuildDate>Sat, 21 Sep 2024 00:20:31 -0600</lastBuildDate>
    <atom:link href="https://leewei.co/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Deep Learning</title>
      <link>https://leewei.co/deep-learning/</link>
      <pubDate>Sat, 21 Sep 2024 00:20:31 -0600</pubDate><author>lee10202013@gmail.com (Lee Wei)</author>
      <guid>https://leewei.co/deep-learning/</guid>
      <description>Day 1 What is Neural Network?
Neuron: holds a number between 0 and 1. The number is also called &amp;ldquo;Activation&amp;rdquo;. Network: multiple layers of neurons connected together. A layer is a series of neurons. Each layer&amp;rsquo;s output affect the neurons of the next layer.
Why the layers?
Each layer construct some parts of information for the next layer until the output layer. Each layer can be represented as a vector. Each neuron in a layer has a weight that connects to the next layer&amp;rsquo;s neuron.</description>
      <content:encoded><![CDATA[<h3 id="day-1">Day 1</h3>
<p>What is Neural Network?</p>
<ul>
<li>Neuron: holds a number between 0 and 1. The number is also called &ldquo;Activation&rdquo;.</li>
<li>Network: multiple layers of neurons connected together.</li>
</ul>
<p>A layer is a series of neurons. Each layer&rsquo;s output affect the neurons of the next layer.</p>
<p>Why the layers?</p>
<ul>
<li>Each layer construct some parts of information for the next layer until the output layer.</li>
<li>Each layer can be represented as a vector.</li>
</ul>
<p>Each neuron in a layer has a weight that connects to the next layer&rsquo;s neuron.
This weight can be seen as a parameter that help detect a certain pattern of our input.</p>
<p>A weighted sum could be any number, but we want this sum value to fit between 0 and 1.
We can utilize a Sigmoid function(old school) or Rectified linear unit.</p>
<p>Bias for inactivity, which can be subtracted to the weighted sum before inputing to Sigmoid or ReLU.</p>
<p><strong>Learning: finding the right weights and biases.</strong></p>
<h3 id="day-2">Day 2</h3>
<p>What is Gradient descent?</p>
<ul>
<li>Finding a minimum of a certain function.</li>
</ul>
<p>Cost function</p>
<ul>
<li>Find the difference between output and the desired result.</li>
<li>If cost is small, then the model produces result that is closed to the desired result.</li>
<li>Input: weights and biases, Output: 1 number(the cost).</li>
</ul>
<p>Gradient, the direction of steepest increase. The opposite of Gradient is the steepest decrease.
The learning process is to adjust the weights to get the minimum cost.</p>
<p>Local minimum is relatively easier to find than Global minimum.</p>
<p><strong>Network Learning: minimizing the cost function.</strong></p>
<h3 id="day-3">Day 3</h3>
<p>What is backpropagation?
An algorithm for determining a single training example for changing the weights in the previous layers.
Which weight changes can rapidly decrease the cost? Some weight changes have bigger affect on the desired output.</p>
<p>A true gradient descent will involve calculating all the data set, which may take long to compute.
Solution: Mini-batches can divide data set into multiple batches and calculate each batch&rsquo;s gradient descent step.
After recursively applying each gradient descent step, we will eventually get the local minimum of our data set.</p>
<p><strong>Backpropagation: tune weights to lean towards the desired output.</strong></p>
]]></content:encoded>
    </item>
  </channel>
</rss>
