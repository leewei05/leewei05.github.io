<!doctype html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <link rel="shortcut icon" href="/images/favicon.png" />

<title>Fall 2025: CS 6150 Graduate Algorithm&nbsp;|&nbsp; :)</title>
<meta
  name="title"
  content="Fall 2025: CS 6150 Graduate Algorithm"
/>
<meta
  name="description"
  content="Week 1 Three key steps Describe algorithm Analyze time complexity Prove correctness: proof by induction Proof by Induction Base case Induction assumption Induction claim Proof Conclusion Binary Search: Inductive proof: Theorem: Given a sorted array A of n elements, binary search correctly finds the index of a target value T if T is in A.
Proof by Induction on the size of the array n:
Base Case (n=1): If the array has only one element, A[0], we compare it to the target T."
/>
<meta
  name="keywords"
  content=""
/>

  <meta name="author" content="Lee Wei" />




<meta property="og:title" content="Fall 2025: CS 6150 Graduate Algorithm" />
<meta property="og:description" content="Week 1 Three key steps Describe algorithm Analyze time complexity Prove correctness: proof by induction Proof by Induction Base case Induction assumption Induction claim Proof Conclusion Binary Search: Inductive proof: Theorem: Given a sorted array A of n elements, binary search correctly finds the index of a target value T if T is in A.
Proof by Induction on the size of the array n:
Base Case (n=1): If the array has only one element, A[0], we compare it to the target T." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://leewei.co/cs6150/" /><meta property="article:section" content="blog" />
<meta property="article:published_time" content="2025-09-10T21:22:23-06:00" />
<meta property="article:modified_time" content="2025-09-10T21:22:23-06:00" />




<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Fall 2025: CS 6150 Graduate Algorithm"/>
<meta name="twitter:description" content="Week 1 Three key steps Describe algorithm Analyze time complexity Prove correctness: proof by induction Proof by Induction Base case Induction assumption Induction claim Proof Conclusion Binary Search: Inductive proof: Theorem: Given a sorted array A of n elements, binary search correctly finds the index of a target value T if T is in A.
Proof by Induction on the size of the array n:
Base Case (n=1): If the array has only one element, A[0], we compare it to the target T."/>




<meta itemprop="name" content="Fall 2025: CS 6150 Graduate Algorithm">
<meta itemprop="description" content="Week 1 Three key steps Describe algorithm Analyze time complexity Prove correctness: proof by induction Proof by Induction Base case Induction assumption Induction claim Proof Conclusion Binary Search: Inductive proof: Theorem: Given a sorted array A of n elements, binary search correctly finds the index of a target value T if T is in A.
Proof by Induction on the size of the array n:
Base Case (n=1): If the array has only one element, A[0], we compare it to the target T."><meta itemprop="datePublished" content="2025-09-10T21:22:23-06:00" />
<meta itemprop="dateModified" content="2025-09-10T21:22:23-06:00" />
<meta itemprop="wordCount" content="2245">
<meta itemprop="keywords" content="" />
<meta name="referrer" content="no-referrer-when-downgrade" />

    
    
    
    <link href="/bundle.min.css" rel="stylesheet" />

    

    


    
</head>

  <body>
    <header>
      <nav>
  <a
    href="/"
    
    >Home</a
  >

  <a
    href="/blog/"
    
    >Blog</a
  >


  <a href="/index.xml">
    <svg xmlns="http://www.w3.org/2000/svg" class="icon" viewBox="0 0 448 512">
      
      <path
        d="M0 64C0 46.3 14.3 32 32 32c229.8 0 416 186.2 416 416c0 17.7-14.3 32-32 32s-32-14.3-32-32C384 253.6 226.4 96 32 96C14.3 96 0 81.7 0 64zM0 416a64 64 0 1 1 128 0A64 64 0 1 1 0 416zM32 160c159.1 0 288 128.9 288 288c0 17.7-14.3 32-32 32s-32-14.3-32-32c0-123.7-100.3-224-224-224c-17.7 0-32-14.3-32-32s14.3-32 32-32z"
      />
    </svg>
    RSS
  </a>

</nav>

<h1>Fall 2025: CS 6150 Graduate Algorithm</h1>


    </header>
    <main>
      
  
    
      <p>
        <i>
          <time
            style="color: var(--text-light);"
            datetime="2025-09-10"
            pubdate
          >
            2025-09-10
          </time>
        </i>
      </p>
    
  
  
  <content>
    <h3 id="week-1">Week 1</h3>
<ul>
<li>Three key steps
<ul>
<li>Describe algorithm</li>
<li>Analyze time complexity</li>
<li>Prove correctness: proof by induction</li>
</ul>
</li>
<li><strong>Proof by Induction</strong>
<ul>
<li>Base case</li>
<li>Induction assumption</li>
<li>Induction claim</li>
<li>Proof</li>
<li>Conclusion</li>
</ul>
</li>
<li><strong>Binary Search: Inductive proof</strong>:</li>
</ul>
<p><strong>Theorem</strong>: Given a sorted array <code>A</code> of <code>n</code> elements, binary search correctly finds the index of a target value <code>T</code> if <code>T</code> is in <code>A</code>.</p>
<p><strong>Proof by Induction on the size of the array <code>n</code>:</strong></p>
<ul>
<li>
<p><strong>Base Case (n=1)</strong>: If the array has only one element, <code>A[0]</code>, we compare it to the target <code>T</code>. If <code>A[0] == T</code>, we have found the element at index 0. If not, the element is not in the array. The algorithm works correctly for the base case.</p>
</li>
<li>
<p><strong>Inductive Hypothesis</strong>: Assume that for any sorted array of size <code>k &lt; n</code>, binary search correctly finds the target value if it is present.</p>
</li>
<li>
<p><strong>Inductive Step (for an array of size n)</strong>:</p>
<ol>
<li>Binary search first compares the target <code>T</code> with the middle element of the array, <code>A[mid]</code>, where <code>mid = floor(n/2)</code>.</li>
<li><strong>Case 1: <code>A[mid] == T</code></strong>: The element is found at index <code>mid</code>. The algorithm terminates and is correct.</li>
<li><strong>Case 2: <code>T &lt; A[mid]</code></strong>: If the target is less than the middle element, then <code>T</code> must be in the left subarray <code>A[0...mid-1]</code>, if it exists in the array at all. This left subarray is a sorted array of size <code>mid</code>, which is smaller than <code>n</code>. By the inductive hypothesis, binary search will correctly find <code>T</code> in this smaller subarray.</li>
<li><strong>Case 3: <code>T &gt; A[mid]</code></strong>: If the target is greater than the middle element, then <code>T</code> must be in the right subarray <code>A[mid+1...n-1]</code>, if it exists in the array at all. This right subarray is a sorted array of size <code>n - mid - 1</code>, which is smaller than <code>n</code>. By the inductive hypothesis, binary search will correctly find <code>T</code> in this smaller subarray.</li>
</ol>
</li>
<li>
<p><strong>Conclusion</strong>: In all cases, the algorithm either finds the target value or reduces the problem to a smaller subarray. By the principle of mathematical induction, binary search is correct for all <code>n &gt;= 1</code>.</p>
</li>
</ul>
<h4 id="time-complexity-of-binary-search">Time Complexity of Binary Search</h4>
<p>The time complexity of binary search can be described by the following recurrence relation:</p>
<p><code>T(n) = T(n/2) + c</code></p>
<p>Where:</p>
<ul>
<li><code>T(n)</code> is the time complexity for an array of size <code>n</code>.</li>
<li><code>T(n/2)</code> is the time taken to search in the subarray of size <code>n/2</code>.</li>
<li><code>c</code> is the constant time taken for the comparison and arithmetic operations.</li>
</ul>
<p>This recurrence can be solved using the <strong>Master Theorem</strong>:</p>
<p>For a recurrence of the form <code>T(n) = aT(n/b) + f(n)</code>, where <code>a &gt;= 1</code> and <code>b &gt; 1</code>, there are three cases. In our case, <code>a = 1</code>, <code>b = 2</code>, and <code>f(n) = c</code> (which is O(n^0)).
This falls into <strong>Case 2</strong> of the Master Theorem, where <code>f(n) = O(n^c)</code> and <code>c = log_b(a)</code>. Here, <code>log_b(a) = log_2(1) = 0</code>, so <code>c = 0</code>.
Therefore, the time complexity of binary search is <code>O(log n)</code>.</p>
<h4 id="master-theorem">Master Theorem</h4>
<p>The Master Theorem is a tool for solving recurrence relations of the form:</p>
<p><code>T(n) = aT(n/b) + f(n)</code></p>
<p>where <code>a &gt;= 1</code> and <code>b &gt; 1</code> are constants, and <code>f(n)</code> is an asymptotically positive function.</p>
<p>The theorem has three cases:</p>
<ul>
<li>
<p><strong>Case 1</strong>: If <code>f(n) = O(n^c)</code> for some constant <code>c &lt; log_b(a)</code>, then <code>T(n) = O(n^(log_b(a)))</code>.</p>
</li>
<li>
<p><strong>Case 2</strong>: If <code>f(n) = O(n^c * (log n)^k)</code> for some constants <code>c = log_b(a)</code> and <code>k &gt;= 0</code>, then <code>T(n) = O(n^c * (log n)^(k+1))</code>.</p>
<ul>
<li>A simpler version of this case is when <code>k=0</code>, if <code>f(n) = O(n^c)</code> where <code>c = log_b(a)</code>, then <code>T(n) = O(n^c * log n)</code>.</li>
</ul>
</li>
<li>
<p><strong>Case 3</strong>: If <code>f(n) = Omega(n^c)</code> for some constant <code>c &gt; log_b(a)</code>, and if <code>a * f(n/b) &lt;= k * f(n)</code> for some constant <code>k &lt; 1</code> and sufficiently large <code>n</code> (the &ldquo;regularity condition&rdquo;), then <code>T(n) = O(f(n))</code>.</p>
</li>
</ul>
<h3 id="week-2">Week 2</h3>
<ul>
<li><strong>Prefix Tree (Trie)</strong>: A tree-like data structure that stores a dynamic set of strings. Each node represents a common prefix of a set of strings.
<ul>
<li><strong>Time Complexity</strong>:
<ul>
<li><code>Insertion</code>: O(L), where L is the length of the string.</li>
<li><code>Search</code>: O(L).</li>
</ul>
</li>
<li><strong>Space Complexity</strong>: O(N*L), where N is the number of strings and L is their average length.</li>
</ul>
</li>
<li><strong>Problem</strong>: Given a collection of N documents, find all documents that contain a given set of m words.
<ul>
<li><strong>Naive Solution</strong>: Iterate through all N documents and for each document, check if it contains the m words. Time complexity: O(N * M * W), where W is the number of words in a document. A simpler view is O(N*m) if we consider the check for m words to be proportional to m.</li>
<li><strong>Inverted Index</strong>: A more efficient solution. For each word in the vocabulary, we store a list of documents in which it appears. To find documents containing a set of words, we take the intersection of the lists for each word.</li>
</ul>
</li>
<li><strong>Dynamic Arrays</strong>: A data structure that can grow in size. In C++, this is implemented as <code>std::vector</code>.
<ul>
<li>It starts with a certain capacity. When the capacity is reached, a new, larger array (usually double the size) is allocated, and all elements are copied over.</li>
<li><strong>Time Complexity</strong>:
<ul>
<li><code>Add</code> (to the end): O(1) on average (amortized), but O(n) in the worst case (when resizing).</li>
<li><code>Delete-from-end</code>: O(1).</li>
<li><code>Clear</code>: O(n).</li>
<li><code>Return i-th element</code>: O(1).</li>
</ul>
</li>
</ul>
</li>
<li><strong>Amortized Analysis</strong>: A method for analyzing the average performance of an algorithm over a sequence of operations, rather than the worst-case performance of a single operation.
<ul>
<li>Even if some operations are very slow, the average time over a sequence of N operations can be efficient, e.g., O(N) total time, which is O(1) amortized time per operation.</li>
</ul>
</li>
<li><strong>Divide and Conquer</strong>: An algorithmic paradigm that involves:
<ol>
<li><strong>Divide</strong>: Breaking the problem into smaller subproblems of the same type.</li>
<li><strong>Conquer</strong>: Solving the subproblems recursively.</li>
<li><strong>Combine</strong>: Combining the solutions to the subproblems to solve the original problem.</li>
</ol>
<ul>
<li>The correctness of divide and conquer algorithms is often proven by induction.</li>
</ul>
</li>
<li><strong>Merge Sort</strong>: A classic example of a divide and conquer algorithm for sorting.
<ul>
<li><strong>Algorithm</strong>:
<ol>
<li>Divide the array into two halves.</li>
<li>Recursively sort each half.</li>
<li>Merge the two sorted halves.</li>
</ol>
</li>
<li><strong>Time Complexity</strong>: <code>T(n) = 2T(n/2) + O(n)</code>, which resolves to <code>O(n log n)</code>.</li>
<li><strong>Proof by Induction</strong>:
<ul>
<li><strong>Base Case (n=1)</strong>: An array of size 1 is already sorted.</li>
<li><strong>Inductive Hypothesis</strong>: Assume that merge sort correctly sorts any array of size <code>k &lt; n</code>.</li>
<li><strong>Inductive Step (for an array of size n)</strong>: Merge sort recursively sorts two subarrays of size <code>n/2</code>. By the inductive hypothesis, these two subarrays are correctly sorted. The <code>merge</code> step then combines these two sorted subarrays into a single sorted array. Therefore, the entire array is sorted.</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="week-3">Week 3</h3>
<ul>
<li><strong>Recursion Tree Method</strong>: A visual method for analyzing the time complexity of recurrence relations.
<ul>
<li><strong>How it works</strong>:
<ol>
<li>Draw a tree to represent the recursive calls. Each node represents the cost of a single subproblem.</li>
<li>Sum the costs at each level of the tree to find the cost per level.</li>
<li>Sum the costs of all levels to get the total cost of the algorithm.</li>
</ol>
</li>
<li><strong>Example: Merge Sort</strong> <code>T(n) = 2T(n/2) + cn</code>
<ul>
<li>The tree has <code>log n</code> levels.</li>
<li>Each level has a total cost of <code>cn</code>.</li>
<li>Total cost = <code>cn * log n</code>, so the complexity is <code>O(n log n)</code>.</li>
</ul>
</li>
</ul>
</li>
<li><strong>Integer Multiplication</strong>: Algorithms for multiplying large integers.
<ul>
<li><strong>Standard (Grade-School) Algorithm</strong>:
<ul>
<li>Time Complexity: <code>O(n^2)</code>.</li>
</ul>
</li>
<li><strong>Karatsuba Algorithm</strong>: A more efficient divide-and-conquer algorithm.
<ul>
<li><strong>Idea</strong>: Reduces 4 subproblems to 3.</li>
<li>Let <code>x</code> and <code>y</code> be two n-bit integers.</li>
<li><code>x = a * 2^(n/2) + b</code></li>
<li><code>y = c * 2^(n/2) + d</code></li>
<li><code>x * y = (ac) * 2^n + (ad + bc) * 2^(n/2) + bd</code></li>
<li>Karatsuba computes <code>ac</code>, <code>bd</code>, and <code>(a+b)(c+d)</code>. Then <code>ad + bc = (a+b)(c+d) - ac - bd</code>.</li>
<li><strong>Recurrence</strong>: <code>T(n) = 3T(n/2) + O(n)</code>.</li>
<li><strong>Time Complexity</strong>: <code>O(n^log2(3))</code>, which is approximately <code>O(n^1.585)</code>.</li>
</ul>
</li>
<li><strong>Faster Algorithms</strong>: For very large numbers, even faster algorithms exist, such as the Toom-Cook and Schönhage-Strassen algorithms.</li>
</ul>
</li>
<li><strong>Akra-Bazzi Theorem</strong>: A generalization of the Master Theorem for solving recurrences of the form:
<code>T(x) = g(x) + \sum_{i=1}^{k} a_i T(b_i x)</code> for <code>x &gt;= x_0</code>.
<ul>
<li><strong>Conditions</strong>:
<ul>
<li><code>a_i &gt; 0</code> and <code>0 &lt; b_i &lt; 1</code> are constants.</li>
<li><code>g(x)</code> is a non-negative function.</li>
</ul>
</li>
<li><strong>Solution</strong>: Find <code>p</code> such that <code>\sum_{i=1}^{k} a_i * (b_i^p) = 1</code>. Then the solution is:
<code>T(x) = Theta(x^p * (1 + \int_{1}^{x} (g(u) / u^(p+1)) du))</code></li>
<li>This theorem is powerful for analyzing recurrences where the subproblems have different sizes.</li>
</ul>
</li>
<li><strong>The Selection Problem (k-th Smallest Element)</strong>: The problem of finding the k-th smallest element in an unsorted list of n elements.
<ul>
<li><strong>Solution 1: Sorting</strong>
<ul>
<li>Sort the list and return the element at index k-1.</li>
<li>Time Complexity: <code>O(n log n)</code>.</li>
</ul>
</li>
<li><strong>Solution 2: Quickselect</strong>
<ul>
<li>A modification of the Quicksort algorithm.</li>
<li><strong>Algorithm</strong>:
<ol>
<li>Choose a pivot element.</li>
<li>Partition the array around the pivot.</li>
<li>If the pivot is at index k-1, we found the element.</li>
<li>If the pivot is at an index greater than k-1, recursively search in the left subarray.</li>
<li>If the pivot is at an index smaller than k-1, recursively search in the right subarray.</li>
</ol>
</li>
<li><strong>Time Complexity</strong>:
<ul>
<li>Average Case: <code>O(n)</code>.</li>
<li>Worst Case: <code>O(n^2)</code> (if the pivot is always the smallest or largest element).</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><strong>Median of Medians (Deterministic Selection Algorithm)</strong>: A linear-time algorithm for the selection problem. It&rsquo;s a way to choose a good pivot for Quickselect to avoid the worst-case scenario.
<ul>
<li><strong>Algorithm</strong>:
<ol>
<li><strong>Divide</strong>: Divide the <code>n</code> elements into <code>n/5</code> groups of 5 elements each.</li>
<li><strong>Find Medians</strong>: Find the median of each of the <code>n/5</code> groups. This can be done in constant time for each group (e.g., by sorting each group of 5). This gives a list of <code>n/5</code> medians.</li>
<li><strong>Find Median of Medians</strong>: Recursively call the algorithm to find the true median of the <code>n/5</code> medians found in the previous step. This median is the pivot.</li>
<li><strong>Partition</strong>: Partition the original array around the pivot. Let the pivot&rsquo;s final position be <code>p</code>.</li>
<li><strong>Recurse</strong>:
<ul>
<li>If <code>p == k-1</code>, the pivot is the k-th smallest element.</li>
<li>If <code>p &gt; k-1</code>, recursively find the k-th smallest element in the subarray to the left of the pivot.</li>
<li>If <code>p &lt; k-1</code>, recursively find the (k-p)-th smallest element in the subarray to the right of the pivot.</li>
</ul>
</li>
</ol>
</li>
<li><strong>Time Complexity</strong>: <code>O(n)</code>.
<ul>
<li>The pivot (median of medians) is guaranteed to be &ldquo;good&rdquo;. At least 3/10 of the elements are smaller than the pivot, and at least 3/10 are larger.</li>
<li>This leads to the recurrence: <code>T(n) &lt;= T(n/5) + T(7n/10) + O(n)</code>, which solves to <code>T(n) = O(n)</code>.</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="week-4">Week 4</h3>
<ul>
<li>
<p><strong>The Subset Sum Problem</strong></p>
<ul>
<li><strong>Problem</strong>: Given a set of <code>n</code> non-negative integers and a target value <code>S</code>, determine if there is a subset of the given set whose elements sum up to <code>S</code>.</li>
<li><strong>Solution 1: Brute Force</strong>
<ul>
<li><strong>Algorithm</strong>: Generate all <code>2^n</code> possible subsets, calculate the sum of each, and check if any sum equals <code>S</code>.</li>
<li><strong>Time Complexity</strong>: <code>O(n * 2^n)</code>. This is infeasible for large <code>n</code>.</li>
</ul>
</li>
<li><strong>Solution 2: Divide and Conquer (Horowitz and Sahni)</strong>
<ul>
<li><strong>Idea</strong>: Split the problem into two smaller instances.</li>
<li><strong>Algorithm</strong>:
<ol>
<li>Split the set of <code>n</code> integers into two halves, <code>A</code> and <code>B</code>, each of size <code>n/2</code>.</li>
<li>Generate all <code>2^(n/2)</code> subset sums for <code>A</code> and store them in a sorted list, <code>sums_A</code>.</li>
<li>Do the same for <code>B</code>, creating a sorted list <code>sums_B</code>.</li>
<li>Iterate through <code>sums_A</code>. For each sum <code>s_A</code>, search for the value <code>S - s_A</code> in <code>sums_B</code> using binary search.</li>
<li>If <code>S - s_A</code> is found at any point, a solution exists.</li>
</ol>
</li>
<li><strong>Time Complexity</strong>: <code>O(n * 2^(n/2))</code>.
<ul>
<li>Generating the sums takes <code>O(2^(n/2))</code>.</li>
<li>Sorting takes <code>O(2^(n/2) * log(2^(n/2))) = O(n * 2^(n/2))</code>.</li>
<li>The final search takes <code>O(2^(n/2) * log(2^(n/2))) = O(n * 2^(n/2))</code> if we do a binary search for each element. A more optimized linear scan with two pointers on the sorted lists achieves this step in <code>O(2^(n/2))</code>. The dominant part is sorting.</li>
</ul>
</li>
</ul>
</li>
<li><strong>Solution 3: Dynamic Programming</strong>
<ul>
<li><strong>Idea</strong>: Build a table to store solutions to subproblems.</li>
<li><strong>Algorithm</strong>: Create a boolean table <code>dp[n+1][S+1]</code>, where <code>dp[i][j]</code> is <code>true</code> if a subset of the first <code>i</code> items can sum to <code>j</code>, and <code>false</code> otherwise.</li>
<li><strong>Recurrence</strong>: <code>dp[i][j] = dp[i-1][j] || (j &gt;= set[i-1] &amp;&amp; dp[i-1][j - set[i-1]])</code>.</li>
<li><strong>Time Complexity</strong>: <code>O(n * S)</code>. This is a pseudo-polynomial time complexity. It is very efficient if <code>S</code> is small, but can be worse than the divide-and-conquer approach if <code>S</code> is very large.</li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>Shortest Path Algorithms</strong></p>
<ul>
<li>
<p><strong>Context</strong>: Finding the shortest path between vertices in a graph. The choice of algorithm depends on the problem (single-source vs. all-pairs) and graph properties (negative weights).</p>
</li>
<li>
<p><strong>Standard Algorithms (Dynamic Programming flavor)</strong>:</p>
<ul>
<li><strong>Dijkstra&rsquo;s Algorithm</strong>: Solves the single-source shortest path problem for graphs with non-negative edge weights.</li>
<li><strong>Bellman-Ford Algorithm</strong>: Solves the single-source problem and can handle negative edge weights (and detect negative cycles).</li>
<li><strong>Floyd-Warshall Algorithm</strong>: Solves the all-pairs shortest path problem.</li>
</ul>
</li>
<li>
<p><strong>Divide and Conquer Approach for All-Pairs Shortest Path (APSP)</strong></p>
<ul>
<li><strong>Idea</strong>: This approach is a recursive formulation that mirrors the structure of the Floyd-Warshall algorithm.</li>
<li><strong>Algorithm</strong>:
<ul>
<li>Let <code>dist(i, j, k)</code> be the shortest path from <code>i</code> to <code>j</code> using only intermediate vertices from the set <code>{1, ..., k}</code>.</li>
<li><strong>Base Case</strong>: <code>dist(i, j, 0)</code> is the weight of the direct edge between <code>i</code> and <code>j</code> (or infinity if no direct edge exists).</li>
<li><strong>Recursive Step</strong>: To compute <code>dist(i, j, k)</code>, the shortest path either uses vertex <code>k</code> or it doesn&rsquo;t.
<ul>
<li>If it doesn&rsquo;t use <code>k</code>, the path is <code>dist(i, j, k-1)</code>.</li>
<li>If it does use <code>k</code>, the path is <code>dist(i, k, k-1) + dist(k, j, k-1)</code>.</li>
</ul>
</li>
<li><strong>Recurrence</strong>: <code>dist(i, j, k) = min(dist(i, j, k-1), dist(i, k, k-1) + dist(k, j, k-1))</code>.</li>
</ul>
</li>
<li><strong>Time Complexity</strong>: <code>O(n^3)</code>, same as the iterative Floyd-Warshall. The &ldquo;divide and conquer&rdquo; aspect comes from breaking down the problem by the set of allowed intermediate vertices.</li>
</ul>
</li>
</ul>
</li>
</ul>

  </content>
  
  

    </main>
    <footer>
      
  <span>© 2024 Lee Wei</span>


  <span>
    |
    Made with
    <a href="https://github.com/maolonglong/hugo-simple/">Hugo ʕ•ᴥ•ʔ Simple</a>
  </span>


    </footer>

    
</body>
</html>
